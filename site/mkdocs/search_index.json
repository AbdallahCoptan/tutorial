{
    "docs": [
        {
            "location": "/", 
            "text": "-\n- mode: markdown; mode: visual-line;  -\n-\n\n\nUL HPC Tutorials\n\n\n\n\n\n\n\n\n  Copyright (c) 2015 UL HPC Team aka. S. Varrette, H. Cartiaux, V. Plugaru, S. Diehl \nhpc-sysadmins@uni.lu\n\n\n\n\n| \nProject Page\n |  \nDocumentation\n | \nIssues\n |\n\n\nThis repository holds a set of tutorials to help the users of the \nUL HPC\n platform to better understand or simply use our platform. \n\n\nThe list of proposed tutorials are displayed on the left panel -- see also the last \nUL HPC School\n program.\n\n\nIssues / Feature request\n\n\nYou can submit bug / issues / feature request using the \nULHPC/tutorials Tracker\n. \n\n\nDevelopments / Contributing to the code\n\n\nIf you want to contribute to the code, you shall be aware of the way this module is organized. \nThese elements are detailed on \ndocs/contributing.md\n.\n\n\nYou are more than welcome to contribute to its development by \nsending a pull request\n. \n\n\nOnline Documentation\n\n\nRead the Docs\n aka RTFD hosts documentation for the open source community and the \nULHPC/sysadmins\n has its documentation (see the \ndocs/\n directly) hosted on \nreadthedocs\n.\n\n\nSee \ndocs/rtfd.md\n for more details.\n\n\nLicence\n\n\nThis project and the sources proposed within this repository are released under the terms of the \nGPL-3.0\n licence.", 
            "title": "Home"
        }, 
        {
            "location": "/#ul-hpc-tutorials", 
            "text": "Copyright (c) 2015 UL HPC Team aka. S. Varrette, H. Cartiaux, V. Plugaru, S. Diehl  hpc-sysadmins@uni.lu   |  Project Page  |   Documentation  |  Issues  |  This repository holds a set of tutorials to help the users of the  UL HPC  platform to better understand or simply use our platform.   The list of proposed tutorials are displayed on the left panel -- see also the last  UL HPC School  program.", 
            "title": "UL HPC Tutorials"
        }, 
        {
            "location": "/#issues-feature-request", 
            "text": "You can submit bug / issues / feature request using the  ULHPC/tutorials Tracker .", 
            "title": "Issues / Feature request"
        }, 
        {
            "location": "/#developments-contributing-to-the-code", 
            "text": "If you want to contribute to the code, you shall be aware of the way this module is organized. \nThese elements are detailed on  docs/contributing.md .  You are more than welcome to contribute to its development by  sending a pull request .", 
            "title": "Developments / Contributing to the code"
        }, 
        {
            "location": "/#online-documentation", 
            "text": "Read the Docs  aka RTFD hosts documentation for the open source community and the  ULHPC/sysadmins  has its documentation (see the  docs/  directly) hosted on  readthedocs .  See  docs/rtfd.md  for more details.", 
            "title": "Online Documentation"
        }, 
        {
            "location": "/#licence", 
            "text": "This project and the sources proposed within this repository are released under the terms of the  GPL-3.0  licence.", 
            "title": "Licence"
        }, 
        {
            "location": "/basic/getting_started/README/", 
            "text": "-\n- mode: markdown; mode: auto-fill; fill-column: 80 -\n-\n\nREADME.md\n\n\nCopyright (c) 2014 \nSebastien Varrette\n \nwww\n\n\n    Time-stamp: \nMar 2014-05-06 12:06 svarrette\n\n\n\n\n\n\nUL HPC Tutorial: Getting Started\n\n\nThis tutorial will guide you through your first steps on the\n\nUL HPC platform\n.  \n\n\nBefore proceeding: \n\n\n\n\nmake sure you have an account (if not, follow \nthis procedure\n), and an SSH client.\n\n\ntake a look at the \nquickstart guide\n\n\nensure you operate from a Linux / Mac environement. Most commands below assumes running in a Terminal in this context. If you're running Windows, you can use Putty tools etc. as described \non this page\n yet it's probably better that you familiarize \"natively\" with Linux-based environment by having a Linux Virtual Machine (consider for that \nVirtualBox\n). \n\n\n\n\nFrom a general perspective, the \nSupport page\n describes how to get help during your UL HPC usage. \n\n\nConvention\n\n\nIn the below tutorial, you'll proposed terminal commands where the prompt is denoted by \n$\n. \n\n\nIn general, we will prefix to precise the execution context (\ni.e.\n your laptop, a cluster frontend or a node). Remember that \n#\n character is a comment. Example: \n\n\n    # This is a comment \n    $\n hostname\n\n    (laptop)$\n hostname         # executed from your personal laptop / workstation\n\n    (access-gaia)$\n hostname    # executed from access server of the Gaia cluster\n\n\n\nPlatform overview.\n\n\nYou can find a brief overview of the platform with key characterization numbers \non this page\n.\n\n\nThe general organization of each cluster is depicted below:\n\n\n\n\nDetails on this organization can be found \nhere\n\n\nConnecting for the first time and preparing your SSH environment\n\n\n\n\nAccess / SSH Tutorial\n\n\n\n\nThe way SSH handles the keys and the configuration files is illustrated in the following figure:\n\n\n\n\nIn order to be able to login to the clusters, you have sent us through the Account request form the \npublic key\n (i.e. \nid_dsa.pub\n, \nid_rsa.pub\n or the \npublic key\n as saved by PuttY) you initially generated, enabling us to configure the \n~/.ssh/authorized_keys\n file of your account.  \n\n\nStep 1a: Connect to UL HPC (Linux / Mac OS / Unix)\n\n\nRun the following commands in a terminal (substituting \nyourlogin\n with the login name you received from us):\n\n\n    (laptop)$\n ssh -p 8022 yourlogin@access-chaos.uni.lu\n\n\n\nIf you want to connect to the gaia cluster, \n\n\n    (laptop)$\n ssh -p 8022 yourlogin@access-gaia.uni.lu\n\n\n\nNow you probably want to avoid taping this long command to connect to the platform. You can customize SSH aliases for that. Edit the file \n~/.ssh/config\n (create it if it does not already exist) and adding the following entries: \n\n\n    Host chaos-cluster\n        Hostname access-chaos.uni.lu\n\n    Host gaia-cluster\n        Hostname access-gaia.uni.lu\n\n    Host *-cluster\n        User yourlogin\n        Port 8022\n        ForwardAgent no\n\n\n\nNow you shall be able to issue the following (simpler) command to connect to the cluster and obtain the welcome banner: \n\n\n    (laptop)$\n ssh gaia-cluster\n\n    (laptop)$\n ssh chaos-cluster\n\n\n\nIn the sequel, we assume these aliases to be defined. \n\n\nStep 1b: Connect to UL HPC (Windows)\n\n\n\n\nDownload \nall the Putty tools\n\n\nextract them in an easy-to-find place, such as \nC:\\Putty\n\n\nload your private key under Pageant\n\n\nopen \nPutty.exe\n (connection type: \nSSH\n)\n\n\nIn \nCategory:Session\n:\n\n\nHost Name: \naccess-{chaos,gaia}.uni.lu\n\n\nPort: 8022\n\n\nSaved session: \n{Chaos,Gaia}\n\n\n\n\n\n\nIn \nCategory:Connection:Data\n :\n\n\nAuto-login username: \nyourlogin\n\n\n\n\n\n\nGo back to \nCategory:Session\n and click on \"Save\"\n\n\nClick on \"Open\"\n\n\n\n\nStep 2: configure your SSH environment on all clusters\n\n\nThe SSH key you provided us secure your connection \nfrom\n your laptop (or personal workstation) \nto\n the cluster frontends. It is thus important to protect them by a passphrase. \n\n\nYou shall have also a new key pair configured in your account to permit a bi-directional transparent connection from one cluster to the other (you can check that in your \n~/.ssh/authorized_keys\n and by successfully running: \n\n\n    (access-gaia)$\n ssh chaos-cluster\n\n\n\nor \n\n\n    (access-chaos)$\n ssh gaia-cluster\n\n\n\nIf that's the case, you can ignore the rest of this section. \n\nOtherwise\n, you will now have to configure a passphrase-free SSH key pair to permit a transparent connection from one cluster to another. \n\n\n\n\n\n\nConnect to the \nchaos\n cluster: \n\n\n(laptop)$\n ssh chaos-cluster\n\n\n\n\n\n\n\ngenerate a new SSH key pair with \nssh-keygen\n (leave the passphrase empty):\n\n\n(access-chaos)$\n ssh-keygen -t dsa\nGenerating public/private dsa key pair.\nEnter file in which to save the key (/home/users/yourlogin/.ssh/id_dsa): \nEnter passphrase (empty for no passphrase): \nYour identification has been saved in /home/users/yourlogin/.ssh/id_dsa.\nYour public key has been saved in /home/users/yourlogin/.ssh/id_dsa.pub.\nThe key fingerprint is:\n1f:1d:a5:66:3b:4a:68:bc:7d:8c:7f:33:c9:77:0d:4a yourlogin@access.chaos-cluster.uni.lux\nThe key's randomart image is:\n`-[ DSA 1024]---`\n|              .  |\n|             o   |\n|            =    |\n|       . . + o   |\n|        S o +    |\n|       . = =E..  |\n|        . =.oo o.|\n|           o. * +|\n|            .. +.|\n`---------------`\n\n\n\n\n\n\n\nauthorize the newly generated public key to be used during challenge/response authentication:\n\n\n(access-chaos)$\n cat ~/.ssh/id_dsa.pub \nssh-dss AAAAB[...]B2== yourlogin@access.chaos-cluster.uni.lux\n(access-chaos)$\n cat ~/.ssh/id_dsa.pub  \n ~/.ssh/authorized_keys\n\n\n\n\n\n\n\nyou can check that it works by connecting to localhost: \n\n\n```\n(access-chaos)$\n ssh -p 8022 localhost\n[...]\n(access-chaos)$\n exit   # or CTRL-D\n```\n\n\n\n\n\n\n\nadd an alias to facilitate the connection to each cluster by adding the following SSH configuration entry in the file \n~/.ssh/config\n: \n\n\nHost gaia-cluster chaos-cluster\n    User yourlogin\n    Port 8022\n\nHost gaia-cluster\n    Hostname access-gaia.uni.lu\nHost chaos-cluster\n    Hostname access-chaos.uni.lu\n\n\n\n\n\n\n\nYou'll have to setup the same key package on the gaia cluster such that you can then work indefferently on one or another cluster. It's also the occasion to learn how to add a new SSH key to your authorized key portfolio. \n\n\n\n\n\n\nOpen another terminal and connect to the gaia cluster \n\n\n(laptop)$\n ssh gaia-cluster\n\n\n\n\n\n\n\nedit the file \n~/.ssh/authorized_keys\n to add your previously generated key on chaos (use \n:wq\n in \nvim\n to save and quit):\n\n\n(access-gaia)$\n vim ~/.ssh/authorized_keys\n\n\n\n\n\n\n\ngo back to the terminal where you're connected on chaos, you shall now be able to connect to gaia, and reversely: \n\n\n(access-chaos)$\n ssh gaia\n[...]\n(access-gaia)$\n exit     # or CRTL-D\n\n\n\n\n\n\n\nYou have a different home directory on each UL HPC site, so you will usually use Rsync or scp to move data around (see \ntransfering files tutorials\n).\n\n\nNow that we are able to connect \nfrom\n chaos \nto\n gaia, we will transfer the SSH keys and configuration in place from chaos and check that we can connnect back: \n\n\n    (access-chaos)$\n scp ~/.ssh/id_dsa* gaia:.ssh/\n    (access-chaos)$\n scp ~/.ssh/config  gaia:.ssh/\n    (access-chaos)$\n ssh gaia\n    [...]\n    (access-gaia)$\n  ssh chaos\n    (access-chaos)$\n exit     # or CRTL-D\n    (access-gaia)$\n  exit     # or CRTL-D\n\n\n\nSo now \nwe have setup a bi-directional transparent connection from one cluster to the other.\n \n\n\nStep 2bis: Using SSH proxycommand setup to access the clusters despite port filtering\n\n\nIt might happen that the port 8022 is filtered from your working place. You can easily bypass this firewall rule using an SSH proxycommand to setup transparently multi-hop connexions \nthrough\n one host (a gateway) to get to the access frontend of the cluster, as depited below: \n\n\n [laptop] -----||--------\n 22 [SSH gateway] ---------\n 8022 [access-{chaos,gaia}]\n            firewall\n\n\n\nThe gateway can be any SSH server which have access to the access frontend of the cluster. The \nGforge @ UL\n is typically used in this context but you can prefer any other alternative (your personal NAS @ home etc.). Then alter the SSH config on yout laptop (in \n~/.ssh/confg\n typically) as follows:\n\n\n\n\n\n\ncreate an entry to be able to connect to the gateway:\n\n\n# Alias for the gateway (not really needed, but convenient), below instanciated \nHost gw\n    User anotherlogin\n    Hostname host.domain.org\n    ForwardAgent no\n\n# Automatic connection to UL HPC from the outside via the gateway\nHost *.ulhpc\n    ProxyCommand ssh gw \"nc -q 0 `basename %h .ulhpc` %p\"\n\n\n\n\n\n\n\nensure you can connect to the gateway:\n\n\n(laptop)$\n ssh gw\n(gateway)$\n exit # or CTRL-D\n\n\n\n\n\n\n\nthe \n.ulhpc\n suffix we mentionned in the previous configuration is an arbitrary suffix you will now specify in your command lines in order to access the UL HPC platform via the gateway as follows: \n\n\n(laptop)$\n ssh gaia.ulhpc\n\n\n\n\n\n\n\nStep 3: transferring files\n\n\nDirectories such as \n$HOME\n, \n$WORK\n or \n$SCRATCH\n are shared among the nodes of the cluster that you are using (including the front-end) via shared filesystems (NFS, Lustre) meaning that:\n\n\n\n\nevery file/directory pushed or created on the front-end is available on the computing nodes\n\n\nevery file/directory pushed or created on the computing nodes is available on the front-end\n\n\n\n\nStep 3a: Linux / OS X / Unix command line tools\n\n\nThe two most common tools you can use for data transfers over SSH:\n\n\n\n\nscp\n: for the full transfer of files and directories (only works fine for single files or directories of small/trivial size)\n\n\nrsync\n: a software application which synchronizes files and directories from one location to another while minimizing data transfer as only the outdated or inexistent elements are transfered (practically required for lengthy complex transfers, which are more likely to be interrupted in the middle).\n\n\n\n\nOf both, normally the second approach should be preferred, as more generic; note that, both ensure a secure transfer of the data, within an encrypted tunnel.\n\n\n\n\n\n\nCreate a new directory on your local machine and download a file to transfer (next-gen sequencing data from the NIH Roadmap Epigenomics Project):\n\n\n(laptop)$\n mkdir file_transfer\n(laptop)$\n cd file_transfer\n(laptop)$\n wget \"ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM409nnn/GSM409307/suppl/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\"\n\n\n\n\n\n\n\nTransfer the file with scp:\n\n\n(laptop)$\n scp GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz gaia-cluster:\n\n\n\n\n\n\n\nConnect to the cluster, check if the file is there and delete it.\n\n\n(laptop)$\n ssh gaia-cluster\n(access-gaia)$\n ls\n(access-gaia)$\n rm GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\n(access-gaia)$\n exit\n\n\n\n\n\n\n\nTransfer the directory with rsync:\n\n\n(laptop)$\n cd ..\n(laptop)$\n rsync -avzu file_transfer gaia-cluster:\n\n\n\n\n\n\n\nDelete the file and retrieve it from the cluster:\n\n\n(laptop)$\n rm file_transfer/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\n(laptop)$\n rsync -avzu gaia-cluster:file_transfer .\n\n\n\n\n\n\n\nBonus\n: Check where the file is located on the cluster after the rsync.\n\n\n\n\n\n\nYou can get more information about these transfer methods in the \nfile transfer documentation\n.\n\n\nStep 3b: Windows / Linux / OS X / Unix GUI tools\n\n\n\n\nDownload the FileZilla client application from \nfilezilla-project.org\n and install it.\n\n\nFirst we need to tell FileZilla about our ssh key:\n\n\nStart the application.\n\n\nGo to the \nSettings\n (either under \nEdit\n or \nFileZilla\n depending on the OS).\n\n\nIn the category \nConnection\n select \nSFTP\n. \n\n\nClick on the button \nAdd keyfile...\n and select your private keyfile (you may need to convert it).\n\n\nFinally click \nOK\n to save and close the settings.\n\n\n\n\n\n\n\n\n\n\n\n\nBack in the main window click on the \nSite Manager\n button on the top left or select \nSite Manager\n from the \nFile\n menu.\n\n\nClick on the \nNew Site\n button and enter/select the following:\n\n\nHost: \naccess-gaia.uni.lu\n\n\nProtocol: \nSFTP - SSH File Transfer Protocol\n\n\nLogon Type: \nInteractive\n\n\nUser: your login\n\n\n\n\n\n\n\n\nClick on the \nConnect\n button.\n\n\nAccept the certificate.\n\n\n\n\nYou should now see something similar to the following window:\n\n\n\n\nOn the very top, beneath the quick connect, you see the message log. Below you have the directory tree and the contents of the current directory for you local computer on the left and the remote location on the right.\n\n\nTo transfer a file, simply drag and drop it from the directory listing on the left side to destination directory on the right (to transfer from local to remote) or vice versa (to transfer from remote to local). You can also select a file by left clicking on it once and then right click on it to get the context menu and select \"Upload\" or \"Download\" to transfer it.\n\n\nIf you skipped step 3a, you may download the following file (50 MB) for testing: \n\n\nftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM409nnn/GSM409307/suppl/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\n (next-gen sequencing data from the NIH Roadmap Epigenomics Project)\n\n\nWhen you click the fifth icon on the top with the two green arrows to toogle the transfer queue, you can see the status of ongoing transfers on the very bottom of the window.\n\n\n\n\nDiscovering, visualizing and reserving UL HPC resources\n\n\nIn the sequel, replace \nlogin\n in the proposed commands with you login on the platform (ex: \nsvarrette\n).   \n\n\nStep 1: the working environment\n\n\n\n\nreference documentation\n\n\n\n\nAfter a successful login onto one of the access node (see \nCluster Access\n), you end into your personal homedir \n$HOME\n which is shared over NFS between the access node and the computing nodes.\n\n\nAgain, remember that your homedir is placed on \nseparate\n NFS servers on each site, which \nARE NOT SYNCHRONIZED\n: data synchronization between each of them remain at your own responsability. We will see below that the UL HPC team prepared for you a script to facilitate the transfer of data between each site. \n\n\nOtherwise, you have to be aware of at least two directories: \n\n\n\n\n$HOME\n: your home directory under NFS. \n\n\n$SCRATCH\n: a non-backed up area put if possible under Lustre for fast I/O operations\n\n\n\n\nYour homedir is under a regular backup policy. Therefore you are asked to pay attention to your disk usage \nand\n the number of files you store there. \n\n\n\n\n\n\nestimate file space usage and summarize disk usage of each FILE, recursively for directories using the \nncdu\n command:\n\n\n(access)$\n ncdu\n\n\n\n\n\n\n\nyou shall also pay attention to the number of files in your homedirectory. You can count them as follows: \n\n\n(access)$\n find . -type f | wc -l\n\n\n\n\n\n\n\nStep 2: web monitoring interfaces\n\n\nEach cluster offers a set of web services to monitore the platform usage: \n\n\n\n\nA \npie-chart overview of the platform usage\n\n\nMonika\n, the visualization interface of the OAR scheduler, which  display the status of the clusters as regards the jobs running on the platform.\n\n\nDrawGantt\n, the Gantt visualization of jobs scheduled on OAR\n\n\nGanglia\n, a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids.\n\n\nCDash\n (internal UL network use)\n\n\n\n\nStep 3: Reserving resources with OAR: the basics\n\n\n\n\nreference documentation\n\n\n\n\nOAR\n is an open-source batch scheduler which provides simple yet flexible facilities for the exploitation of the UL HPC clusters.\n\n\n\n\nit permits to schedule jobs for users on the cluster resource\n\n\na \nOAR resource\n corresponds to a node or part of it (CPU/core)\n\n\na \nOAR job\n is characterized by an execution time (walltime) on a set of resources. \n  There exists two types of jobs: \n\n\ninteractive\n: you get a shell on the first reserve node\n\n\npassive\n: classical batch job where the script passed as argument to \noarsub\n is executed \non the first reserved node\n \n\n\n\n\nWe will now see the basic commands of OAR. \n\n\n\n\nConnect to one of the UL HPC  frontend. You can request resources in interactive mode:\n(access)$\n oarsub -I\n\n\n\n\n\n\n\nNotice that with no parameters, oarsub gave you one resource (one core) for two hour. You were also directly connected to the node you reserved with an interactive shell.\n  No exit the reservation: \n\n\n    (node)$\n exit      # or CTRL-D\n\n\n\nWhen you run exit, you are disconnected and your reservation is terminated. \n\n\nTo avoid anticipated termination of your jobs in case or errors (terminal closed by mistake), \nyou can reserve and connect in 2 steps using the job id associated to your reservation. \n\n\n\n\nFirst run a passive job \ni.e.\n run a predefined command -- here \nsleep 10d\n to delay the execution for 10 days -- on the first reserved node:\n(access)$\n oarsub \"sleep 10d\"\n[ADMISSION RULE] Set default walltime to 7200.\n[ADMISSION RULE] Modify resource description with type constraints\nOAR_JOB_ID=919309\n\n\n\n\n\n\n\nYou noticed that you received a job ID (in the above example: \n919309\n), which you can later use to connect to the reserved resource(s):\n\n\n    (access)$\n oarsub -C 919309        # adapt the job ID accordingly ;)\n    Connect to OAR job 919309 via the node e-cluster1-13\n    [OAR] OAR_JOB_ID=919309\n    [OAR] Your nodes are:\n        e-cluster1-13*1\n\n    (e-cluster1-13)$\n java -version\n    (e-cluster1-13)$\n hostname -f\n    (e-cluster1-13)$\n whoami\n    (e-cluster1-13)$\n env | grep OAR   # discover environment variables set by OAR\n    (e-cluster1-13)$\n exit             # or CTRL-D\n\n\n\nQuestion: At which moment the job \n919309\n will end?\n \n\n\na. after 10 days\nb. after 2 hours\nc. never, only when I'll delete the job  \n\n\nQuestion: manipulate the \n$OAR_NODEFILE\n variable over the command-line to extract the following information, once connected to your job\n\n\na. the list of hostnames where a core is reserved (one per line) \n   * \nhint\n: \nman cat\n\nb. number of reserved cores (one per line)\n   * \nhint\n: \nman wc\n --  use \nwc -l\n over the pipe \n|\n command\nc. number of reserved nodes (one per line)\n   * \nhint\n: \nman uniq\n -- use \nuniq\n over the pipe \n|\n command\nd. number of cores reserved per node together with the node name (one per line)\n   * Example of output: \n\n\n        12 gaia-11\n        12 gaia-15\n\n\n\n\n\nhint\n: \nman uniq\n -- use \nuniq -c\n over the pipe \n|\n command\ne. \n(for geeks)\n output the number of reserved nodes times number of cores per node\n\n\n\n\nExample of output:\n\n\n    gaia-11*12\n    gaia-15*12\n\n\n\n\n\n\n\nhint\n: \nman awk\n -- use \nprintf\n command of \nawk\n over the pipe command, for instance \nawk '{ printf \"%s*%d\\n\",$2,$1 }'\n. You might prefer \nsed\n or any other advanced geek command.\n\n\n\n\n\n\nStep 4: Job management\n\n\nNormally, the previously run job is still running.\n\n\n\n\nYou can check the status of your running jobs using \noarstat\n command:\n(access)$\n oarstat      # access all jobs \n(access)$\n oarstat -u   # access all your jobs\n\n\n\n\n\n\n\nThen you can delete your job by running \noardel\n command:\n\n\n    (access)$\n oardel 919309\n\n\n\n\n\nyou can see your consumption (in an historical computational measure named \nCPU hour\n i.e. the work done by a CPU in one hour of wall clock time) over a given time period using \noarstat --accounting \"YYYY-MM-DD, YYYY-MM-DD\" -u \nyoulogin\n:\n(access)$\n oarstat --accounting \"2013-01-01, 2013-12-31\" -u \nlogin\n\n\n\n\n\n\n\n\nIn particular, take a look at the difference between the \nasked\n resources and the \nused\n ones\n\n\nIn all remaining examples of reservation in this section, remember to delete the reserved jobs afterwards (using \noardel\n or \nCTRL-D\n)\n\n\nYou probably want to use more than one core, and you might want them for a different duration than two hours. \nThe \n-l\n switch allows you to pass a comma-separated list of parameters specifying the needed resources for the job.\n\n\n\n\n\n\nReserve interactively 4 cores for 6 hours (delete the job afterwards) \n\n\n(access)$\n oarsub -I -l core=6,walltime=6\n\n\n\n\n\n\n\nReserve interactively 2 nodes for 3h15 (delete the job afterwards): \n\n\n(access)$\n oarsub -I -l nodes=3,walltime=3:15\n\n\n\n\n\n\n\nHierarchical filtering of resources\n\n\nOAR features a very powerful resource filtering/matching engine able to specify resources in a \nhierarchical\n  way using the \n/\n delimiter. The resource property hierarchy is as follows: \n\n\n    enclosure -\n nodes -\n cpu -\n core\n\n\n\n\n\n\n\nReserve interactively 2 cores on 3 different nodes belonging to the same enclosure (\ntotal: 6 cores\n) for 3h15:\n\n\n(access)$\n oarsub -I -l /enclosure=1/nodes=3/core=2,walltime=3:15\n\n\n\n\n\n\n\nReserve interactively two full nodes belonging to the different enclosure for 6 hours: \n\n\n(access)$\n oarsub -I -l /enclosure=2/nodes=1,walltime=6\n\n\n\n\n\n\n\nQuestion: reserve interactively 2 cpus on 2 nodes belonging to the same enclosure for 4 hours\n  \n\n\nQuestion: in the following statements, explain the advantage and drawback (in terms of latency/bandwidth etc.) of each of the proposed approaches\n\n\na. \noarsub -I -l /nodes=2/cpu=1\n vs \noarsub -I -l cpu=2\n vs \noarsub -I -l /nodes=1/cpu=2\n\nb. \noarsub -I -l /enclosure=1/nodes=2\n vs \noarsub -I -l nodes=2\n vs \noarsub -I -l /enclosure=2/nodes=1\n\n\nUsing OAR properties\n\n\nYou might have notice on \nMonika\n for each site a list of properties assigned to each resources. \n\n\nThe \n-p\n switch allows you to specialize (as an SQL syntax) the property you wish to use when selecting the resources. The syntax is as follows: \noarsub -p \"\n property \n='\n value \n'\"\n\n\nYou can find the available OAR properties on the \nUL HPC documentation\n. The main ones are described below\n\n\n\n\n\n\n\n\nProperty\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nFull hostname of the resource\n\n\n-p \"host='h-cluster1-14.chaos-cluster.uni.lux'\"\n\n\n\n\n\n\nnetwork_address\n\n\nShort hostname of the resource\n\n\n-p \"network_address='h-cluster1-14'\"\n\n\n\n\n\n\ngpu\n\n\nGPU availability (gaia only)\n\n\n-p \"gpu='YES'\"\n\n\n\n\n\n\nnodeclass\n\n\nNode class (chaos only) 'h','d','e','s'\n\n\n-p \"nodeclass='h'\"\n\n\n\n\n\n\n\n\n\n\n\n\nreserve interactively 4 cores on a GPU node for 8 hours (\nthis holds only on the \ngaia\n cluster\n) (\ntotal: 4 cores\n)\n\n\n(access-gaia)$\n oarsub -I -l nodes=1/core=4,walltime=8 -p \"gpu=\u2019YES\u2019\"\n\n\n\n\n\n\n\nreserve interactively 4 cores on the GPU node \ngaia-65\n for 8 hours (\nthis holds only on the \ngaia\n cluster\n) (\ntotal: 4 cores\n)\n\n\n(access-gaia)$\n oarsub -I -l nodes=1/core=4,walltime=8 -p \"gpu='yes'\" -p \"network_address='gaia-65'\"\n\n\n\n\n\n\n\nQuestion: reserve interactively 2 nodes among the \nh-cluster1-*\n nodes (\nthis holds only on the \nchaos\n cluster\n) using the \nnodeclass\n property\n  \n\n\nYou can combine filters using the \n+\n sign.\n\n\n\n\nreserve interactively 4 cores, each on 2 GPU nodes and 20 cores on any other nodes (\ntotal: 28 cores\n)\n(access-gaia)$\n oarsub -I -l \"{gpu='YES'}/nodes=2/core=4+{gpu='NO'}/core=20\"\n[ADMISSION RULE] Set default walltime to 7200.\n[ADMISSION RULE] Modify resource description with type and ibpool constraints\nOAR_JOB_ID=2828104\nInteractive mode : waiting...\nStarting...\n\nConnect to OAR job 2828104 via the node gaia-11\n[OAR] OAR_JOB_ID=2828104\n[OAR] Your nodes are:\n    gaia-11*12\n    gaia-12*4\n    gaia-59*4\n    gaia-63*4\n    gaia-65*4\n\n\n\n\n\n\n\nReserving specific resources \nbigsmp\nand \nbigmem\n\n\nSome nodes are very specific (for instance the nodes with 1TB of memory or the BCS subsystem of Gaia composed of 4 motherboards of 4 processors with a total of 160 cores aggregated in a ccNUMA architecture). \n\nDue to this specificity, they are NOT scheduled by default\n  and can only be reserved with an explicit oarsub parameter: \n-t bigmem\n for \n-t bigsmp\n\n\n\n\nreserve interactively 2 cpu on the bigsmp node belonging to the same board for 3 hours: (\ntotal: 32 cores\n)\n(access-gaia)$\n oarsub -t bigsmp -I -l /board=1/cpu=2,walltime=3\n\n\n\n\n\n\n\nQuestion: why are these resources not scheduled by default?\n  \n\n\nOAR Containers\n\n\nWith OAR, it is possible to execute jobs within another one. This functionality is called \ncontainer jobs\n and is invoked using the \n-t container\n switch.\n\n\n\n\ncreate a container job of 2 nodes for 4h30: \n(access)$\n oarsub -t container -l nodes=2,walltime=4:30:00 \"sleep 1d\" \n[ADMISSION RULE] Modify resource description with type and ibpool constraints\nOAR_JOB_ID=2828112\n(access)$\n oarstat -u\n\n\n\n\n\n\n\nThis creates a kind of \"tunnel\" inside witch you can push subjobs using the \n-t inner=\ncontainer_job_id\n.\n\n\n\n\nreserve 3 sleep jobs (of different delay) over 10 cores within the previously created container job\n(access)$\n oarsub -t inner=2828112 -l core=10 \"sleep 3m\"   # Job 1 \n(access)$\n oarsub -t inner=2828112 -l core=10 \"sleep 2m\"   # Job 2\n(access)$\n oarsub -t inner=2828112 -l core=10 \"sleep 1m\"   # Job 3\n\n\n\n\n\n\n\nThese jobs will be scheduled as follows\n\n\n          ^ \n          |                        \n          +======================== ... ========================+\n          |                      ^       Container Job (4h30)   |\n       ^  |  +--------+----+     |                              |\n    20c|  |  |    J2  | J3 |     |24c                           | \n       |  |  +--------+----+     |                              |\n       v  |  |      J1     |     v                              | \n          +==+=============+========= ... ======================+\n          |   \n------\n---\n\n          |     2min    1min\n          +--------------------------------------------------------------\n time\n\n\n\nQuestion: Check the way your jobs have been scheduled\n\n\na. using \noarstat -u -f -j \nsubjob_id\n (take a look at the \nassigned_resources\n)\nb. using the \nOAR drawgantt\n interface  \n\n\nQuestion: explain the interest of container jobs for the platform managers\n\n\nReservation at a given period of time\n\n\nYou can use the \n-r \"YYYY-MM-DD HH:MM:SS\"\n option of \noarsub\n to specify the date you wish the reservation to be issued. This is of particular interest for you to book in advance resources out of the working hours (at night and/or over week ends) \n\n\nStep 5: Using modules\n\n\nEnvironment Modules\n is a software package that allows us to provide a \nmultitude of applications and libraries in multiple versions\n on the UL HPC platform. The tool itself is used to manage environment variables such as \nPATH\n, \nLD_LIBRARY_PATH\n and \nMANPATH\n, enabling the easy loading and unloading of application/library profiles and their dependencies.\n\n\nWe will have multiple occasion to use modules in the other tutorials so there is nothing special we foresee here. You are just encouraged to read the following resources: \n\n\n\n\nIntroduction to Environment Modules by Wolfgang Baumann\n\n\nModules tutorial @ NERSC\n\n\nUL HPC documentation on modules\n\n\n\n\nStep 6 (advanced): Job management and Persistent Terminal Sessions using GNU Screen\n\n\nGNU Screen\n is a tool to manage persistent terminal sessions. \nIt becomes interesting since you will probably end at some moment with the following  scenario:\n\n\n\n\nyou frequently program and run computations on the UL HPC platforma \ni.e\n on a remote Linux/Unix computer, typically working in six different terminal logins to the access server from your office workstation, cranking up long-running computations that are still not finished and are outputting important information (calculation status or results), when you have not 2 interactive jobs running... But it's time to catch the bus and/or the train to go back home. \n\n\n\n\nProbably what you do in the above scenario is to \n\n\na. clear and shutdown all running terminal sessions\nb. once at home when the kids are in bed, you're logging in again... And have to set up the whole environment again (six logins, 2 interactive jobs etc. )\nc. repeat the following morning when you come back to the office. \n\n\nEnter the long-existing and very simple, but totally indispensable \nGNU screen\n command. It has the ability to completely detach running processes from one terminal and reattach it intact (later) from a different terminal login. \n\n\nPre-requisite: screen configuration file \n~/.screenrc\n\n\nWhile not mandatory, we advise you to rely on our customized configuration file for screen \n.screenrc\n available on \nGithub\n. \n\nNormally, you have nothing to do since we already setup this file for you in your homedir.\nOtherwise, simply clone the \nULHPC dotfile repository\n and make a symbolic link \n~/.screenrc\n targeting the file \nscreen/screenrc\n of the repository.\n\n\nBasic commands\n\n\nYou can start a screen session (\ni.e.\n creates a single window with a shell in it) with the \nscreen\n command.\nIts main command-lines options are listed below: \n\n\n\n\nscreen\n: start a new screen \n\n\nscreen -ls\n: does not start screen, but prints a list of \npid.tty.host\n strings identifying your current screen sessions. \n\n\nscreen -r\n: resumes a detached screen session\n\n\nscreen -x\n: attach to a not detached screen session. (Multi display mode \ni.e.\n when you and another user are trying to access the same session at the same time)\n\n\n\n\nOnce within a screen, you can invoke a screen command which consist of a \"\nCTRL + a\n\" sequence followed by one other character. The main commands are: \n\n\n\n\nCTRL + a c\n: (create) creates a new Screen window. The default Screen number is zero.\n\n\nCTRL + a n\n: (next) switches to the next window.\n\n\nCTRL + a p\n: (prev) switches to the previous window.\n\n\nCTRL + a d\n: (detach) detaches from a Screen\n\n\nCTRL + a A\n: (title) rename the current window\n\n\nCTRL + a 0-9\n: switches between windows 0 through 9.\n\n\nCTRL + a k\n or \nCTRL + d\n: (kill) destroy the current window\n\n\nCTRL + a ?\n: (help) display a list of all the command options available for Screen.\n\n\n\n\nSample Usage on the UL HPC platform: Kernel compilation\n\n\nWe will illustrate the usage of GNU screen by performing a compilation of a recent linux kernel.\n\n\n\n\n\n\nstart a new screen session\n\n\n(access)$\n screen\n\n\n\n\n\n\n\nrename the screen window \"Frontend\" (using \nCTRL+a A\n)\n\n\n\n\n\n\ncreate the directory to host the files\n\n\n(access)$\n mkdir -p $WORK/PS1/src\n(access)$\n cd $WORK/PS1/src\n\n\n\n\n\n\n\ncreate a new window and rename it \"Compile\"\n\n\n\n\n\n\nwithin this new window, start a new interactive job over 1 nodes for 6 hours\n\n\n(access)$\n oarsub -I -l nodes=1,walltime=6\n\n\n\n\n\n\n\ndetach from this screen (using \nCTRL+a d\n)\n\n\n\n\nkill your current SSH connection and your terminal\n\n\nre-open your terminal and connect back to the cluster frontend \n\n\n\n\nlist your running screens: \n\n\n(access)$\n screen -ls\nThere is a screen on:\n    9143.pts-0.access   (05/04/2014 11:29:43 PM) (Detached)\n1 Socket in /var/run/screen/S-svarrette.\n\n\n\n\n\n\n\nre-attach your previous screen session\n\n\n(access)$\n screen -r      # OR screen -r 9143.pts-0.access (see above socket name)\n\n\n\n\n\n\n\nin the \"Compile\" windows, go to the working directory and download the Linux kernel sources\n\n\n(node)$\n cd $WORK/PS1/src\n(node)$\n wget -q -c http://www.kernel.org/pub/linux/kernel/v3.x/linux-3.13.6.tar.gz\n\n\n\n\n\n\n\nIMPORTANT\n to ovoid overloading the \nshared\n file system with the many small files involves in the kernel compilation (\ni.e.\n NFS and/or Lustre), we will perform the compilation in the \nlocal\n file system, \ni.e.\n either in \n/tmp\n or (probably more efficient) in \n/dev/shm\n (\ni.e\n in the RAM):\n\n\n    (node)$\n mkdir /dev/shm/PS1\n    (node)$\n cd /dev/shm/PS1\n    (node)$\n tar xzf $WORK/PS1/src/linux-3.13.6.tar.gz\n    (node)$\n cd linux-3.13.6\n    (node)$\n make mrproper\n    (node)$\n make alldefconfig\n    (node)$\n make 2\n1 | tee /dev/shm/PS1/kernel_compile.log\n\n\n\n\n\nYou can now detach from the screen and take a coffee\n\n\n\n\nThe last compilation command make use of \ntee\n, a nice tool which read from standard input and write to standard output \nand\n files. This permits to save in a log file the message written in the standard output. \n\n\nQuestion: why using the \nmake 2\n1\n sequence in the last command?\n \n\n\nQuestion: why working in \n/dev/shm\n is more efficient?\n\n\n\n\nReattach from time to time to your screen to see the status of the compilation\n\n\n\n\nYour compilation is successful if it ends with the sequence: \n\n\n[...]\nKernel: arch/x86/boot/bzImage is ready  (#2)\n\n\n\n\n\n\n\nRestart the compilation, this time using parallel jobs within the Makefile invocation (\n-j\n option of make)\n\n\n(node)$\n make clean\n(node)$\n time make -j `cat $OAR_NODEFILE|wc -l` 2\n1 | tee /dev/shm/PS1/kernel_compile.2.log\n\n\n\n\n\n\n\nThe table below should convince you to always run \nmake\n with the \n-j\n option whenever you can...\n\n\n\n\n\n\n\n\nContext\n\n\ntime (\nmake\n)\n\n\ntime (\nmake -j 16\n)\n\n\n\n\n\n\n\n\n\n\nCompilation in \n/tmp\n(HDD / chaos)\n\n\n4m6.656s\n\n\n0m22.981s\n\n\n\n\n\n\nCompilation in \n/tmp\n(SSD / gaia)\n\n\n3m52.895s\n\n\n0m17.508s\n\n\n\n\n\n\nCompilation in \n/dev/shm\n (RAM)\n\n\n3m11.649s\n\n\n0m17.990s\n\n\n\n\n\n\n\n\n\n\nUse the \nGanglia\n interface on the node you monitor the impact of the compilation process on the node", 
            "title": "Getting Started"
        }, 
        {
            "location": "/basic/getting_started/README/#ul-hpc-tutorial-getting-started", 
            "text": "This tutorial will guide you through your first steps on the UL HPC platform .    Before proceeding:    make sure you have an account (if not, follow  this procedure ), and an SSH client.  take a look at the  quickstart guide  ensure you operate from a Linux / Mac environement. Most commands below assumes running in a Terminal in this context. If you're running Windows, you can use Putty tools etc. as described  on this page  yet it's probably better that you familiarize \"natively\" with Linux-based environment by having a Linux Virtual Machine (consider for that  VirtualBox ).    From a general perspective, the  Support page  describes how to get help during your UL HPC usage.", 
            "title": "UL HPC Tutorial: Getting Started"
        }, 
        {
            "location": "/basic/getting_started/README/#convention", 
            "text": "In the below tutorial, you'll proposed terminal commands where the prompt is denoted by  $ .   In general, we will prefix to precise the execution context ( i.e.  your laptop, a cluster frontend or a node). Remember that  #  character is a comment. Example:       # This is a comment \n    $  hostname\n\n    (laptop)$  hostname         # executed from your personal laptop / workstation\n\n    (access-gaia)$  hostname    # executed from access server of the Gaia cluster", 
            "title": "Convention"
        }, 
        {
            "location": "/basic/getting_started/README/#platform-overview", 
            "text": "You can find a brief overview of the platform with key characterization numbers  on this page .  The general organization of each cluster is depicted below:   Details on this organization can be found  here", 
            "title": "Platform overview."
        }, 
        {
            "location": "/basic/getting_started/README/#connecting-for-the-first-time-and-preparing-your-ssh-environment", 
            "text": "Access / SSH Tutorial   The way SSH handles the keys and the configuration files is illustrated in the following figure:   In order to be able to login to the clusters, you have sent us through the Account request form the  public key  (i.e.  id_dsa.pub ,  id_rsa.pub  or the  public key  as saved by PuttY) you initially generated, enabling us to configure the  ~/.ssh/authorized_keys  file of your account.    Step 1a: Connect to UL HPC (Linux / Mac OS / Unix)  Run the following commands in a terminal (substituting  yourlogin  with the login name you received from us):      (laptop)$  ssh -p 8022 yourlogin@access-chaos.uni.lu  If you want to connect to the gaia cluster,       (laptop)$  ssh -p 8022 yourlogin@access-gaia.uni.lu  Now you probably want to avoid taping this long command to connect to the platform. You can customize SSH aliases for that. Edit the file  ~/.ssh/config  (create it if it does not already exist) and adding the following entries:       Host chaos-cluster\n        Hostname access-chaos.uni.lu\n\n    Host gaia-cluster\n        Hostname access-gaia.uni.lu\n\n    Host *-cluster\n        User yourlogin\n        Port 8022\n        ForwardAgent no  Now you shall be able to issue the following (simpler) command to connect to the cluster and obtain the welcome banner:       (laptop)$  ssh gaia-cluster\n\n    (laptop)$  ssh chaos-cluster  In the sequel, we assume these aliases to be defined.   Step 1b: Connect to UL HPC (Windows)   Download  all the Putty tools  extract them in an easy-to-find place, such as  C:\\Putty  load your private key under Pageant  open  Putty.exe  (connection type:  SSH )  In  Category:Session :  Host Name:  access-{chaos,gaia}.uni.lu  Port: 8022  Saved session:  {Chaos,Gaia}    In  Category:Connection:Data  :  Auto-login username:  yourlogin    Go back to  Category:Session  and click on \"Save\"  Click on \"Open\"", 
            "title": "Connecting for the first time and preparing your SSH environment"
        }, 
        {
            "location": "/basic/getting_started/README/#step-2-configure-your-ssh-environment-on-all-clusters", 
            "text": "The SSH key you provided us secure your connection  from  your laptop (or personal workstation)  to  the cluster frontends. It is thus important to protect them by a passphrase.   You shall have also a new key pair configured in your account to permit a bi-directional transparent connection from one cluster to the other (you can check that in your  ~/.ssh/authorized_keys  and by successfully running:       (access-gaia)$  ssh chaos-cluster  or       (access-chaos)$  ssh gaia-cluster  If that's the case, you can ignore the rest of this section.  Otherwise , you will now have to configure a passphrase-free SSH key pair to permit a transparent connection from one cluster to another.     Connect to the  chaos  cluster:   (laptop)$  ssh chaos-cluster    generate a new SSH key pair with  ssh-keygen  (leave the passphrase empty):  (access-chaos)$  ssh-keygen -t dsa\nGenerating public/private dsa key pair.\nEnter file in which to save the key (/home/users/yourlogin/.ssh/id_dsa): \nEnter passphrase (empty for no passphrase): \nYour identification has been saved in /home/users/yourlogin/.ssh/id_dsa.\nYour public key has been saved in /home/users/yourlogin/.ssh/id_dsa.pub.\nThe key fingerprint is:\n1f:1d:a5:66:3b:4a:68:bc:7d:8c:7f:33:c9:77:0d:4a yourlogin@access.chaos-cluster.uni.lux\nThe key's randomart image is:\n`-[ DSA 1024]---`\n|              .  |\n|             o   |\n|            =    |\n|       . . + o   |\n|        S o +    |\n|       . = =E..  |\n|        . =.oo o.|\n|           o. * +|\n|            .. +.|\n`---------------`    authorize the newly generated public key to be used during challenge/response authentication:  (access-chaos)$  cat ~/.ssh/id_dsa.pub \nssh-dss AAAAB[...]B2== yourlogin@access.chaos-cluster.uni.lux\n(access-chaos)$  cat ~/.ssh/id_dsa.pub    ~/.ssh/authorized_keys    you can check that it works by connecting to localhost:   ```\n(access-chaos)$  ssh -p 8022 localhost\n[...]\n(access-chaos)$  exit   # or CTRL-D\n```    add an alias to facilitate the connection to each cluster by adding the following SSH configuration entry in the file  ~/.ssh/config :   Host gaia-cluster chaos-cluster\n    User yourlogin\n    Port 8022\n\nHost gaia-cluster\n    Hostname access-gaia.uni.lu\nHost chaos-cluster\n    Hostname access-chaos.uni.lu    You'll have to setup the same key package on the gaia cluster such that you can then work indefferently on one or another cluster. It's also the occasion to learn how to add a new SSH key to your authorized key portfolio.     Open another terminal and connect to the gaia cluster   (laptop)$  ssh gaia-cluster    edit the file  ~/.ssh/authorized_keys  to add your previously generated key on chaos (use  :wq  in  vim  to save and quit):  (access-gaia)$  vim ~/.ssh/authorized_keys    go back to the terminal where you're connected on chaos, you shall now be able to connect to gaia, and reversely:   (access-chaos)$  ssh gaia\n[...]\n(access-gaia)$  exit     # or CRTL-D    You have a different home directory on each UL HPC site, so you will usually use Rsync or scp to move data around (see  transfering files tutorials ).  Now that we are able to connect  from  chaos  to  gaia, we will transfer the SSH keys and configuration in place from chaos and check that we can connnect back:       (access-chaos)$  scp ~/.ssh/id_dsa* gaia:.ssh/\n    (access-chaos)$  scp ~/.ssh/config  gaia:.ssh/\n    (access-chaos)$  ssh gaia\n    [...]\n    (access-gaia)$   ssh chaos\n    (access-chaos)$  exit     # or CRTL-D\n    (access-gaia)$   exit     # or CRTL-D  So now  we have setup a bi-directional transparent connection from one cluster to the other.", 
            "title": "Step 2: configure your SSH environment on all clusters"
        }, 
        {
            "location": "/basic/getting_started/README/#step-2bis-using-ssh-proxycommand-setup-to-access-the-clusters-despite-port-filtering", 
            "text": "It might happen that the port 8022 is filtered from your working place. You can easily bypass this firewall rule using an SSH proxycommand to setup transparently multi-hop connexions  through  one host (a gateway) to get to the access frontend of the cluster, as depited below:    [laptop] -----||--------  22 [SSH gateway] ---------  8022 [access-{chaos,gaia}]\n            firewall  The gateway can be any SSH server which have access to the access frontend of the cluster. The  Gforge @ UL  is typically used in this context but you can prefer any other alternative (your personal NAS @ home etc.). Then alter the SSH config on yout laptop (in  ~/.ssh/confg  typically) as follows:    create an entry to be able to connect to the gateway:  # Alias for the gateway (not really needed, but convenient), below instanciated \nHost gw\n    User anotherlogin\n    Hostname host.domain.org\n    ForwardAgent no\n\n# Automatic connection to UL HPC from the outside via the gateway\nHost *.ulhpc\n    ProxyCommand ssh gw \"nc -q 0 `basename %h .ulhpc` %p\"    ensure you can connect to the gateway:  (laptop)$  ssh gw\n(gateway)$  exit # or CTRL-D    the  .ulhpc  suffix we mentionned in the previous configuration is an arbitrary suffix you will now specify in your command lines in order to access the UL HPC platform via the gateway as follows:   (laptop)$  ssh gaia.ulhpc", 
            "title": "Step 2bis: Using SSH proxycommand setup to access the clusters despite port filtering"
        }, 
        {
            "location": "/basic/getting_started/README/#step-3-transferring-files", 
            "text": "Directories such as  $HOME ,  $WORK  or  $SCRATCH  are shared among the nodes of the cluster that you are using (including the front-end) via shared filesystems (NFS, Lustre) meaning that:   every file/directory pushed or created on the front-end is available on the computing nodes  every file/directory pushed or created on the computing nodes is available on the front-end   Step 3a: Linux / OS X / Unix command line tools  The two most common tools you can use for data transfers over SSH:   scp : for the full transfer of files and directories (only works fine for single files or directories of small/trivial size)  rsync : a software application which synchronizes files and directories from one location to another while minimizing data transfer as only the outdated or inexistent elements are transfered (practically required for lengthy complex transfers, which are more likely to be interrupted in the middle).   Of both, normally the second approach should be preferred, as more generic; note that, both ensure a secure transfer of the data, within an encrypted tunnel.    Create a new directory on your local machine and download a file to transfer (next-gen sequencing data from the NIH Roadmap Epigenomics Project):  (laptop)$  mkdir file_transfer\n(laptop)$  cd file_transfer\n(laptop)$  wget \"ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM409nnn/GSM409307/suppl/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\"    Transfer the file with scp:  (laptop)$  scp GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz gaia-cluster:    Connect to the cluster, check if the file is there and delete it.  (laptop)$  ssh gaia-cluster\n(access-gaia)$  ls\n(access-gaia)$  rm GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\n(access-gaia)$  exit    Transfer the directory with rsync:  (laptop)$  cd ..\n(laptop)$  rsync -avzu file_transfer gaia-cluster:    Delete the file and retrieve it from the cluster:  (laptop)$  rm file_transfer/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz\n(laptop)$  rsync -avzu gaia-cluster:file_transfer .    Bonus : Check where the file is located on the cluster after the rsync.    You can get more information about these transfer methods in the  file transfer documentation .", 
            "title": "Step 3: transferring files"
        }, 
        {
            "location": "/basic/getting_started/README/#step-3b-windows-linux-os-x-unix-gui-tools", 
            "text": "Download the FileZilla client application from  filezilla-project.org  and install it.  First we need to tell FileZilla about our ssh key:  Start the application.  Go to the  Settings  (either under  Edit  or  FileZilla  depending on the OS).  In the category  Connection  select  SFTP .   Click on the button  Add keyfile...  and select your private keyfile (you may need to convert it).  Finally click  OK  to save and close the settings.       Back in the main window click on the  Site Manager  button on the top left or select  Site Manager  from the  File  menu.  Click on the  New Site  button and enter/select the following:  Host:  access-gaia.uni.lu  Protocol:  SFTP - SSH File Transfer Protocol  Logon Type:  Interactive  User: your login     Click on the  Connect  button.  Accept the certificate.   You should now see something similar to the following window:   On the very top, beneath the quick connect, you see the message log. Below you have the directory tree and the contents of the current directory for you local computer on the left and the remote location on the right.  To transfer a file, simply drag and drop it from the directory listing on the left side to destination directory on the right (to transfer from local to remote) or vice versa (to transfer from remote to local). You can also select a file by left clicking on it once and then right click on it to get the context menu and select \"Upload\" or \"Download\" to transfer it.  If you skipped step 3a, you may download the following file (50 MB) for testing:   ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM409nnn/GSM409307/suppl/GSM409307_UCSD.H1.H3K4me1.LL228.bed.gz  (next-gen sequencing data from the NIH Roadmap Epigenomics Project)  When you click the fifth icon on the top with the two green arrows to toogle the transfer queue, you can see the status of ongoing transfers on the very bottom of the window.", 
            "title": "Step 3b: Windows / Linux / OS X / Unix GUI tools"
        }, 
        {
            "location": "/basic/getting_started/README/#discovering-visualizing-and-reserving-ul-hpc-resources", 
            "text": "In the sequel, replace  login  in the proposed commands with you login on the platform (ex:  svarrette ).", 
            "title": "Discovering, visualizing and reserving UL HPC resources"
        }, 
        {
            "location": "/basic/getting_started/README/#step-1-the-working-environment", 
            "text": "reference documentation   After a successful login onto one of the access node (see  Cluster Access ), you end into your personal homedir  $HOME  which is shared over NFS between the access node and the computing nodes.  Again, remember that your homedir is placed on  separate  NFS servers on each site, which  ARE NOT SYNCHRONIZED : data synchronization between each of them remain at your own responsability. We will see below that the UL HPC team prepared for you a script to facilitate the transfer of data between each site.   Otherwise, you have to be aware of at least two directories:    $HOME : your home directory under NFS.   $SCRATCH : a non-backed up area put if possible under Lustre for fast I/O operations   Your homedir is under a regular backup policy. Therefore you are asked to pay attention to your disk usage  and  the number of files you store there.     estimate file space usage and summarize disk usage of each FILE, recursively for directories using the  ncdu  command:  (access)$  ncdu    you shall also pay attention to the number of files in your homedirectory. You can count them as follows:   (access)$  find . -type f | wc -l", 
            "title": "Step 1: the working environment"
        }, 
        {
            "location": "/basic/getting_started/README/#step-2-web-monitoring-interfaces", 
            "text": "Each cluster offers a set of web services to monitore the platform usage:    A  pie-chart overview of the platform usage  Monika , the visualization interface of the OAR scheduler, which  display the status of the clusters as regards the jobs running on the platform.  DrawGantt , the Gantt visualization of jobs scheduled on OAR  Ganglia , a scalable distributed monitoring system for high-performance computing systems such as clusters and Grids.  CDash  (internal UL network use)", 
            "title": "Step 2: web monitoring interfaces"
        }, 
        {
            "location": "/basic/getting_started/README/#step-3-reserving-resources-with-oar-the-basics", 
            "text": "reference documentation   OAR  is an open-source batch scheduler which provides simple yet flexible facilities for the exploitation of the UL HPC clusters.   it permits to schedule jobs for users on the cluster resource  a  OAR resource  corresponds to a node or part of it (CPU/core)  a  OAR job  is characterized by an execution time (walltime) on a set of resources. \n  There exists two types of jobs:   interactive : you get a shell on the first reserve node  passive : classical batch job where the script passed as argument to  oarsub  is executed  on the first reserved node     We will now see the basic commands of OAR.    Connect to one of the UL HPC  frontend. You can request resources in interactive mode: (access)$  oarsub -I    Notice that with no parameters, oarsub gave you one resource (one core) for two hour. You were also directly connected to the node you reserved with an interactive shell.\n  No exit the reservation:       (node)$  exit      # or CTRL-D  When you run exit, you are disconnected and your reservation is terminated.   To avoid anticipated termination of your jobs in case or errors (terminal closed by mistake), \nyou can reserve and connect in 2 steps using the job id associated to your reservation.    First run a passive job  i.e.  run a predefined command -- here  sleep 10d  to delay the execution for 10 days -- on the first reserved node: (access)$  oarsub \"sleep 10d\"\n[ADMISSION RULE] Set default walltime to 7200.\n[ADMISSION RULE] Modify resource description with type constraints\nOAR_JOB_ID=919309    You noticed that you received a job ID (in the above example:  919309 ), which you can later use to connect to the reserved resource(s):      (access)$  oarsub -C 919309        # adapt the job ID accordingly ;)\n    Connect to OAR job 919309 via the node e-cluster1-13\n    [OAR] OAR_JOB_ID=919309\n    [OAR] Your nodes are:\n        e-cluster1-13*1\n\n    (e-cluster1-13)$  java -version\n    (e-cluster1-13)$  hostname -f\n    (e-cluster1-13)$  whoami\n    (e-cluster1-13)$  env | grep OAR   # discover environment variables set by OAR\n    (e-cluster1-13)$  exit             # or CTRL-D  Question: At which moment the job  919309  will end?    a. after 10 days\nb. after 2 hours\nc. never, only when I'll delete the job    Question: manipulate the  $OAR_NODEFILE  variable over the command-line to extract the following information, once connected to your job  a. the list of hostnames where a core is reserved (one per line) \n   *  hint :  man cat \nb. number of reserved cores (one per line)\n   *  hint :  man wc  --  use  wc -l  over the pipe  |  command\nc. number of reserved nodes (one per line)\n   *  hint :  man uniq  -- use  uniq  over the pipe  |  command\nd. number of cores reserved per node together with the node name (one per line)\n   * Example of output:           12 gaia-11\n        12 gaia-15   hint :  man uniq  -- use  uniq -c  over the pipe  |  command\ne.  (for geeks)  output the number of reserved nodes times number of cores per node   Example of output:      gaia-11*12\n    gaia-15*12    hint :  man awk  -- use  printf  command of  awk  over the pipe command, for instance  awk '{ printf \"%s*%d\\n\",$2,$1 }' . You might prefer  sed  or any other advanced geek command.", 
            "title": "Step 3: Reserving resources with OAR: the basics"
        }, 
        {
            "location": "/basic/getting_started/README/#step-4-job-management", 
            "text": "Normally, the previously run job is still running.   You can check the status of your running jobs using  oarstat  command: (access)$  oarstat      # access all jobs \n(access)$  oarstat -u   # access all your jobs    Then you can delete your job by running  oardel  command:      (access)$  oardel 919309   you can see your consumption (in an historical computational measure named  CPU hour  i.e. the work done by a CPU in one hour of wall clock time) over a given time period using  oarstat --accounting \"YYYY-MM-DD, YYYY-MM-DD\" -u  youlogin : (access)$  oarstat --accounting \"2013-01-01, 2013-12-31\" -u  login     In particular, take a look at the difference between the  asked  resources and the  used  ones  In all remaining examples of reservation in this section, remember to delete the reserved jobs afterwards (using  oardel  or  CTRL-D )  You probably want to use more than one core, and you might want them for a different duration than two hours. \nThe  -l  switch allows you to pass a comma-separated list of parameters specifying the needed resources for the job.    Reserve interactively 4 cores for 6 hours (delete the job afterwards)   (access)$  oarsub -I -l core=6,walltime=6    Reserve interactively 2 nodes for 3h15 (delete the job afterwards):   (access)$  oarsub -I -l nodes=3,walltime=3:15    Hierarchical filtering of resources  OAR features a very powerful resource filtering/matching engine able to specify resources in a  hierarchical   way using the  /  delimiter. The resource property hierarchy is as follows:       enclosure -  nodes -  cpu -  core    Reserve interactively 2 cores on 3 different nodes belonging to the same enclosure ( total: 6 cores ) for 3h15:  (access)$  oarsub -I -l /enclosure=1/nodes=3/core=2,walltime=3:15    Reserve interactively two full nodes belonging to the different enclosure for 6 hours:   (access)$  oarsub -I -l /enclosure=2/nodes=1,walltime=6    Question: reserve interactively 2 cpus on 2 nodes belonging to the same enclosure for 4 hours     Question: in the following statements, explain the advantage and drawback (in terms of latency/bandwidth etc.) of each of the proposed approaches  a.  oarsub -I -l /nodes=2/cpu=1  vs  oarsub -I -l cpu=2  vs  oarsub -I -l /nodes=1/cpu=2 \nb.  oarsub -I -l /enclosure=1/nodes=2  vs  oarsub -I -l nodes=2  vs  oarsub -I -l /enclosure=2/nodes=1  Using OAR properties  You might have notice on  Monika  for each site a list of properties assigned to each resources.   The  -p  switch allows you to specialize (as an SQL syntax) the property you wish to use when selecting the resources. The syntax is as follows:  oarsub -p \"  property  ='  value  '\"  You can find the available OAR properties on the  UL HPC documentation . The main ones are described below     Property  Description  Example      host  Full hostname of the resource  -p \"host='h-cluster1-14.chaos-cluster.uni.lux'\"    network_address  Short hostname of the resource  -p \"network_address='h-cluster1-14'\"    gpu  GPU availability (gaia only)  -p \"gpu='YES'\"    nodeclass  Node class (chaos only) 'h','d','e','s'  -p \"nodeclass='h'\"       reserve interactively 4 cores on a GPU node for 8 hours ( this holds only on the  gaia  cluster ) ( total: 4 cores )  (access-gaia)$  oarsub -I -l nodes=1/core=4,walltime=8 -p \"gpu=\u2019YES\u2019\"    reserve interactively 4 cores on the GPU node  gaia-65  for 8 hours ( this holds only on the  gaia  cluster ) ( total: 4 cores )  (access-gaia)$  oarsub -I -l nodes=1/core=4,walltime=8 -p \"gpu='yes'\" -p \"network_address='gaia-65'\"    Question: reserve interactively 2 nodes among the  h-cluster1-*  nodes ( this holds only on the  chaos  cluster ) using the  nodeclass  property     You can combine filters using the  +  sign.   reserve interactively 4 cores, each on 2 GPU nodes and 20 cores on any other nodes ( total: 28 cores ) (access-gaia)$  oarsub -I -l \"{gpu='YES'}/nodes=2/core=4+{gpu='NO'}/core=20\"\n[ADMISSION RULE] Set default walltime to 7200.\n[ADMISSION RULE] Modify resource description with type and ibpool constraints\nOAR_JOB_ID=2828104\nInteractive mode : waiting...\nStarting...\n\nConnect to OAR job 2828104 via the node gaia-11\n[OAR] OAR_JOB_ID=2828104\n[OAR] Your nodes are:\n    gaia-11*12\n    gaia-12*4\n    gaia-59*4\n    gaia-63*4\n    gaia-65*4    Reserving specific resources  bigsmp and  bigmem  Some nodes are very specific (for instance the nodes with 1TB of memory or the BCS subsystem of Gaia composed of 4 motherboards of 4 processors with a total of 160 cores aggregated in a ccNUMA architecture).  Due to this specificity, they are NOT scheduled by default   and can only be reserved with an explicit oarsub parameter:  -t bigmem  for  -t bigsmp   reserve interactively 2 cpu on the bigsmp node belonging to the same board for 3 hours: ( total: 32 cores ) (access-gaia)$  oarsub -t bigsmp -I -l /board=1/cpu=2,walltime=3    Question: why are these resources not scheduled by default?     OAR Containers  With OAR, it is possible to execute jobs within another one. This functionality is called  container jobs  and is invoked using the  -t container  switch.   create a container job of 2 nodes for 4h30:  (access)$  oarsub -t container -l nodes=2,walltime=4:30:00 \"sleep 1d\" \n[ADMISSION RULE] Modify resource description with type and ibpool constraints\nOAR_JOB_ID=2828112\n(access)$  oarstat -u    This creates a kind of \"tunnel\" inside witch you can push subjobs using the  -t inner= container_job_id .   reserve 3 sleep jobs (of different delay) over 10 cores within the previously created container job (access)$  oarsub -t inner=2828112 -l core=10 \"sleep 3m\"   # Job 1 \n(access)$  oarsub -t inner=2828112 -l core=10 \"sleep 2m\"   # Job 2\n(access)$  oarsub -t inner=2828112 -l core=10 \"sleep 1m\"   # Job 3    These jobs will be scheduled as follows            ^ \n          |                        \n          +======================== ... ========================+\n          |                      ^       Container Job (4h30)   |\n       ^  |  +--------+----+     |                              |\n    20c|  |  |    J2  | J3 |     |24c                           | \n       |  |  +--------+----+     |                              |\n       v  |  |      J1     |     v                              | \n          +==+=============+========= ... ======================+\n          |    ------ --- \n          |     2min    1min\n          +--------------------------------------------------------------  time  Question: Check the way your jobs have been scheduled  a. using  oarstat -u -f -j  subjob_id  (take a look at the  assigned_resources )\nb. using the  OAR drawgantt  interface    Question: explain the interest of container jobs for the platform managers  Reservation at a given period of time  You can use the  -r \"YYYY-MM-DD HH:MM:SS\"  option of  oarsub  to specify the date you wish the reservation to be issued. This is of particular interest for you to book in advance resources out of the working hours (at night and/or over week ends)", 
            "title": "Step 4: Job management"
        }, 
        {
            "location": "/basic/getting_started/README/#step-5-using-modules", 
            "text": "Environment Modules  is a software package that allows us to provide a  multitude of applications and libraries in multiple versions  on the UL HPC platform. The tool itself is used to manage environment variables such as  PATH ,  LD_LIBRARY_PATH  and  MANPATH , enabling the easy loading and unloading of application/library profiles and their dependencies.  We will have multiple occasion to use modules in the other tutorials so there is nothing special we foresee here. You are just encouraged to read the following resources:    Introduction to Environment Modules by Wolfgang Baumann  Modules tutorial @ NERSC  UL HPC documentation on modules", 
            "title": "Step 5: Using modules"
        }, 
        {
            "location": "/basic/getting_started/README/#step-6-advanced-job-management-and-persistent-terminal-sessions-using-gnu-screen", 
            "text": "GNU Screen  is a tool to manage persistent terminal sessions. \nIt becomes interesting since you will probably end at some moment with the following  scenario:   you frequently program and run computations on the UL HPC platforma  i.e  on a remote Linux/Unix computer, typically working in six different terminal logins to the access server from your office workstation, cranking up long-running computations that are still not finished and are outputting important information (calculation status or results), when you have not 2 interactive jobs running... But it's time to catch the bus and/or the train to go back home.    Probably what you do in the above scenario is to   a. clear and shutdown all running terminal sessions\nb. once at home when the kids are in bed, you're logging in again... And have to set up the whole environment again (six logins, 2 interactive jobs etc. )\nc. repeat the following morning when you come back to the office.   Enter the long-existing and very simple, but totally indispensable  GNU screen  command. It has the ability to completely detach running processes from one terminal and reattach it intact (later) from a different terminal login.   Pre-requisite: screen configuration file  ~/.screenrc  While not mandatory, we advise you to rely on our customized configuration file for screen  .screenrc  available on  Github .  \nNormally, you have nothing to do since we already setup this file for you in your homedir.\nOtherwise, simply clone the  ULHPC dotfile repository  and make a symbolic link  ~/.screenrc  targeting the file  screen/screenrc  of the repository.  Basic commands  You can start a screen session ( i.e.  creates a single window with a shell in it) with the  screen  command.\nIts main command-lines options are listed below:    screen : start a new screen   screen -ls : does not start screen, but prints a list of  pid.tty.host  strings identifying your current screen sessions.   screen -r : resumes a detached screen session  screen -x : attach to a not detached screen session. (Multi display mode  i.e.  when you and another user are trying to access the same session at the same time)   Once within a screen, you can invoke a screen command which consist of a \" CTRL + a \" sequence followed by one other character. The main commands are:    CTRL + a c : (create) creates a new Screen window. The default Screen number is zero.  CTRL + a n : (next) switches to the next window.  CTRL + a p : (prev) switches to the previous window.  CTRL + a d : (detach) detaches from a Screen  CTRL + a A : (title) rename the current window  CTRL + a 0-9 : switches between windows 0 through 9.  CTRL + a k  or  CTRL + d : (kill) destroy the current window  CTRL + a ? : (help) display a list of all the command options available for Screen.   Sample Usage on the UL HPC platform: Kernel compilation  We will illustrate the usage of GNU screen by performing a compilation of a recent linux kernel.    start a new screen session  (access)$  screen    rename the screen window \"Frontend\" (using  CTRL+a A )    create the directory to host the files  (access)$  mkdir -p $WORK/PS1/src\n(access)$  cd $WORK/PS1/src    create a new window and rename it \"Compile\"    within this new window, start a new interactive job over 1 nodes for 6 hours  (access)$  oarsub -I -l nodes=1,walltime=6    detach from this screen (using  CTRL+a d )   kill your current SSH connection and your terminal  re-open your terminal and connect back to the cluster frontend    list your running screens:   (access)$  screen -ls\nThere is a screen on:\n    9143.pts-0.access   (05/04/2014 11:29:43 PM) (Detached)\n1 Socket in /var/run/screen/S-svarrette.    re-attach your previous screen session  (access)$  screen -r      # OR screen -r 9143.pts-0.access (see above socket name)    in the \"Compile\" windows, go to the working directory and download the Linux kernel sources  (node)$  cd $WORK/PS1/src\n(node)$  wget -q -c http://www.kernel.org/pub/linux/kernel/v3.x/linux-3.13.6.tar.gz    IMPORTANT  to ovoid overloading the  shared  file system with the many small files involves in the kernel compilation ( i.e.  NFS and/or Lustre), we will perform the compilation in the  local  file system,  i.e.  either in  /tmp  or (probably more efficient) in  /dev/shm  ( i.e  in the RAM):      (node)$  mkdir /dev/shm/PS1\n    (node)$  cd /dev/shm/PS1\n    (node)$  tar xzf $WORK/PS1/src/linux-3.13.6.tar.gz\n    (node)$  cd linux-3.13.6\n    (node)$  make mrproper\n    (node)$  make alldefconfig\n    (node)$  make 2 1 | tee /dev/shm/PS1/kernel_compile.log   You can now detach from the screen and take a coffee   The last compilation command make use of  tee , a nice tool which read from standard input and write to standard output  and  files. This permits to save in a log file the message written in the standard output.   Question: why using the  make 2 1  sequence in the last command?    Question: why working in  /dev/shm  is more efficient?   Reattach from time to time to your screen to see the status of the compilation   Your compilation is successful if it ends with the sequence:   [...]\nKernel: arch/x86/boot/bzImage is ready  (#2)    Restart the compilation, this time using parallel jobs within the Makefile invocation ( -j  option of make)  (node)$  make clean\n(node)$  time make -j `cat $OAR_NODEFILE|wc -l` 2 1 | tee /dev/shm/PS1/kernel_compile.2.log    The table below should convince you to always run  make  with the  -j  option whenever you can...     Context  time ( make )  time ( make -j 16 )      Compilation in  /tmp (HDD / chaos)  4m6.656s  0m22.981s    Compilation in  /tmp (SSD / gaia)  3m52.895s  0m17.508s    Compilation in  /dev/shm  (RAM)  3m11.649s  0m17.990s      Use the  Ganglia  interface on the node you monitor the impact of the compilation process on the node", 
            "title": "Step 6 (advanced): Job management and Persistent Terminal Sessions using GNU Screen"
        }, 
        {
            "location": "/basic/sequential_jobs/README/", 
            "text": "HPC workflow with sequential jobs\n\n\nPrerequisites\n\n\nMake sure you have followed the tutorial \"Getting started\".\n\n\nIntro\n\n\nFor many users, the typical usage of the HPC facilities is to execute 1 program with many parameters.\nOn your local machine, you can just start your program 100 times sequentially.\nHowever, you will obtain better results if you parallelize the executions on a HPC Cluster.\n\n\nDuring this session, we will see 3 use cases:\n\n\n\n\nExercise 1: Use the serial launcher (1 node, in sequential and parallel mode);\n\n\nExercise 2: Use the generic launcher, distribute your executions on several nodes (python script);\n\n\nExercise 3: Advanced use case, using a Java program: \"JCell\".\n\n\n\n\nWe will use the following github repositories:\n\n\n\n\nULHPC/launcher-scripts\n\n\nULHPC/tutorials\n\n\n\n\nPractical session 2\n\n\nConnect the the cluster access node, and set-up the environment for this tutorial\n\n\n(yourmachine)$\n ssh chaos-cluster\n\n\n\nIf your network connection is unstable, use \nscreen\n:\n\n\n(access)$\n screen\n\n\n\nWe will use 2 directory:\n\n\n\n\n$HOME\n: default home directory, \nbacked up\n, maximum 50GB, for important files\n\n\n$WORK\n: work directory, \nnon backed up\n, maximum 500GB\n\n\n\n\nCreate a sub directory $WORK/PS2, and work inside it\n\n\n(access)$\n mkdir $WORK/PS2\n(access)$\n cd $WORK/PS2\n\n\n\nIn the following parts, we will assume that you are working in this directory.\n\n\nClone the repositories \nULHPC/tutorials\n and \nULHPC/launcher-scripts.git\n\n\n(access)$\n git clone https://github.com/ULHPC/launcher-scripts.git\n(access)$\n git clone https://github.com/ULHPC/tutorials.git\n\n\n\nIn order to edit files in your terminal, you are expected to use your preferred text editor:\n\n\n\n\nnano\n\n\nvim\n\n\nemacs\n\n\n...\n\n\n\n\nIf you have never used any of them, \nnano\n is intuitive, but vim and emacs are more powerful.\n\n\nExercise 1: Parametric experiment with Gromacs\n\n\nGromacs is a popular molecular dynamics software.\nIn this exercise, we will process some example input files, and make the parameter \nfourier_spacing\n varies from 0.1 to 0.2 in increments of 0.005.\n\n\nCreate a file which contains the list of parameters:\n\n\n(access)$\n seq 0.1 0.005 0.2 \n $WORK/PS2/param_file\n\n\n\nStep 1: Naive workflow\n\n\nWe will use the launcher \nNAIVE_AKA_BAD_launcher_serial.sh\n (full path: \n$WORK/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n).\n\n\nEdit the following variables:\n\n\n\n\nTASK\n must contain the path of the executable, \n\n\nARG_TASK_FILE\n must contain the path of your parameter file.\n(node)$\n nano $WORK/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n\nTASK=\"$WORK/PS2/tutorials/basic/sequential_jobs/scripts/run_gromacs_sim.sh\"\nARG_TASK_FILE=$WORK/PS2/param_file\n\n\n\n\n\n\n\nLaunch the job, in interactive mode and execute the launcher:\n\n\n(access)$\n oarsub -I -l core=1\n(node)$ $WORK/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n\n\n\nOr\n in passive mode (the output will be written in a file named \nOAR.\nJOBID\n.stdout\n)\n\n\n(access)$\n oarsub -l core=1 $WORK/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n\n\n\nStep 2: Optimal method using GNU parallel (GNU Parallel)\n\n\nWe will use the launcher \nlauncher_serial.sh\n (full path: \n$WORK/PS2/launcher-scripts/bash/serial/launcher_serial.sh\n).\n\n\nEdit the following variables:\n\n\n(access)$\n nano $WORK/PS2/launcher-scripts/bash/serial/launcher_serial.sh\n\nTASK=\"$WORK/PS2/tutorials/basic/sequential_jobs/scripts/run_gromacs_sim.sh\"\nARG_TASK_FILE=$WORK/PS2/param_file\n\n\n\nSubmit the (passive) job with \noarsub\n\n\n(access)$\n oarsub -l nodes=1 $WORK/PS2/launcher-scripts/bash/serial/launcher_serial.sh\n\n\n\nQuestion\n: compare and explain the execution time with both launchers:\n\n\n\n\nNaive workflow: time = 16m 32s\n\n\nParallel workflow: time = 2m 11s\n\n\n\n\n/!\\ In order to compare execution times, you must always use the same type of nodes (CPU/Memory), \nusing \nproperties\n\nin your \noarsub\n command.\n\n\nExercise 2: Watermarking images in Python\n\n\nWe will use another program, \nwatermark.py\n (full path: \n$WORK/PS2/tutorials/basic/sequential_jobs/scripts/watermark.py\n),\nand we will distribute the computation on 2 nodes with the launcher \nparallel_launcher.sh\n\n(full path: \n$WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n).\n\n\nThis python script will apply a watermark to the images (using the Python Imaging library).\n\n\nThe command works like this:\n\n\npython watermark.py \npath/to/watermark_image\n \nsource_image\n\n\n\n\nWe will work with 2 files:\n\n\n\n\ncopyright.png\n: a transparent images, which can be applied as a watermark\n\n\nimages.tgz\n: a compressed file, containing 30 JPG pictures (of the Gaia Cluster :) ).\n\n\n\n\nStep 1: Prepare the input files\n\n\nCopy the source files in your $WORK directory.\n\n\n(access)\n$ tar xvf /tmp/images.tgz -C $WORK/PS2/\n(access)\n$ cp /tmp/copyright.png $WORK/PS2\n\n(access)\n$ cd $WORK/PS2\n\n\n\nStep 2: Create a list of parameters\n\n\nWe must create a file containing a list of parameters, each line will be passed to \nwatermark.py\n.\n\n\nls -d -1 $WORK/PS2/images/*.JPG | awk -v watermark=$WORK/PS2/copyright.png '{print watermark \" \" $1}' \n $WORK/PS2/generic_launcher_param\n\\_____________________________/   \\_________________________________________________________________/ \\_________________________________/\n               1                                                    2                                                3\n\n\n\n\n\nls -d -1\n: list the images\n\n\nawk ...\n: prefix each line with the first parameter (watermark file)\n\n\n: redirect the output to the file $WORK/generic_launcher_param\n\n\n\n\nStep 3: Configure the launcher\n\n\nWe will use the launcher \nparallel_launcher.sh\n (full path: \n$WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n).\n\n\nEdit the following variables:\n\n\n(access)$\n nano $WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\nTASK=\"$WORK/PS2/tutorials/basic/sequential_jobs/scripts/watermark.py\"\nARG_TASK_FILE=\"$WORK/PS2/generic_launcher_param\"\n# number of cores needed for 1 task\nNB_CORE_PER_TASK=2\n\n\n\nStep 4: Submit the job\n\n\nWe will spawn 1 process / 2 cores\n\n\n(access)$\n oarsub -l nodes=2 $WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\n\n\nStep 5: Download the files\n\n\nOn your laptop, transfer the files in the current directory and look at them with your favorite viewer:\n\n\n(yourmachine)$\n rsync -avz chaos-cluster:/work/users/\nLOGIN\n/PS2/images .\n\n\n\nQuestion\n: which nodes are you using, identify your nodes with the command \noarstat -f -j \nJOBID\n or Monika\n(\nChaos\n, \nGaia\n)\n\n\nExercise 3: Advanced use case, using a Java program: \"JCell\"\n\n\nLet's use \nJCell\n, a framework for working with genetic algorithms, programmed in Java.\n\n\nWe will use 3 scripts:\n\n\n\n\njcell_config_gen.sh\n (full path: \n$WORK/PS2/tutorials/basic/sequential_jobs/scripts/jcell_config_gen.sh\n)\n\n\n\n\nWe want to execute Jcell, and change the parameters MutationProb and CrossoverProb.\nThis script will install JCell, generate a tarball containing all the configuration files,\nand the list of parameters to be given to the launcher.\n\n\n\n\njcell_wrapper.sh\n (full path: \n$WORK/PS2/tutorials/basic/sequential_jobs/scripts/jcell_wrapper.sh\n)\n\n\n\n\nThis script is a wrapper, and will start one execution of jcell with the configuration file given in parameter.\nIf a result already exists, then the execution will be skipped.\nThanks to this simple test, our workflow is fault tolerant, \nif the job is interrupted and restarted, only the missing results will be computed.\n\n\n\n\nparallel_launcher.sh\n (full path: \n$WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n)\n\n\n\n\nThis script will drive the experiment, start and balance the java processes on all the reserved resources.\n\n\nStep 1: Generate the configuration files:\n\n\nExecute this script:\n\n\n    (access)$\n $WORK/PS2/tutorials/basic/sequential_jobs/scripts/jcell_config_gen.sh\n\n\n\nThis script will generate the following files in \n$WORK/PS2/jcell\n:\n\n\n\n\nconfig.tgz\n\n\njcell_param\n\n\n\n\nStep 2: Edit the launcher configuration, in the file \n$WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n.\n\n\nThis application is cpu-bound and not memory-bound, so we can set the value of \nNB_CORE_PER_TASK\n to 1.\nUsing these parameters, the launcher will spaw one java process per core on all the reserved nodes.\n\n\n    (access)$\n nano $WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\n    TASK=\"$WORK/PS2/tutorials/basic/sequential_jobs/scripts/jcell_wrapper.sh\"\n    ARG_TASK_FILE=\"$WORK/PS2/jcell/jcell_param\"\n    # number of cores needed for 1 task\n    NB_CORE_PER_TASK=1\n\n\n\nStep 3: Submit the job\n\n\n    (access)$\n oarsub -l nodes=2 $WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\n\n\nStep 4. Retrieve the results on your laptop:\n\n\n    (yourmachine)$\n rsync -avz chaos-cluster:/work/users/\nLOGIN\n/PS2/jcell/results .\n\n\n\nQuestion\n: check the system load and memory usage with Ganglia\n(\nChaos\n, \nGaia\n)\n\n\nAt the end, please, clean up your home and work directories :)\n\n\nPlease, don't store unnecessary files on the cluster's storage servers:\n\n\n(access)$\n rm -rf $WORK/PS2\n\n\n\nGoing further:\n\n\n\n\nCheckpoint / restart with BLCR\n\n\nOAR array jobs (fr)", 
            "title": "HPC workflow with sequential jobs"
        }, 
        {
            "location": "/basic/sequential_jobs/README/#hpc-workflow-with-sequential-jobs", 
            "text": "", 
            "title": "HPC workflow with sequential jobs"
        }, 
        {
            "location": "/basic/sequential_jobs/README/#prerequisites", 
            "text": "Make sure you have followed the tutorial \"Getting started\".", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/basic/sequential_jobs/README/#intro", 
            "text": "For many users, the typical usage of the HPC facilities is to execute 1 program with many parameters.\nOn your local machine, you can just start your program 100 times sequentially.\nHowever, you will obtain better results if you parallelize the executions on a HPC Cluster.  During this session, we will see 3 use cases:   Exercise 1: Use the serial launcher (1 node, in sequential and parallel mode);  Exercise 2: Use the generic launcher, distribute your executions on several nodes (python script);  Exercise 3: Advanced use case, using a Java program: \"JCell\".   We will use the following github repositories:   ULHPC/launcher-scripts  ULHPC/tutorials", 
            "title": "Intro"
        }, 
        {
            "location": "/basic/sequential_jobs/README/#practical-session-2", 
            "text": "", 
            "title": "Practical session 2"
        }, 
        {
            "location": "/basic/sequential_jobs/README/#connect-the-the-cluster-access-node-and-set-up-the-environment-for-this-tutorial", 
            "text": "(yourmachine)$  ssh chaos-cluster  If your network connection is unstable, use  screen :  (access)$  screen  We will use 2 directory:   $HOME : default home directory,  backed up , maximum 50GB, for important files  $WORK : work directory,  non backed up , maximum 500GB   Create a sub directory $WORK/PS2, and work inside it  (access)$  mkdir $WORK/PS2\n(access)$  cd $WORK/PS2  In the following parts, we will assume that you are working in this directory.  Clone the repositories  ULHPC/tutorials  and  ULHPC/launcher-scripts.git  (access)$  git clone https://github.com/ULHPC/launcher-scripts.git\n(access)$  git clone https://github.com/ULHPC/tutorials.git  In order to edit files in your terminal, you are expected to use your preferred text editor:   nano  vim  emacs  ...   If you have never used any of them,  nano  is intuitive, but vim and emacs are more powerful.", 
            "title": "Connect the the cluster access node, and set-up the environment for this tutorial"
        }, 
        {
            "location": "/basic/sequential_jobs/README/#exercise-1-parametric-experiment-with-gromacs", 
            "text": "Gromacs is a popular molecular dynamics software.\nIn this exercise, we will process some example input files, and make the parameter  fourier_spacing  varies from 0.1 to 0.2 in increments of 0.005.  Create a file which contains the list of parameters:  (access)$  seq 0.1 0.005 0.2   $WORK/PS2/param_file  Step 1: Naive workflow  We will use the launcher  NAIVE_AKA_BAD_launcher_serial.sh  (full path:  $WORK/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh ).  Edit the following variables:   TASK  must contain the path of the executable,   ARG_TASK_FILE  must contain the path of your parameter file. (node)$  nano $WORK/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh\n\nTASK=\"$WORK/PS2/tutorials/basic/sequential_jobs/scripts/run_gromacs_sim.sh\"\nARG_TASK_FILE=$WORK/PS2/param_file    Launch the job, in interactive mode and execute the launcher:  (access)$  oarsub -I -l core=1\n(node)$ $WORK/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh  Or  in passive mode (the output will be written in a file named  OAR. JOBID .stdout )  (access)$  oarsub -l core=1 $WORK/PS2/launcher-scripts/bash/serial/NAIVE_AKA_BAD_launcher_serial.sh  Step 2: Optimal method using GNU parallel (GNU Parallel)  We will use the launcher  launcher_serial.sh  (full path:  $WORK/PS2/launcher-scripts/bash/serial/launcher_serial.sh ).  Edit the following variables:  (access)$  nano $WORK/PS2/launcher-scripts/bash/serial/launcher_serial.sh\n\nTASK=\"$WORK/PS2/tutorials/basic/sequential_jobs/scripts/run_gromacs_sim.sh\"\nARG_TASK_FILE=$WORK/PS2/param_file  Submit the (passive) job with  oarsub  (access)$  oarsub -l nodes=1 $WORK/PS2/launcher-scripts/bash/serial/launcher_serial.sh  Question : compare and explain the execution time with both launchers:   Naive workflow: time = 16m 32s  Parallel workflow: time = 2m 11s   /!\\ In order to compare execution times, you must always use the same type of nodes (CPU/Memory), \nusing  properties \nin your  oarsub  command.", 
            "title": "Exercise 1: Parametric experiment with Gromacs"
        }, 
        {
            "location": "/basic/sequential_jobs/README/#exercise-2-watermarking-images-in-python", 
            "text": "We will use another program,  watermark.py  (full path:  $WORK/PS2/tutorials/basic/sequential_jobs/scripts/watermark.py ),\nand we will distribute the computation on 2 nodes with the launcher  parallel_launcher.sh \n(full path:  $WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh ).  This python script will apply a watermark to the images (using the Python Imaging library).  The command works like this:  python watermark.py  path/to/watermark_image   source_image   We will work with 2 files:   copyright.png : a transparent images, which can be applied as a watermark  images.tgz : a compressed file, containing 30 JPG pictures (of the Gaia Cluster :) ).   Step 1: Prepare the input files  Copy the source files in your $WORK directory.  (access) $ tar xvf /tmp/images.tgz -C $WORK/PS2/\n(access) $ cp /tmp/copyright.png $WORK/PS2\n\n(access) $ cd $WORK/PS2  Step 2: Create a list of parameters  We must create a file containing a list of parameters, each line will be passed to  watermark.py .  ls -d -1 $WORK/PS2/images/*.JPG | awk -v watermark=$WORK/PS2/copyright.png '{print watermark \" \" $1}'   $WORK/PS2/generic_launcher_param\n\\_____________________________/   \\_________________________________________________________________/ \\_________________________________/\n               1                                                    2                                                3   ls -d -1 : list the images  awk ... : prefix each line with the first parameter (watermark file)  : redirect the output to the file $WORK/generic_launcher_param   Step 3: Configure the launcher  We will use the launcher  parallel_launcher.sh  (full path:  $WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh ).  Edit the following variables:  (access)$  nano $WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\nTASK=\"$WORK/PS2/tutorials/basic/sequential_jobs/scripts/watermark.py\"\nARG_TASK_FILE=\"$WORK/PS2/generic_launcher_param\"\n# number of cores needed for 1 task\nNB_CORE_PER_TASK=2  Step 4: Submit the job  We will spawn 1 process / 2 cores  (access)$  oarsub -l nodes=2 $WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh  Step 5: Download the files  On your laptop, transfer the files in the current directory and look at them with your favorite viewer:  (yourmachine)$  rsync -avz chaos-cluster:/work/users/ LOGIN /PS2/images .  Question : which nodes are you using, identify your nodes with the command  oarstat -f -j  JOBID  or Monika\n( Chaos ,  Gaia )", 
            "title": "Exercise 2: Watermarking images in Python"
        }, 
        {
            "location": "/basic/sequential_jobs/README/#exercise-3-advanced-use-case-using-a-java-program-jcell", 
            "text": "Let's use  JCell , a framework for working with genetic algorithms, programmed in Java.  We will use 3 scripts:   jcell_config_gen.sh  (full path:  $WORK/PS2/tutorials/basic/sequential_jobs/scripts/jcell_config_gen.sh )   We want to execute Jcell, and change the parameters MutationProb and CrossoverProb.\nThis script will install JCell, generate a tarball containing all the configuration files,\nand the list of parameters to be given to the launcher.   jcell_wrapper.sh  (full path:  $WORK/PS2/tutorials/basic/sequential_jobs/scripts/jcell_wrapper.sh )   This script is a wrapper, and will start one execution of jcell with the configuration file given in parameter.\nIf a result already exists, then the execution will be skipped.\nThanks to this simple test, our workflow is fault tolerant, \nif the job is interrupted and restarted, only the missing results will be computed.   parallel_launcher.sh  (full path:  $WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh )   This script will drive the experiment, start and balance the java processes on all the reserved resources.  Step 1: Generate the configuration files:  Execute this script:      (access)$  $WORK/PS2/tutorials/basic/sequential_jobs/scripts/jcell_config_gen.sh  This script will generate the following files in  $WORK/PS2/jcell :   config.tgz  jcell_param   Step 2: Edit the launcher configuration, in the file  $WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh .  This application is cpu-bound and not memory-bound, so we can set the value of  NB_CORE_PER_TASK  to 1.\nUsing these parameters, the launcher will spaw one java process per core on all the reserved nodes.      (access)$  nano $WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh\n\n    TASK=\"$WORK/PS2/tutorials/basic/sequential_jobs/scripts/jcell_wrapper.sh\"\n    ARG_TASK_FILE=\"$WORK/PS2/jcell/jcell_param\"\n    # number of cores needed for 1 task\n    NB_CORE_PER_TASK=1  Step 3: Submit the job      (access)$  oarsub -l nodes=2 $WORK/PS2/launcher-scripts/bash/generic/parallel_launcher.sh  Step 4. Retrieve the results on your laptop:      (yourmachine)$  rsync -avz chaos-cluster:/work/users/ LOGIN /PS2/jcell/results .  Question : check the system load and memory usage with Ganglia\n( Chaos ,  Gaia )", 
            "title": "Exercise 3: Advanced use case, using a Java program: \"JCell\""
        }, 
        {
            "location": "/basic/sequential_jobs/README/#at-the-end-please-clean-up-your-home-and-work-directories", 
            "text": "Please, don't store unnecessary files on the cluster's storage servers:  (access)$  rm -rf $WORK/PS2", 
            "title": "At the end, please, clean up your home and work directories :)"
        }, 
        {
            "location": "/basic/sequential_jobs/README/#going-further", 
            "text": "Checkpoint / restart with BLCR  OAR array jobs (fr)", 
            "title": "Going further:"
        }, 
        {
            "location": "/advanced/RESIF/README/", 
            "text": "UL HPC Tutorial: Using RESIF to manage software modules\n\n\nThe objective of this tutorial is to present how to interact with the software installed on the UL HPC platform, from using provided software to extending the software collection by adding new software on the platform, and also reproducing the software environment on a local workstation.\n\n\nThis course is divided in three chapters that are going through each of these use cases.\n\n\nThe \nfirst chapter\n details the architecture of the software collection and the basic tools to start using the software.\n\nThe \nsecond chapter\n defines the process of adding new software to the existing stack as a user using RESIF, the tool developed internally to manage the software stack on the UL HPC platform.\n\nThe \nthird chapter\n explains the process of reproducing the software stack present on the platform, or part of it, in a local environment.\n\n\nUsing the software environment available on the UL HPC platform\n\n\nBefore starting this tutorial, please make sure you are on a compute node of Gaia/Chaos and not on the access node. To get resources on a compute node, use the following command:\n\n\n(access)$\n oarsub -I -l nodes=1,walltime=1:00:00\n\n(for more details about this command and the node reservation process on the clusters, please referer to the \nULHPC documentation\n.)\n\n\nUsing the software available on the UL HPC platform is done through \nLmod\n which provide a \nmodule\n command that we review in the following section.\nLmod allows us to provide a multitude of applications and libraries in multiple versions. These tools use special files named \"modules\" that define ways to manage environment variables such as PATH, LD_LIBRARY_PATH and MANPATH, enabling the easy loading and unloading of application/library profiles and their dependencies.\n\n\nmodule\n command basics and workflow\n\n\nFirstly, we activate the newest software stack:\n\n\nsource /opt/apps/resif/default_user.sh\n\n\n\nBy using \nmodule available\n (or the shorter forms \nmodule avail\n or \nmodule av\n) we can list all the software modules of the software stack:  \n\n\n(node)$\n module avail\n------------------- /opt/apps/devel/v1.1-20150414/core/modules/bio ----------------\nbio/ABySS/1.3.4-goolf-1.4.10-Python-2.7.3        bio/Bowtie2/2.2.2-goolf-1.4.10   (D)\nbio/ABySS/1.3.4-ictce-5.3.0-Python-2.7.3  (D)    bio/Cufflinks/2.0.2-goolf-1.4.10\n[...]\n\n\n\nNote that this command returns a lot of information since there is a lot of installed software. To reduce the output we can search for what we are interested in, for example:\n\n\n(node)$\n module avail gromacs\n----------- /opt/apps/devel/v1.1-20150414/core/modules/bio --------------\nbio/GROMACS/4.6.1-ictce-5.3.0-hybrid    bio/GROMACS/4.6.1-ictce-5.3.0-mt\nbio/GROMACS/4.6.5-goolf-1.4.10-mt (D)\n[...]\n\n\n\nThis will only output the software modules from the software stack that contain \"gromacs\" (case insensitive) in their name.\n\n\nTo start using an application in the version we require, for example \nbio/GROMACS/4.6.5-goolf-1.4.10-mt\n, we are going to \nload\n its software module:\n\n\n(node)$\n module load bio/GROMACS/4.6.5-goolf-1.4.10-mt\n\n\nWe can now use the software by running its commands (e.g. for Gromacs we can now use \nmdrun\n).\n\n\nTo check the currently loaded modules, we use the \nmodule list\n command:\n\n\n(node)$\n module list\nCurrently Loaded Modules:\n    1) compiler/GCC/4.7.2             4) toolchain/gompi/1.4.10                            7) numlib/ScaLAPACK/2.0.2-gompi-1.4.10-OpenBLAS-0.2.6-LAPACK-3.4.2\n    2) system/hwloc/1.6.2-GCC-4.7.2   5) numlib/OpenBLAS/0.2.6-gompi-1.4.10-LAPACK-3.4.2   8) toolchain/goolf/1.4.10\n    3) mpi/OpenMPI/1.6.4-GCC-4.7.2    6) numlib/FFTW/3.3.3-gompi-1.4.10                    9) bio/GROMACS/4.6.5-goolf-1.4.10-mt\n\n\n\nWhen we've finished working with the application, we can remove its environment profile with \nmodule unload\n:\n\n\n(node)$\n module unload bio/GROMACS/4.6.5-goolf-1.4.10-mt\n\n\nHowever, this will not remove its dependencies from the environment:\n\n\n(node)$\n module list\nCurrently Loaded Modules:\n    1) compiler/GCC/4.7.2             4) toolchain/gompi/1.4.10                            7) numlib/ScaLAPACK/2.0.2-gompi-1.4.10-OpenBLAS-0.2.6-LAPACK-3.4.2\n    2) system/hwloc/1.6.2-GCC-4.7.2   5) numlib/OpenBLAS/0.2.6-gompi-1.4.10-LAPACK-3.4.2   8) toolchain/goolf/1.4.10\n    3) mpi/OpenMPI/1.6.4-GCC-4.7.2    6) numlib/FFTW/3.3.3-gompi-1.4.10\n\n\n\nTo remove all the loaded modules at once we use the \nmodule purge\n command:\n\n\n(node)$\n module purge\n\n\nNext we are going to look at the hierarchical architecture of the modules.\n\n\nSoftware stack architecture\n\n\nThe upper layer of the architecture is what we call a \nsoftware set\n. It is a collection of software, for example we define a \ncore\n set that only contains commonly-used and tested software and an \nexperimental\n set that contains untested software.\n\nThe main goal of these categories is to provide information on the degree of support for the various software.\n\n\nInside these sets, software is named in regards to a \nnaming scheme\n which classifies the software (e.g. compilers, physics) and allows for a better structuring of results with the \nmodule avail\n command.\n\nThe software named using this scheme has the following format: \nsoftware_class/software_name/software_complete_version\n where\n\n- \nsoftware_class\n describes the category among the following classes: [base, bio, cae, chem, compiler, data, debugger, devel, lang, lib, math, mpi, numlib, phys, system, toolchain, tools, vis]\n- \nsoftware_name\n is the name of the software (e.g. GROMACS, MATLAB, R or ABySS)\n- \nsoftware_complete_version\n is the full version of the software: containing the version of the software itself followed by the type and version of the main dependencies it relies on (e.g. compiler) with the following format: software_version-dependencies_versions\n\n\nThe \nmodule avail\n command will thus have the output shown below, where we can note:\n\n\n\n\nthe core software set is shown first\n\n\napplication names are prefixed with the category (class)\n\n\nfull versions including tool dependencies are shown\n\n\nthe default module for each application is marked with a \n(D)\n, thus by loading \ncompiler/GCC\n the system would in effect load \ncompiler/GCC/4.8.2\n```\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/bio -------------------------------------------------------------------------------\n   bio/ABySS/1.3.4-goolf-1.4.10-Python-2.7.3        bio/Bowtie2/2.2.2-goolf-1.4.10   (D)    bio/GROMACS/4.6.1-ictce-5.3.0-hybrid     bio/GROMACS/4.6.5-goolf-1.4.10-mt (D)\n   bio/Bowtie2/2.0.2-ictce-5.3.0                    bio/Cufflinks/2.0.2-ictce-5.3.0  (D)    bio/GROMACS/4.6.5-goolf-1.4.10-hybrid    bio/SAMtools/0.1.18-ictce-5.3.0   (D)\n\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/cae -------------------------------------------------------------------------------\n   cae/ABAQUS/6.11.1    cae/OpenFOAM/2.3.0-goolf-1.4.10    cae/OpenFOAM/2.3.0-ictce-5.3.0 (D)\n\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/chem ------------------------------------------------------------------------------\n   chem/ABINIT/7.2.1-x86_64_linux_gnu4.5                chem/GPAW/0.9.0.8965-goolf-1.4.10-Python-2.7.3        chem/QuantumESPRESSO/5.0.2-goolf-1.4.10              chem/libctl/3.2.1-goolf-1.4.10\n   chem/ASE/3.6.0.2515-ictce-5.3.0-Python-2.7.3  (D)    chem/QuantumESPRESSO/5.0.2-goolf-1.4.10-hybrid        chem/QuantumESPRESSO/5.0.2-ictce-5.3.0        (D)\n\n---------------------------------------------------------------------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/compiler ----------------------------------------------------------------------------\n   compiler/GCC/4.7.2    compiler/GCC/4.8.2 (D)    compiler/icc/2013.3.163    compiler/ifort/2013.3.163\n\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/data ------------------------------------------------------------------------------\n   data/HDF5/1.8.7-goolf-1.4.10    data/HDF5/1.8.10-patch1-goolf-1.4.10        data/netCDF/4.2-goolf-1.4.10            data/netCDF-C++/4.2-goolf-1.4.10\n   data/HDF5/1.8.9-ictce-5.3.0     data/h5utils/1.12.1-ictce-5.3.0      (D)    data/netCDF/4.2.1.1-ictce-5.3.0  (D)    data/netCDF-Fortran/4.2-ictce-5.3.0  (D)\n```\n\n\n\n\n\n\n\nAdding a software to an existing stack on UL HPC\n\n\nIn this part, we are going to show the steps to add a software that is not already provided on the platform yet is available as a module in the \nEasyBuild database\n. Using the RESIF tool is the preferred way to add new software.\nFirst of all we are going to install RESIF and then use it to add a software (bzip2 in this example).\n\n\nNote: The following commands must be executed in an OAR job.\n\n\nInstallation of RESIF\n\n\nRESIF requires the following prerequisites (already available on the platform):\n\n\n\n\nPython 2.6\n or above\n\n\npip (included in the latest Python)\n\n\ngit\n\n\n\n\nRESIF installation:\n\n\n    (node)$\n pip install --install-option=\"--prefix=$HOME/.local\" resif\n\n\n\nInitially, we need to add the following paths to the environment:\n\n\n    (node)$\n export PATH=$PATH:~/.local/bin\n    (node)$\n export PYTHONPATH=$PYTHONPATH:~/.local/lib/python2.7/site-packages\n\n\n\nAlso, \n\n\nRESIF initialization which will download the required module sources to build new software:\n\n\n    (node)$\n resif init\n\n\n\nInstallation of additional software\n\n\nFirst we need to create a file that lists the applications we want to install.\n\nTo search for the names of the application configuration files, we use RESIF as follow:\n\n\n    (node)$\n resif search bzip2\n    [...]\n    * bzip2-1.0.6.eb\n    [...]\n\n\n\nCreate a file (we are assuming it is in the home directory), name it \nswsets.yaml\n and insert the following content:\n\n\n    mysoftware:\n      - bzip2-1.0.6.eb\n\n\n\nThis is a \nYAML\n format file that RESIF reads, the internal layout is the following:\n\n\n    software_set1_name:\n      - software1_configurationfile\n      - software2_configurationfile\n    software_set2_name:\n      - software3_configurationfile\n\n\n\nIt can list as many software, divided in as many software sets as we require to structure our installation.\n\n\nNow we install the software using \nresif build\n:\n\n\n    (node)$\n resif build --installdir $HOME/.local/resif --swsets-config ~/swsets.yaml mysoftware\n\n\n\nThis will install the software using ~/.local/resif as the root of the installation.\n\n\nTo make the software modules available through the \nmodule\n command, we need to add their path:\n\n\n    (node)$\n module use $HOME/.local/resif/mysoftware/modules/all\n\n\n\nNow, we can see \nbzip2\n at the very beginning of the output of the list of the software modules:\n\n\n    (node)$\n module avail\n    ----- /home/users/username/.local/resif/mysoftware/core/modules/all -----\n    tools/bzip2/1.0.6\n\n\n\nThe software is installed, and we can load its profile with \nmodule load tools/bzip2/1.0.6\n.\n\n\nRESIF offers many more possibilities than this basic functionality, for more details check the \ndocumentation\n.\n\n\nReplicating the software sets in a local environment\n\n\nIn this part, we are going to create a software stack from scratch. This is especially useful if we want to work on a local workstation (e.g. a laptop) with the same tools as those available on the platform.\n\n\nWe assume that RESIF is already installed as per the instructions described \nabove\n.\n\nNote: RESIF internally depends on \nEasyBuild\n which is compatible with Linux/OSX but not Windows. On Windows, you can configure a Linux Virtual Machine to use RESIF and the software built using it.\n\n\nAt this point, please make sure to unset the RESIF_ROOTINSTALL environment variable.\n This is a necessary step since this variable would interfere with the creation of a new software stack (which defines a new rootinstall).\n\nTo do so, execute the following command:\n\n\n    unset RESIF_ROOTINSTALL\n\n\n\nDirect method\n\n\nA \nswsets.yaml\n file that describes which software we want to install needs to be created.\n\nAs an example, create it in your home directory with the following content:\n\n\n    core:\n      - bzip2-1.0.6.eb\n      - ABINIT-7.2.1-x86_64_linux_gnu4.5.eb\n\n\n\nWe include here only the bzip2 library and the ABINIT software as they are fast to deploy.\n\n\nWe can now install this simple software stack with the following command:\n\n\n    $\n resif cleaninstall --swsets-config ~/swsets.yaml core\n\n\n\nThis will install everything using \n~/.local/resif\n as the root of the installation.\n\n\nTo use this newly installed software stack, you need to source the LOADME file inside of the rootinstall directory.\n\nThis path should look like this: \n~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh\n.\n\n\n    $\n source ~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh\n\n\n\nThe software stack is now ready to be used with the \nmodule\n command.\n\n\nIndirect method\n\n\nIn this method we are going to create the architecture and then add software to it.\n\n\nWe create the architecture using the \nbootstrap\n subcommand:\n\n\n    $\n resif bootstrap\n\n\n\nThis will create the architecture using \n~/.local/resif\n as the root of the installation.\n\n\nWe now need to make this architecture active: you need to source the LOADME file inside of the rootinstall directory.\n\nThis path should look like this: \n~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh\n.\n\n\n    $\n source ~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh\n\n\n\nThen we need to create the file that describe the software we want to install. In the home directory, create a file named \nswsets.yaml\n and make it match the following content:\n\n\n    core:\n      - bzip2-1.0.6.eb\n      - ABINIT-7.2.1-x86_64_linux_gnu4.5.eb\n\n\n\nWe include here only the bzip2 library and the ABINIT software as they are fast to deploy.\n\n\nWe now only need to build the given software:\n\n\n    $\n resif build --swsets-config ~/swsets.yaml core\n\n\n\nThis will install the software listed in the \nswsets.yaml\n file. The software stack is now ready to be used.\n\n\nTo learn more about RESIF and how to control more parameters of its usage, please refer to the \ndocumentation\n.\n\n\nTo conclude this tutorial, here is a schema that summarizes the previous parts:", 
            "title": "RESIF"
        }, 
        {
            "location": "/advanced/RESIF/README/#ul-hpc-tutorial-using-resif-to-manage-software-modules", 
            "text": "The objective of this tutorial is to present how to interact with the software installed on the UL HPC platform, from using provided software to extending the software collection by adding new software on the platform, and also reproducing the software environment on a local workstation.  This course is divided in three chapters that are going through each of these use cases.  The  first chapter  details the architecture of the software collection and the basic tools to start using the software. \nThe  second chapter  defines the process of adding new software to the existing stack as a user using RESIF, the tool developed internally to manage the software stack on the UL HPC platform. \nThe  third chapter  explains the process of reproducing the software stack present on the platform, or part of it, in a local environment.", 
            "title": "UL HPC Tutorial: Using RESIF to manage software modules"
        }, 
        {
            "location": "/advanced/RESIF/README/#using-the-software-environment-available-on-the-ul-hpc-platform", 
            "text": "Before starting this tutorial, please make sure you are on a compute node of Gaia/Chaos and not on the access node. To get resources on a compute node, use the following command:  (access)$  oarsub -I -l nodes=1,walltime=1:00:00 \n(for more details about this command and the node reservation process on the clusters, please referer to the  ULHPC documentation .)  Using the software available on the UL HPC platform is done through  Lmod  which provide a  module  command that we review in the following section.\nLmod allows us to provide a multitude of applications and libraries in multiple versions. These tools use special files named \"modules\" that define ways to manage environment variables such as PATH, LD_LIBRARY_PATH and MANPATH, enabling the easy loading and unloading of application/library profiles and their dependencies.  module  command basics and workflow  Firstly, we activate the newest software stack:  source /opt/apps/resif/default_user.sh  By using  module available  (or the shorter forms  module avail  or  module av ) we can list all the software modules of the software stack:    (node)$  module avail\n------------------- /opt/apps/devel/v1.1-20150414/core/modules/bio ----------------\nbio/ABySS/1.3.4-goolf-1.4.10-Python-2.7.3        bio/Bowtie2/2.2.2-goolf-1.4.10   (D)\nbio/ABySS/1.3.4-ictce-5.3.0-Python-2.7.3  (D)    bio/Cufflinks/2.0.2-goolf-1.4.10\n[...]  Note that this command returns a lot of information since there is a lot of installed software. To reduce the output we can search for what we are interested in, for example:  (node)$  module avail gromacs\n----------- /opt/apps/devel/v1.1-20150414/core/modules/bio --------------\nbio/GROMACS/4.6.1-ictce-5.3.0-hybrid    bio/GROMACS/4.6.1-ictce-5.3.0-mt\nbio/GROMACS/4.6.5-goolf-1.4.10-mt (D)\n[...]  This will only output the software modules from the software stack that contain \"gromacs\" (case insensitive) in their name.  To start using an application in the version we require, for example  bio/GROMACS/4.6.5-goolf-1.4.10-mt , we are going to  load  its software module:  (node)$  module load bio/GROMACS/4.6.5-goolf-1.4.10-mt  We can now use the software by running its commands (e.g. for Gromacs we can now use  mdrun ).  To check the currently loaded modules, we use the  module list  command:  (node)$  module list\nCurrently Loaded Modules:\n    1) compiler/GCC/4.7.2             4) toolchain/gompi/1.4.10                            7) numlib/ScaLAPACK/2.0.2-gompi-1.4.10-OpenBLAS-0.2.6-LAPACK-3.4.2\n    2) system/hwloc/1.6.2-GCC-4.7.2   5) numlib/OpenBLAS/0.2.6-gompi-1.4.10-LAPACK-3.4.2   8) toolchain/goolf/1.4.10\n    3) mpi/OpenMPI/1.6.4-GCC-4.7.2    6) numlib/FFTW/3.3.3-gompi-1.4.10                    9) bio/GROMACS/4.6.5-goolf-1.4.10-mt  When we've finished working with the application, we can remove its environment profile with  module unload :  (node)$  module unload bio/GROMACS/4.6.5-goolf-1.4.10-mt  However, this will not remove its dependencies from the environment:  (node)$  module list\nCurrently Loaded Modules:\n    1) compiler/GCC/4.7.2             4) toolchain/gompi/1.4.10                            7) numlib/ScaLAPACK/2.0.2-gompi-1.4.10-OpenBLAS-0.2.6-LAPACK-3.4.2\n    2) system/hwloc/1.6.2-GCC-4.7.2   5) numlib/OpenBLAS/0.2.6-gompi-1.4.10-LAPACK-3.4.2   8) toolchain/goolf/1.4.10\n    3) mpi/OpenMPI/1.6.4-GCC-4.7.2    6) numlib/FFTW/3.3.3-gompi-1.4.10  To remove all the loaded modules at once we use the  module purge  command:  (node)$  module purge  Next we are going to look at the hierarchical architecture of the modules.  Software stack architecture  The upper layer of the architecture is what we call a  software set . It is a collection of software, for example we define a  core  set that only contains commonly-used and tested software and an  experimental  set that contains untested software. \nThe main goal of these categories is to provide information on the degree of support for the various software.  Inside these sets, software is named in regards to a  naming scheme  which classifies the software (e.g. compilers, physics) and allows for a better structuring of results with the  module avail  command. \nThe software named using this scheme has the following format:  software_class/software_name/software_complete_version  where \n-  software_class  describes the category among the following classes: [base, bio, cae, chem, compiler, data, debugger, devel, lang, lib, math, mpi, numlib, phys, system, toolchain, tools, vis]\n-  software_name  is the name of the software (e.g. GROMACS, MATLAB, R or ABySS)\n-  software_complete_version  is the full version of the software: containing the version of the software itself followed by the type and version of the main dependencies it relies on (e.g. compiler) with the following format: software_version-dependencies_versions  The  module avail  command will thus have the output shown below, where we can note:   the core software set is shown first  application names are prefixed with the category (class)  full versions including tool dependencies are shown  the default module for each application is marked with a  (D) , thus by loading  compiler/GCC  the system would in effect load  compiler/GCC/4.8.2 ```\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/bio -------------------------------------------------------------------------------\n   bio/ABySS/1.3.4-goolf-1.4.10-Python-2.7.3        bio/Bowtie2/2.2.2-goolf-1.4.10   (D)    bio/GROMACS/4.6.1-ictce-5.3.0-hybrid     bio/GROMACS/4.6.5-goolf-1.4.10-mt (D)\n   bio/Bowtie2/2.0.2-ictce-5.3.0                    bio/Cufflinks/2.0.2-ictce-5.3.0  (D)    bio/GROMACS/4.6.5-goolf-1.4.10-hybrid    bio/SAMtools/0.1.18-ictce-5.3.0   (D)\n\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/cae -------------------------------------------------------------------------------\n   cae/ABAQUS/6.11.1    cae/OpenFOAM/2.3.0-goolf-1.4.10    cae/OpenFOAM/2.3.0-ictce-5.3.0 (D)\n\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/chem ------------------------------------------------------------------------------\n   chem/ABINIT/7.2.1-x86_64_linux_gnu4.5                chem/GPAW/0.9.0.8965-goolf-1.4.10-Python-2.7.3        chem/QuantumESPRESSO/5.0.2-goolf-1.4.10              chem/libctl/3.2.1-goolf-1.4.10\n   chem/ASE/3.6.0.2515-ictce-5.3.0-Python-2.7.3  (D)    chem/QuantumESPRESSO/5.0.2-goolf-1.4.10-hybrid        chem/QuantumESPRESSO/5.0.2-ictce-5.3.0        (D)\n\n---------------------------------------------------------------------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/compiler ----------------------------------------------------------------------------\n   compiler/GCC/4.7.2    compiler/GCC/4.8.2 (D)    compiler/icc/2013.3.163    compiler/ifort/2013.3.163\n\n------------------------------------------------------------------------------ /opt/apps/resif/devel/v1.1-20150414/core/modules/data ------------------------------------------------------------------------------\n   data/HDF5/1.8.7-goolf-1.4.10    data/HDF5/1.8.10-patch1-goolf-1.4.10        data/netCDF/4.2-goolf-1.4.10            data/netCDF-C++/4.2-goolf-1.4.10\n   data/HDF5/1.8.9-ictce-5.3.0     data/h5utils/1.12.1-ictce-5.3.0      (D)    data/netCDF/4.2.1.1-ictce-5.3.0  (D)    data/netCDF-Fortran/4.2-ictce-5.3.0  (D)\n```", 
            "title": "Using the software environment available on the UL HPC platform"
        }, 
        {
            "location": "/advanced/RESIF/README/#adding-a-software-to-an-existing-stack-on-ul-hpc", 
            "text": "In this part, we are going to show the steps to add a software that is not already provided on the platform yet is available as a module in the  EasyBuild database . Using the RESIF tool is the preferred way to add new software.\nFirst of all we are going to install RESIF and then use it to add a software (bzip2 in this example).  Note: The following commands must be executed in an OAR job.  Installation of RESIF  RESIF requires the following prerequisites (already available on the platform):   Python 2.6  or above  pip (included in the latest Python)  git   RESIF installation:      (node)$  pip install --install-option=\"--prefix=$HOME/.local\" resif  Initially, we need to add the following paths to the environment:      (node)$  export PATH=$PATH:~/.local/bin\n    (node)$  export PYTHONPATH=$PYTHONPATH:~/.local/lib/python2.7/site-packages  Also,   RESIF initialization which will download the required module sources to build new software:      (node)$  resif init  Installation of additional software  First we need to create a file that lists the applications we want to install. \nTo search for the names of the application configuration files, we use RESIF as follow:      (node)$  resif search bzip2\n    [...]\n    * bzip2-1.0.6.eb\n    [...]  Create a file (we are assuming it is in the home directory), name it  swsets.yaml  and insert the following content:      mysoftware:\n      - bzip2-1.0.6.eb  This is a  YAML  format file that RESIF reads, the internal layout is the following:      software_set1_name:\n      - software1_configurationfile\n      - software2_configurationfile\n    software_set2_name:\n      - software3_configurationfile  It can list as many software, divided in as many software sets as we require to structure our installation.  Now we install the software using  resif build :      (node)$  resif build --installdir $HOME/.local/resif --swsets-config ~/swsets.yaml mysoftware  This will install the software using ~/.local/resif as the root of the installation.  To make the software modules available through the  module  command, we need to add their path:      (node)$  module use $HOME/.local/resif/mysoftware/modules/all  Now, we can see  bzip2  at the very beginning of the output of the list of the software modules:      (node)$  module avail\n    ----- /home/users/username/.local/resif/mysoftware/core/modules/all -----\n    tools/bzip2/1.0.6  The software is installed, and we can load its profile with  module load tools/bzip2/1.0.6 .  RESIF offers many more possibilities than this basic functionality, for more details check the  documentation .", 
            "title": "Adding a software to an existing stack on UL HPC"
        }, 
        {
            "location": "/advanced/RESIF/README/#replicating-the-software-sets-in-a-local-environment", 
            "text": "In this part, we are going to create a software stack from scratch. This is especially useful if we want to work on a local workstation (e.g. a laptop) with the same tools as those available on the platform.  We assume that RESIF is already installed as per the instructions described  above . \nNote: RESIF internally depends on  EasyBuild  which is compatible with Linux/OSX but not Windows. On Windows, you can configure a Linux Virtual Machine to use RESIF and the software built using it.  At this point, please make sure to unset the RESIF_ROOTINSTALL environment variable.  This is a necessary step since this variable would interfere with the creation of a new software stack (which defines a new rootinstall). \nTo do so, execute the following command:      unset RESIF_ROOTINSTALL  Direct method  A  swsets.yaml  file that describes which software we want to install needs to be created. \nAs an example, create it in your home directory with the following content:      core:\n      - bzip2-1.0.6.eb\n      - ABINIT-7.2.1-x86_64_linux_gnu4.5.eb  We include here only the bzip2 library and the ABINIT software as they are fast to deploy.  We can now install this simple software stack with the following command:      $  resif cleaninstall --swsets-config ~/swsets.yaml core  This will install everything using  ~/.local/resif  as the root of the installation.  To use this newly installed software stack, you need to source the LOADME file inside of the rootinstall directory. \nThis path should look like this:  ~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh .      $  source ~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh  The software stack is now ready to be used with the  module  command.  Indirect method  In this method we are going to create the architecture and then add software to it.  We create the architecture using the  bootstrap  subcommand:      $  resif bootstrap  This will create the architecture using  ~/.local/resif  as the root of the installation.  We now need to make this architecture active: you need to source the LOADME file inside of the rootinstall directory. \nThis path should look like this:  ~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh .      $  source ~/.local/resif/devel/vx.y-YYYYMMDD/LOADME-vx.y-YYYYMMDD.sh  Then we need to create the file that describe the software we want to install. In the home directory, create a file named  swsets.yaml  and make it match the following content:      core:\n      - bzip2-1.0.6.eb\n      - ABINIT-7.2.1-x86_64_linux_gnu4.5.eb  We include here only the bzip2 library and the ABINIT software as they are fast to deploy.  We now only need to build the given software:      $  resif build --swsets-config ~/swsets.yaml core  This will install the software listed in the  swsets.yaml  file. The software stack is now ready to be used.  To learn more about RESIF and how to control more parameters of its usage, please refer to the  documentation .  To conclude this tutorial, here is a schema that summarizes the previous parts:", 
            "title": "Replicating the software sets in a local environment"
        }, 
        {
            "location": "/advanced/EasyBuild/README/", 
            "text": "README.md\n\n\nCopyright (c) 2014 Xavier Besseron \n\n\n\n\nUL HPC Tutorial: Build software with EasyBuild on UL HPC platform\n\n\nThe objective of this tutorial is to show how EasyBuild can be used to ease, automate and script the build of software on the UL HPC platforms. \n\n\nTwo use-cases are considered. First, we are going to build software that are supported by EasyBuild. In a second time, we will see through a simple example how to add support for a new software in EasyBuild.\n\n\nThe benefit of using EasyBuild for your builds is that it allows automated and reproducable build of software. Once a build has been made, the build script (via the \nEasyConfig file\n) or the installed software (via the \nmodule file\n) can be shared with other users.\n\n\nBefore starting this tutorial, ensure you are able to \nconnect to the chaos and gaia cluster\n. \n\nFor all your compilation with Easybuild, you must work on a computing node:\n\n\n(access)$\n  oarsub -I -l nodes=1,walltime=4\n\n\n\nThe latest version of this tutorial is available on our\n\nreadthedocs\n documentation.\n\n\nShort introduction to EasyBuild\n\n\nEasyBuild is a tool that allows to perform automated and reproducible compilation and installation of software. A large number of scientific software are supported (479 software packages in the last release).\n\n\nAll builds and installations are performed at user level, so you don't need the admin rights. \nThe software are installed in your home directory (by default in \n$HOME/.local/easybuild/software/\n) and a module file is generated (by default in \n$HOME/.local/easybuild/modules/\n) to use the software.\n\n\nEasyBuild relies on two main concepts: \nToolchains\n and \nEasyConfig file\n.\n\n\nA \ntoolchain\n corresponds to a compiler and a set of libraries which are commonly used to build a software. The two main toolchains frequently used on the UL HPC platform are the GOOLF and the ICTCE toolchains. GOOLF is based on the GCC compiler and on open-source libraries (OpenMPI, OpenBLAS, etc.). ICTCE is based on the Intel compiler and on Intel libraries (Intel MPI, Intel Math Kernel Library, etc.). \n\n\nAn \nEasyConfig file\n is a simple text file that describes the build process of a software. For most software that uses standard procedure (like \nconfigure\n, \nmake\n and \nmake install\n), this file is very simple. Many EasyConfig files are already provided with EasyBuild.\n\n\nEasyConfig files and generated modules are named using the following convention:\n\nSoftware-Name\n-\nSoftware-Version\n-\nToolchain-Name\n-\nToolchain-Version\n \n\n\nAdditional details are available on EasyBuild website:\n\n\n\n\nEasyBuild homepage\n\n\nEasyBuild documentation\n\n\nWhat is EasyBuild?\n\n\nToolchains\n\n\nEasyConfig files\n\n\nList of supported software packages\n\n\n\n\nEasyBuild on UL HPC platform\n\n\nTo use EasyBuild on a compute node, load the EasyBuild module:\n\n\n$\n module avail EasyBuild\n\n------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools -------------\n    tools/EasyBuild/2.0.0\n\n------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/base -------------\n    base/EasyBuild/install-2.1.0\n\n$\n module load base/EasyBuild/install-2.1.0\n\n\n\nThe EasyBuild command is \neb\n. Check the version you have loaded:\n\n\n$\n eb --version\n\nThis is EasyBuild 2.1.0dev-r6fee583a88e99d1384314790a419c83e85f18f3d (framework: 2.1.0dev-r2aa673bb5f61cb2d65e4a3037cc2337e6df2d3e6, easyblocks: 2.1.0dev-r6fee583a88e99d1384314790a419c83e85f18f3d) on host h-cluster1-11.\n\n\n\nNote that this version number is a bit peculiar because this is a custom installation on the cluster.\n\n\nTo get help on the EasyBuild options, use the \n-h\n or \n-H\n option flags:\n\n\n$\n eb -h\n$\n eb -H\n\n\n\nBuild software using provided EasyConfig file\n\n\nIn this part, we propose to be High Performance Linpack (HPL) using EasyBuild. \nHPL is supported by EasyBuild, this means that an EasyConfig file allowing to build HPL is already provided with EasyBuild.\n\n\nFirst, let's see which HPL are available on the cluster:\n\n\n$\n module avail HPL\n\n------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools -------------\n    tools/HPL/2.0-goolf-1.4.10\n\n\n\nThen, search for available EasyConfig files with HPL in their name. The EasyConfig files are named with the \n.eb\n extension.\n\n\n$\n eb -S HPL\n\n== temporary log file in case of crash /tmp/eb-p2DT7H/easybuild-ligIot.log\n== Searching (case-insensitive) for 'HPL' in /opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs \nCFGS1=/opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs/h/HPL\n * $CFGS1/HPL-2.0-cgmpolf-1.1.6.eb\n * $CFGS1/HPL-2.0-cgmvolf-1.1.12rc1.eb\n * $CFGS1/HPL-2.0-cgmvolf-1.2.7.eb\n * $CFGS1/HPL-2.0-cgoolf-1.1.7.eb\n * $CFGS1/HPL-2.0-foss-2014b.eb\n * $CFGS1/HPL-2.0-goalf-1.1.0-no-OFED.eb\n * $CFGS1/HPL-2.0-goolf-1.4.10.eb\n * $CFGS1/HPL-2.0-goolf-1.5.16.eb\n * $CFGS1/HPL-2.0-ictce-4.0.6.eb\n * $CFGS1/HPL-2.0-ictce-5.3.0.eb\n * $CFGS1/HPL-2.0-ictce-6.0.5.eb\n * $CFGS1/HPL-2.0-ictce-6.1.5.eb\n * $CFGS1/HPL-2.0-iomkl-4.6.13.eb\n * $CFGS1/HPL-2.1-foss-2015a.eb\n * $CFGS1/HPL-2.1-gimkl-1.5.9.eb\n * $CFGS1/HPL-2.1-gmpolf-1.4.8.eb\n * $CFGS1/HPL-2.1-gmvolf-1.7.20.eb\n * $CFGS1/HPL-2.1-goolf-1.7.20.eb\n * $CFGS1/HPL-2.1-goolfc-1.4.10.eb\n * $CFGS1/HPL-2.1-goolfc-2.6.10.eb\n * $CFGS1/HPL-2.1-gpsolf-2014.12.eb\n * $CFGS1/HPL-2.1-ictce-6.3.5.eb\n * $CFGS1/HPL-2.1-ictce-7.1.2.eb\n * $CFGS1/HPL-2.1-intel-2014.10.eb\n * $CFGS1/HPL-2.1-intel-2014.11.eb\n * $CFGS1/HPL-2.1-intel-2014b.eb\n * $CFGS1/HPL-2.1-intel-2015.02.eb\n * $CFGS1/HPL-2.1-intel-2015a.eb\n * $CFGS1/HPL-2.1-intel-para-2014.12.eb\n * $CFGS1/HPL-2.1-iomkl-2015.01.eb\n * $CFGS1/HPL-2.1-iomkl-2015.02.eb\n * $CFGS1/HPL_parallel-make.patch\n== temporary log file(s) /tmp/eb-p2DT7H/easybuild-ligIot.log* have been removed.\n== temporary directory /tmp/eb-p2DT7H has been removed.\n\n\n\nIf we try to build \nHPL-2.0-goolf-1.4.10\n, nothing will be done as it is already installed on the cluster.\n\n\n$\n eb HPL-2.0-goolf-1.4.10.eb\n\n== temporary log file in case of crash /tmp/eb-JKadCH/easybuild-SoXdix.log\n== tools/HPL/2.0-goolf-1.4.10 is already installed (module found), skipping\n== No easyconfigs left to be built.\n== Build succeeded for 0 out of 0\n== temporary log file(s) /tmp/eb-JKadCH/easybuild-SoXdix.log* have been removed.\n== temporary directory /tmp/eb-JKadCH has been removed.\n\n\n\nHowever the build can be forced using the \n-f\n option flag. Then this software will be re-built.\n(Tip: prefix your command with \ntime\n to know its duration)\n\n\n$\n time eb HPL-2.0-goolf-1.4.10.eb -f\n\n== temporary log file in case of crash /tmp/eb-FAO8AO/easybuild-ea15Cq.log\n== processing EasyBuild easyconfig /opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs/h/HPL/HPL-2.0-goolf-1.4.10.eb\n== building and installing tools/HPL/2.0-goolf-1.4.10...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/tools/HPL/2.0-goolf-1.4.10/easybuild/easybuild-HPL-2.0-20150624.113223.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-FAO8AO/easybuild-ea15Cq.log* have been removed.\n== temporary directory /tmp/eb-FAO8AO has been removed.\n\nreal    1m10.619s\nuser    0m49.387s\nsys     0m7.828s\n\n\n\nLet's have a look at \nHPL-2.0-ictce-5.3.0\n which is not installed yet. \nWe can check if a software and its dependencies are installed using the \n-Dr\n option flag:\n\n\n$\n eb HPL-2.0-ictce-5.3.0.eb -Dr\n\n== temporary log file in case of crash /tmp/eb-HlZDMR/easybuild-JbndYN.log\nDry run: printing build status of easyconfigs and dependencies\nCFGS=/opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs\n * [x] $CFGS/i/icc/icc-2013.3.163.eb (module: compiler/icc/2013.3.163)\n * [x] $CFGS/i/ifort/ifort-2013.3.163.eb (module: compiler/ifort/2013.3.163)\n * [x] $CFGS/i/iccifort/iccifort-2013.3.163.eb (module: toolchain/iccifort/2013.3.163)\n * [x] $CFGS/i/impi/impi-4.1.0.030-iccifort-2013.3.163.eb (module: mpi/impi/4.1.0.030-iccifort-2013.3.163)\n * [x] $CFGS/i/iimpi/iimpi-5.3.0.eb (module: toolchain/iimpi/5.3.0)\n * [x] $CFGS/i/imkl/imkl-11.0.3.163-iimpi-5.3.0.eb (module: numlib/imkl/11.0.3.163-iimpi-5.3.0)\n * [x] $CFGS/i/ictce/ictce-5.3.0.eb (module: toolchain/ictce/5.3.0)\n * [ ] $CFGS/h/HPL/HPL-2.0-ictce-5.3.0.eb (module: tools/HPL/2.0-ictce-5.3.0)\n== temporary log file(s) /tmp/eb-HlZDMR/easybuild-JbndYN.log* have been removed.\n== temporary directory /tmp/eb-HlZDMR has been removed.\n\n\n\nHPL-2.0-ictce-5.3.0\n is not available but all it dependencies are. Let's build it:\n\n\n$\n time eb HPL-2.0-ictce-5.3.0.eb\n\n== temporary log file in case of crash /tmp/eb-UFlEv7/easybuild-uVbm24.log\n== processing EasyBuild easyconfig /opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs/h/HPL/HPL-2.0-ictce-5.3.0.eb\n== building and installing tools/HPL/2.0-ictce-5.3.0...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/tools/HPL/2.0-ictce-5.3.0/easybuild/easybuild-HPL-2.0-20150624.113547.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-UFlEv7/easybuild-uVbm24.log* have been removed.\n== temporary directory /tmp/eb-UFlEv7 has been removed.\n\nreal    1m25.849s\nuser    0m49.039s\nsys     0m10.961s\n\n\n\nTo see the newly installed modules, you need to add the path where they were installed to the MODULEPATH. On the cluster you have to use the \nmodule use\n command:\n\n\n$\n module use $HOME/.local/easybuild/modules/all/\n\n\n\nCheck which HPL modules are available now:\n\n\n$\n module avail HPL\n\n------------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all -------------\n    tools/HPL/2.0-goolf-1.4.10    tools/HPL/2.0-ictce-5.3.0 (D)\n\n---------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools ----------------\n    tools/HPL/2.0-goolf-1.4.10\n\n\n\nThe two newly-built versions of HPL are now available for your user. You can use them with the usually \nmodule load\n command.\n\n\nAmending an existing EasyConfig file\n\n\nIt is possible to amend existing EasyConfig file to build software with slightly different parameters. \n\n\nAs a example, we are going to build the lastest version of HPL (2.1) with ICTCE toolchain. We use the \n--try-software-version\n option flag to overide the HPL version.\n\n\n$\n time eb HPL-2.0-ictce-5.3.0.eb --try-software-version=2.1\n\n== temporary log file in case of crash /tmp/eb-ocChbK/easybuild-liMmlk.log\n== processing EasyBuild easyconfig /tmp/eb-ocChbK/tweaked_easyconfigs/HPL-2.1-ictce-5.3.0.eb\n== building and installing tools/HPL/2.1-ictce-5.3.0...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/tools/HPL/2.1-ictce-5.3.0/easybuild/easybuild-HPL-2.1-20150624.114243.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-ocChbK/easybuild-liMmlk.log* have been removed.\n== temporary directory /tmp/eb-ocChbK has been removed.\n\nreal    1m24.933s\nuser    0m53.167s\nsys     0m11.533s\n\n$\n module avail HPL\n\n------------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all -------------\n    tools/HPL/2.0-goolf-1.4.10    tools/HPL/2.0-ictce-5.3.0    tools/HPL/2.1-ictce-5.3.0 (D)\n\n---------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools ----------------\n    tools/HPL/2.0-goolf-1.4.10\n\n\n\nWe obtained HPL 2.1 without writing any EasyConfig file.\n\n\nThere are multiple ways to amend a EasyConfig file. Check the \n--try-*\n option flags for all the possibilities.\n\n\nBuild software using your own EasyConfig file\n\n\nFor this example, we create an EasyConfig file to build GZip 1.4 with the GOOLF toolchain.\nOpen your favorite editor and create a file named \ngzip-1.4-goolf-1.4.10.eb\n with the following content:\n\n\neasyblock = 'ConfigureMake'\n\nname = 'gzip'\nversion = '1.4'\n\nhomepage = 'http://www.gnu.org/software/gzip/'\ndescription = \"gzip (GNU zip) is a popular data compression program as a replacement for compress\"\n\n# use the GOOLF toolchain\ntoolchain = {'name': 'goolf', 'version': '1.4.10'}\n\n# specify that GCC compiler should be used to build gzip\npreconfigopts = \"CC='gcc'\"\n\n# source tarball filename\nsources = ['%s-%s.tar.gz'%(name,version)]\n\n# download location for source files\nsource_urls = ['http://ftpmirror.gnu.org/gzip']\n\n# make sure the gzip and gunzip binaries are available after installation\nsanity_check_paths = {\n                      'files': [\"bin/gunzip\", \"bin/gzip\"],\n                      'dirs': []\n                     }\n\n# run 'gzip -h' and 'gzip --version' after installation\nsanity_check_commands = [True, ('gzip', '--version')]\n\n\n\nThis is a simple EasyConfig. Most of the fields are self-descriptive. No build method is explicitely defined, so it uses by default the standard \nconfigure/make/make install\n approach.\n\n\nLet's build GZip with this EasyConfig file:\n\n\n$\n time eb gzip-1.4-goolf-1.4.10.eb\n\n== temporary log file in case of crash /tmp/eb-hiyyN1/easybuild-ynLsHC.log\n== processing EasyBuild easyconfig /mnt/nfs/users/homedirs/mschmitt/gzip-1.4-goolf-1.4.10.eb\n== building and installing base/gzip/1.4-goolf-1.4.10...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/base/gzip/1.4-goolf-1.4.10/easybuild/easybuild-gzip-1.4-20150624.114745.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-hiyyN1/easybuild-ynLsHC.log* have been removed.\n== temporary directory /tmp/eb-hiyyN1 has been removed.\n\nreal    1m39.982s\nuser    0m52.743s\nsys     0m11.297s\n\n\n\nWe can now check that our version of GZip is available via the modules:\n\n\n$\n module avail gzip\n\n--------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all ---------\n    base/gzip/1.4-goolf-1.4.10\n\n\n\nTo go further\n\n\n\n\nEasyBuild homepage\n\n\nEasyBuild documentation\n\n\nGetting started\n\n\nUsing EasyBuild\n\n\nStep-by-step guide", 
            "title": "Easybuild"
        }, 
        {
            "location": "/advanced/EasyBuild/README/#ul-hpc-tutorial-build-software-with-easybuild-on-ul-hpc-platform", 
            "text": "The objective of this tutorial is to show how EasyBuild can be used to ease, automate and script the build of software on the UL HPC platforms.   Two use-cases are considered. First, we are going to build software that are supported by EasyBuild. In a second time, we will see through a simple example how to add support for a new software in EasyBuild.  The benefit of using EasyBuild for your builds is that it allows automated and reproducable build of software. Once a build has been made, the build script (via the  EasyConfig file ) or the installed software (via the  module file ) can be shared with other users.  Before starting this tutorial, ensure you are able to  connect to the chaos and gaia cluster .  For all your compilation with Easybuild, you must work on a computing node:  (access)$   oarsub -I -l nodes=1,walltime=4  The latest version of this tutorial is available on our readthedocs  documentation.", 
            "title": "UL HPC Tutorial: Build software with EasyBuild on UL HPC platform"
        }, 
        {
            "location": "/advanced/EasyBuild/README/#short-introduction-to-easybuild", 
            "text": "EasyBuild is a tool that allows to perform automated and reproducible compilation and installation of software. A large number of scientific software are supported (479 software packages in the last release).  All builds and installations are performed at user level, so you don't need the admin rights. \nThe software are installed in your home directory (by default in  $HOME/.local/easybuild/software/ ) and a module file is generated (by default in  $HOME/.local/easybuild/modules/ ) to use the software.  EasyBuild relies on two main concepts:  Toolchains  and  EasyConfig file .  A  toolchain  corresponds to a compiler and a set of libraries which are commonly used to build a software. The two main toolchains frequently used on the UL HPC platform are the GOOLF and the ICTCE toolchains. GOOLF is based on the GCC compiler and on open-source libraries (OpenMPI, OpenBLAS, etc.). ICTCE is based on the Intel compiler and on Intel libraries (Intel MPI, Intel Math Kernel Library, etc.).   An  EasyConfig file  is a simple text file that describes the build process of a software. For most software that uses standard procedure (like  configure ,  make  and  make install ), this file is very simple. Many EasyConfig files are already provided with EasyBuild.  EasyConfig files and generated modules are named using the following convention: Software-Name - Software-Version - Toolchain-Name - Toolchain-Version    Additional details are available on EasyBuild website:   EasyBuild homepage  EasyBuild documentation  What is EasyBuild?  Toolchains  EasyConfig files  List of supported software packages", 
            "title": "Short introduction to EasyBuild"
        }, 
        {
            "location": "/advanced/EasyBuild/README/#easybuild-on-ul-hpc-platform", 
            "text": "To use EasyBuild on a compute node, load the EasyBuild module:  $  module avail EasyBuild\n\n------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools -------------\n    tools/EasyBuild/2.0.0\n\n------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/base -------------\n    base/EasyBuild/install-2.1.0\n\n$  module load base/EasyBuild/install-2.1.0  The EasyBuild command is  eb . Check the version you have loaded:  $  eb --version\n\nThis is EasyBuild 2.1.0dev-r6fee583a88e99d1384314790a419c83e85f18f3d (framework: 2.1.0dev-r2aa673bb5f61cb2d65e4a3037cc2337e6df2d3e6, easyblocks: 2.1.0dev-r6fee583a88e99d1384314790a419c83e85f18f3d) on host h-cluster1-11.  Note that this version number is a bit peculiar because this is a custom installation on the cluster.  To get help on the EasyBuild options, use the  -h  or  -H  option flags:  $  eb -h\n$  eb -H", 
            "title": "EasyBuild on UL HPC platform"
        }, 
        {
            "location": "/advanced/EasyBuild/README/#build-software-using-provided-easyconfig-file", 
            "text": "In this part, we propose to be High Performance Linpack (HPL) using EasyBuild. \nHPL is supported by EasyBuild, this means that an EasyConfig file allowing to build HPL is already provided with EasyBuild.  First, let's see which HPL are available on the cluster:  $  module avail HPL\n\n------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools -------------\n    tools/HPL/2.0-goolf-1.4.10  Then, search for available EasyConfig files with HPL in their name. The EasyConfig files are named with the  .eb  extension.  $  eb -S HPL\n\n== temporary log file in case of crash /tmp/eb-p2DT7H/easybuild-ligIot.log\n== Searching (case-insensitive) for 'HPL' in /opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs \nCFGS1=/opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs/h/HPL\n * $CFGS1/HPL-2.0-cgmpolf-1.1.6.eb\n * $CFGS1/HPL-2.0-cgmvolf-1.1.12rc1.eb\n * $CFGS1/HPL-2.0-cgmvolf-1.2.7.eb\n * $CFGS1/HPL-2.0-cgoolf-1.1.7.eb\n * $CFGS1/HPL-2.0-foss-2014b.eb\n * $CFGS1/HPL-2.0-goalf-1.1.0-no-OFED.eb\n * $CFGS1/HPL-2.0-goolf-1.4.10.eb\n * $CFGS1/HPL-2.0-goolf-1.5.16.eb\n * $CFGS1/HPL-2.0-ictce-4.0.6.eb\n * $CFGS1/HPL-2.0-ictce-5.3.0.eb\n * $CFGS1/HPL-2.0-ictce-6.0.5.eb\n * $CFGS1/HPL-2.0-ictce-6.1.5.eb\n * $CFGS1/HPL-2.0-iomkl-4.6.13.eb\n * $CFGS1/HPL-2.1-foss-2015a.eb\n * $CFGS1/HPL-2.1-gimkl-1.5.9.eb\n * $CFGS1/HPL-2.1-gmpolf-1.4.8.eb\n * $CFGS1/HPL-2.1-gmvolf-1.7.20.eb\n * $CFGS1/HPL-2.1-goolf-1.7.20.eb\n * $CFGS1/HPL-2.1-goolfc-1.4.10.eb\n * $CFGS1/HPL-2.1-goolfc-2.6.10.eb\n * $CFGS1/HPL-2.1-gpsolf-2014.12.eb\n * $CFGS1/HPL-2.1-ictce-6.3.5.eb\n * $CFGS1/HPL-2.1-ictce-7.1.2.eb\n * $CFGS1/HPL-2.1-intel-2014.10.eb\n * $CFGS1/HPL-2.1-intel-2014.11.eb\n * $CFGS1/HPL-2.1-intel-2014b.eb\n * $CFGS1/HPL-2.1-intel-2015.02.eb\n * $CFGS1/HPL-2.1-intel-2015a.eb\n * $CFGS1/HPL-2.1-intel-para-2014.12.eb\n * $CFGS1/HPL-2.1-iomkl-2015.01.eb\n * $CFGS1/HPL-2.1-iomkl-2015.02.eb\n * $CFGS1/HPL_parallel-make.patch\n== temporary log file(s) /tmp/eb-p2DT7H/easybuild-ligIot.log* have been removed.\n== temporary directory /tmp/eb-p2DT7H has been removed.  If we try to build  HPL-2.0-goolf-1.4.10 , nothing will be done as it is already installed on the cluster.  $  eb HPL-2.0-goolf-1.4.10.eb\n\n== temporary log file in case of crash /tmp/eb-JKadCH/easybuild-SoXdix.log\n== tools/HPL/2.0-goolf-1.4.10 is already installed (module found), skipping\n== No easyconfigs left to be built.\n== Build succeeded for 0 out of 0\n== temporary log file(s) /tmp/eb-JKadCH/easybuild-SoXdix.log* have been removed.\n== temporary directory /tmp/eb-JKadCH has been removed.  However the build can be forced using the  -f  option flag. Then this software will be re-built.\n(Tip: prefix your command with  time  to know its duration)  $  time eb HPL-2.0-goolf-1.4.10.eb -f\n\n== temporary log file in case of crash /tmp/eb-FAO8AO/easybuild-ea15Cq.log\n== processing EasyBuild easyconfig /opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs/h/HPL/HPL-2.0-goolf-1.4.10.eb\n== building and installing tools/HPL/2.0-goolf-1.4.10...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/tools/HPL/2.0-goolf-1.4.10/easybuild/easybuild-HPL-2.0-20150624.113223.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-FAO8AO/easybuild-ea15Cq.log* have been removed.\n== temporary directory /tmp/eb-FAO8AO has been removed.\n\nreal    1m10.619s\nuser    0m49.387s\nsys     0m7.828s  Let's have a look at  HPL-2.0-ictce-5.3.0  which is not installed yet. \nWe can check if a software and its dependencies are installed using the  -Dr  option flag:  $  eb HPL-2.0-ictce-5.3.0.eb -Dr\n\n== temporary log file in case of crash /tmp/eb-HlZDMR/easybuild-JbndYN.log\nDry run: printing build status of easyconfigs and dependencies\nCFGS=/opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs\n * [x] $CFGS/i/icc/icc-2013.3.163.eb (module: compiler/icc/2013.3.163)\n * [x] $CFGS/i/ifort/ifort-2013.3.163.eb (module: compiler/ifort/2013.3.163)\n * [x] $CFGS/i/iccifort/iccifort-2013.3.163.eb (module: toolchain/iccifort/2013.3.163)\n * [x] $CFGS/i/impi/impi-4.1.0.030-iccifort-2013.3.163.eb (module: mpi/impi/4.1.0.030-iccifort-2013.3.163)\n * [x] $CFGS/i/iimpi/iimpi-5.3.0.eb (module: toolchain/iimpi/5.3.0)\n * [x] $CFGS/i/imkl/imkl-11.0.3.163-iimpi-5.3.0.eb (module: numlib/imkl/11.0.3.163-iimpi-5.3.0)\n * [x] $CFGS/i/ictce/ictce-5.3.0.eb (module: toolchain/ictce/5.3.0)\n * [ ] $CFGS/h/HPL/HPL-2.0-ictce-5.3.0.eb (module: tools/HPL/2.0-ictce-5.3.0)\n== temporary log file(s) /tmp/eb-HlZDMR/easybuild-JbndYN.log* have been removed.\n== temporary directory /tmp/eb-HlZDMR has been removed.  HPL-2.0-ictce-5.3.0  is not available but all it dependencies are. Let's build it:  $  time eb HPL-2.0-ictce-5.3.0.eb\n\n== temporary log file in case of crash /tmp/eb-UFlEv7/easybuild-uVbm24.log\n== processing EasyBuild easyconfig /opt/apps/resif/devel/v1.1-20150414/.installRef/easybuild-easyconfigs/easybuild/easyconfigs/h/HPL/HPL-2.0-ictce-5.3.0.eb\n== building and installing tools/HPL/2.0-ictce-5.3.0...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/tools/HPL/2.0-ictce-5.3.0/easybuild/easybuild-HPL-2.0-20150624.113547.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-UFlEv7/easybuild-uVbm24.log* have been removed.\n== temporary directory /tmp/eb-UFlEv7 has been removed.\n\nreal    1m25.849s\nuser    0m49.039s\nsys     0m10.961s  To see the newly installed modules, you need to add the path where they were installed to the MODULEPATH. On the cluster you have to use the  module use  command:  $  module use $HOME/.local/easybuild/modules/all/  Check which HPL modules are available now:  $  module avail HPL\n\n------------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all -------------\n    tools/HPL/2.0-goolf-1.4.10    tools/HPL/2.0-ictce-5.3.0 (D)\n\n---------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools ----------------\n    tools/HPL/2.0-goolf-1.4.10  The two newly-built versions of HPL are now available for your user. You can use them with the usually  module load  command.", 
            "title": "Build software using provided EasyConfig file"
        }, 
        {
            "location": "/advanced/EasyBuild/README/#amending-an-existing-easyconfig-file", 
            "text": "It is possible to amend existing EasyConfig file to build software with slightly different parameters.   As a example, we are going to build the lastest version of HPL (2.1) with ICTCE toolchain. We use the  --try-software-version  option flag to overide the HPL version.  $  time eb HPL-2.0-ictce-5.3.0.eb --try-software-version=2.1\n\n== temporary log file in case of crash /tmp/eb-ocChbK/easybuild-liMmlk.log\n== processing EasyBuild easyconfig /tmp/eb-ocChbK/tweaked_easyconfigs/HPL-2.1-ictce-5.3.0.eb\n== building and installing tools/HPL/2.1-ictce-5.3.0...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/tools/HPL/2.1-ictce-5.3.0/easybuild/easybuild-HPL-2.1-20150624.114243.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-ocChbK/easybuild-liMmlk.log* have been removed.\n== temporary directory /tmp/eb-ocChbK has been removed.\n\nreal    1m24.933s\nuser    0m53.167s\nsys     0m11.533s\n\n$  module avail HPL\n\n------------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all -------------\n    tools/HPL/2.0-goolf-1.4.10    tools/HPL/2.0-ictce-5.3.0    tools/HPL/2.1-ictce-5.3.0 (D)\n\n---------------- /opt/apps/resif/devel/v1.1-20150414/core/modules/tools ----------------\n    tools/HPL/2.0-goolf-1.4.10  We obtained HPL 2.1 without writing any EasyConfig file.  There are multiple ways to amend a EasyConfig file. Check the  --try-*  option flags for all the possibilities.", 
            "title": "Amending an existing EasyConfig file"
        }, 
        {
            "location": "/advanced/EasyBuild/README/#build-software-using-your-own-easyconfig-file", 
            "text": "For this example, we create an EasyConfig file to build GZip 1.4 with the GOOLF toolchain.\nOpen your favorite editor and create a file named  gzip-1.4-goolf-1.4.10.eb  with the following content:  easyblock = 'ConfigureMake'\n\nname = 'gzip'\nversion = '1.4'\n\nhomepage = 'http://www.gnu.org/software/gzip/'\ndescription = \"gzip (GNU zip) is a popular data compression program as a replacement for compress\"\n\n# use the GOOLF toolchain\ntoolchain = {'name': 'goolf', 'version': '1.4.10'}\n\n# specify that GCC compiler should be used to build gzip\npreconfigopts = \"CC='gcc'\"\n\n# source tarball filename\nsources = ['%s-%s.tar.gz'%(name,version)]\n\n# download location for source files\nsource_urls = ['http://ftpmirror.gnu.org/gzip']\n\n# make sure the gzip and gunzip binaries are available after installation\nsanity_check_paths = {\n                      'files': [\"bin/gunzip\", \"bin/gzip\"],\n                      'dirs': []\n                     }\n\n# run 'gzip -h' and 'gzip --version' after installation\nsanity_check_commands = [True, ('gzip', '--version')]  This is a simple EasyConfig. Most of the fields are self-descriptive. No build method is explicitely defined, so it uses by default the standard  configure/make/make install  approach.  Let's build GZip with this EasyConfig file:  $  time eb gzip-1.4-goolf-1.4.10.eb\n\n== temporary log file in case of crash /tmp/eb-hiyyN1/easybuild-ynLsHC.log\n== processing EasyBuild easyconfig /mnt/nfs/users/homedirs/mschmitt/gzip-1.4-goolf-1.4.10.eb\n== building and installing base/gzip/1.4-goolf-1.4.10...\n== fetching files...\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== building...\n== testing...\n== installing...\n== taking care of extensions...\n== packaging...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== COMPLETED: Installation ended successfully\n== Results of the build can be found in the log file /home/users/mschmitt/.local/easybuild/software/base/gzip/1.4-goolf-1.4.10/easybuild/easybuild-gzip-1.4-20150624.114745.log\n== Build succeeded for 1 out of 1\n== temporary log file(s) /tmp/eb-hiyyN1/easybuild-ynLsHC.log* have been removed.\n== temporary directory /tmp/eb-hiyyN1 has been removed.\n\nreal    1m39.982s\nuser    0m52.743s\nsys     0m11.297s  We can now check that our version of GZip is available via the modules:  $  module avail gzip\n\n--------- /mnt/nfs/users/homedirs/mschmitt/.local/easybuild/modules/all ---------\n    base/gzip/1.4-goolf-1.4.10", 
            "title": "Build software using your own EasyConfig file"
        }, 
        {
            "location": "/advanced/EasyBuild/README/#to-go-further", 
            "text": "EasyBuild homepage  EasyBuild documentation  Getting started  Using EasyBuild  Step-by-step guide", 
            "title": "To go further"
        }, 
        {
            "location": "/advanced/OSU_MicroBenchmarks/README/", 
            "text": "-\n- mode: markdown; mode: auto-fill; fill-column: 80 -\n-\n\n\nREADME.md\n\n\nCopyright (c) 2013 Sebastien Varrette \n\n\n    Time-stamp: \nMer 2014-05-07 17:22 svarrette\n\n\n\n\n\n\nUL HPC Tutorial: OSU Micro-Benchmarks on UL HPC platform\n\n\nThe objective of this tutorial is to compile and run on of the \nOSU micro-benchmarks\n which permit to measure the performance of an MPI implementation.  \n\n\nYou can work in groups for this training, yet individual work is encouraged to\nensure you understand and practice the usage of an HPC platform.  \n\n\nIn all cases, ensure you are able to \nconnect to the chaos and gaia cluster\n. \n\n\n/!\\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A COMPUTING NODE\n\n(access)$\n  oarsub -I -l enclosure=1/nodes=2,walltime=4\n\n\n\nAdvanced users only\n: rely on \nscreen\n (see\n  \ntutorial\n) on the frontend\n  prior to running any \noarsub\n command to be more resilient to disconnection.  \n\n\nThe latest version of this tutorial is available on\n\nGithub\n\n\nObjectives\n\n\nThe \nOSU micro-benchmarks\n feature a series of MPI benchmarks that measure the performances of various MPI operations: \n\n\n\n\nPoint-to-Point MPI Benchmarks\n: Latency, multi-threaded latency, multi-pair latency, multiple bandwidth / message rate test bandwidth, bidirectional bandwidth\n\n\nCollective MPI Benchmarks\n: Collective latency tests for various MPI collective operations such as MPI_Allgather, MPI_Alltoall, MPI_Allreduce, MPI_Barrier, MPI_Bcast, MPI_Gather, MPI_Reduce, MPI_Reduce_Scatter, MPI_Scatter and vector collectives.\n\n\nOne-sided MPI Benchmarks\n: one-sided put latency (active/passive), one-sided put bandwidth (active/passive), one-sided put bidirectional bandwidth, one-sided get latency (active/passive), one-sided get bandwidth (active/passive), one-sided accumulate latency (active/passive), compare and swap latency (passive), and fetch and operate (passive) for MVAPICH2 (MPI-2 and MPI-3).\n\n\n\n\nThe 4.3 version also features OpenSHMEM benchmarks, a 1-sided communications library.\n\n\nIn this tutorial, we will focus on two of the available tests:\n\n\n\n\nosu_get_latency\n - Latency Test\n\n\n\n\nThe latency tests are carried out in a ping-pong fashion. The sender sends a message with a certain data size to the receiver and waits for a reply from the receiver. The receiver receives the message from the sender and sends back a reply with the same data size. Many iterations of this ping-pong test are carried out and average one-way latency numbers are obtained. Blocking version of MPI functions (MPI_Send and MPI_Recv) are used in the tests.\n* \nosu_get_bw\n - Bandwidth Test\n\n\nThe bandwidth tests were carried out by having the sender sending out a fixed number (equal to the window size) of back-to-back messages to the receiver and then waiting for a reply from the receiver. The receiver sends the reply only after receiving all these messages. This process is repeated for several iterations and the bandwidth is calculated based on the elapsed time (from the time sender sends the first message until the time it receives the reply back from the receiver) and the number of bytes sent by the sender. The objective of this bandwidth test is to determine the maximum sustained date rate that can be achieved at the network level. Thus, non-blocking version of MPI functions (MPI_Isend and MPI_Irecv) were used in the test.\n\n\nThe idea is to compare the different MPI implementations available on the \nUL HPC platform\n.: \n\n\n\n\nIntel MPI\n\n\nOpenMPI\n\n\nMVAPICH2\n\n\n\n\nThe benchamrking campain will typically involves for each MPI suit: \n\n\n\n\ntwo nodes, belonging to the same enclosure\n\n\ntwo nodes, belonging to different enclosures\n\n\n\n\nPre-requisites\n\n\nClone the \nlauncher-script repository\n\n\n$\n cd \n$\n mkdir -p git/ULHPC \n cd  git/ULHPC\n$\n git clone https://github.com/ULHPC/launcher-scripts.git\n\n\n\nNow you shall get the 4.3 release of the \nOSU micro-benchmarks\n\n\n$\n mkdir ~/TP \n$\n cd ~/TP\n$\n wget --no-check-certificate https://scm.mvapich.cse.ohio-state.edu/benchmarks/osu-micro-benchmarks-4.3.tar.gz\n$\n tar xvzf osu-micro-benchmarks-4.3.tar.gz\n$\n cd osu-micro-benchmarks-4.3\n\n\n\nOSU Micro-benchmarks with Intel MPI\n\n\nWe are first going to use the\n\nIntel Cluster Toolkit Compiler Edition\n,\nwhich provides Intel C/C++ and Fortran compilers, Intel MPI. \n\n\nWe will compile the \nOSU micro-benchmarks\n in a specific directory (that a good habbit)\n\n\n$\n cd ~/TP/osu-micro-benchmarks-4.3\n$\n module avail 2\n1 | grep -i MPI\n$\n module load ictce/6.1.5\n$\n module list\nCurrently Loaded Modulefiles:\n   1) icc/2013_sp1.1.106     2) ifort/2013_sp1.1.106   3) impi/4.1.3.045         4) imkl/11.1.1.106        5) ictce/6.1.5\n$\n mkdir build.impi \n cd build.impi\n$\n ../configure CC=mpiicc --prefix=`pwd`/install\n$\n make \n make install\n\n\n\nIf everything goes fine, you shall have the \nOSU micro-benchmarks\n installed in the directory \ninstall/libexec/osu-micro-benchmarks/mpi/\n.\n\n\nOnce compiled, ensure you are able to run it: \n\n\n$\n cd install/libexec/osu-micro-benchmarks/mpi/one-sided/\n$\n mpirun -hostfile $OAR_NODEFILE -perhost 1 ./osu_get_latency\n$\n mpirun -hostfile $OAR_NODEFILE -perhost 1 ./osu_get_bw\n\n\n\nNow you can use the \nMPI generic launcher\n to run the code: \n\n\n$\n cd ~/TP/osu-micro-benchmarks-4.3/\n$\n mkdir runs  \n cd runs\n$\n ln -s ~/git/ULHPC/launcher-scripts/bash/MPI/mpi_launcher.sh launcher_osu_impi\n$\n ./launcher_osu_impi --basedir $HOME/TP/osu-micro-benchmarks-4.3/build.impi/install/libexec/osu-micro-benchmarks/mpi/one-sided --npernode 1 --module ictce/6.1.5 --exe osu_get_latency,osu_get_bw\n\n\n\nIf you want to avoid this long list of arguments, just create a file \nlauncher_osu_impi.default.conf\n to contain: \n\n\n$\n cat launcher_osu_impi.default.conf # this command will fail if you have not already created the file !\n# Defaults settings for running the OSU Micro benchmarks compiled with Intel MPI\nNAME=impi\n\nMODULE_TO_LOADstr=ictce/6.1.5\nMPI_PROG_BASEDIR=$HOME/TP/osu-micro-benchmarks-4.3/build.impi/install/libexec/osu-micro-benchmarks/mpi/one-sided/\n\nMPI_PROGstr=osu_get_latency,osu_get_bw\nMPI_NPERNODE=1\n\n\n\nNow you can run the launcher script interactively.\n\n\n$\n ./launcher_osu_impi\n\n\n\nYou might want also to host the output files in the local directory (under the date)\n\n\n$\n ./launcher_osu_impi --datadir data/`date +%Y-%m-%d`\n\n\n\nOSU Micro-benchmarks with OpenMPI\n\n\nWe will repeat the procedure, this time using OpenMPI. \n\n\n$\n cd ~/TP/osu-micro-benchmarks-4.3/\n$\n module purge\n$\n module load OpenMPI/1.7.3-GCC-4.8.2\n$\n mkdir build.openmpi \n cd build.openmpi\n$\n ../configure CC=mpicc --prefix=`pwd`/install\n$\n make \n make install\n\n\n\nIf everything goes fine, you shall have the \nOSU micro-benchmarks\n installed in the directory \ninstall/libexec/osu-micro-benchmarks/mpi/\n.\n\n\nOnce compiled, ensure you are able to run it: \n\n\n$\n cd install/libexec/osu-micro-benchmarks/mpi/one-sided/\n$\n mpirun -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -npernode 1 ./osu_get_latency\n$\n mpirun -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -npernode 1 ./osu_get_bw\n\n\n\nAgain, we will rely on the \nMPI generic launcher\n to run the code: \n\n\n$\n cd ~/TP/osu-micro-benchmarks-4.3/runs\n$\n ln -s ~/git/ULHPC/launcher-scripts/bash/MPI/mpi_launcher.sh launcher_osu_openmpi\n$\n cat launcher_osu_openmpi.default.conf  # this command will fail if you have not already created the file !\n# Defaults settings for running the OSU Micro benchmarks wompiled with OpenMPI\nNAME=openmpi\n\nMODULE_TO_LOADstr=OpenMPI/1.7.3-GCC-4.8.2\nMPI_PROG_BASEDIR=$HOME/TP/osu-micro-benchmarks-4.3/build.openmpi/install/libexec/osu-micro-benchmarks/mpi/one-sided/\n\nMPI_PROGstr=osu_get_latency,osu_get_bw\nMPI_NPERNODE=1\n\n\n\nNow you can run the launcher script interactively.\n\n\n$\n ./launcher_osu_openmpi\n\n\n\nYou might want also to host the output files in the local directory (under the date)\n\n\n$\n ./launcher_osu_openmpi --datadir data/`date +%Y-%m-%d`\n\n\n\nOSU Micro-benchmarks with MVAPICH2\n\n\nRepeat the procedure, this time using MVAPICH2. \n\n\n$\n cd ~/TP/osu-micro-benchmarks-4.3/\n$\n module purge\n$\n module load MVAPICH2/1.7-GCC-4.6.3\n$\n mkdir build.mvapich2 \n cd build.mvapich2\n$\n ../configure CC=mpicc --prefix=`pwd`/install\n$\n make \n make install\n\n\n\nIf everything goes fine, you shall have the \nOSU micro-benchmarks\n installed in the directory \ninstall/libexec/osu-micro-benchmarks/mpi/\n.\n\n\nAs before, rely on the \nMPI generic launcher\n to run the code: \n\n\n$\n cd ~/TP/osu-micro-benchmarks-4.3/runs\n$\n ln -s ~/git/ULHPC/launcher-scripts/bash/MPI/mpi_launcher.sh launcher_osu_mvapich2\n$\n cat launcher_osu_mvapich2.default.conf # this command will fail if you have not already created the file !\n# Defaults settings for running the OSU Micro benchmarks wompiled with MVAPICH2\nNAME=mvapich2\n\nMODULE_TO_LOADstr=MVAPICH2/1.7-GCC-4.6.3\nMPI_PROG_BASEDIR=$HOME/TP/osu-micro-benchmarks-4.3/build.mvapich2/install/libexec/osu-micro-benchmarks/mpi/one-sided/\n\nMPI_PROGstr=osu_get_latency,osu_get_bw\nMPI_NPERNODE=1\n\n\n\nNow you can run the launcher script interactively.\n\n\n$\n ./launcher_osu_mvapich2\n\n\n\nYou might want also to host the output files in the local directory (under the date)\n\n\n$\n ./launcher_osu_mvapich2 --datadir data/`date +%Y-%m-%d`\n\n\n\nBenchmarking on two nodes\n\n\nOperate the benchmarking campain (in the three cases) in the following context: \n\n\n\n\n\n\n2 nodes belonging to the same enclosure. Use for that:\n\n\n$\n oarsub -l enclosure=1/nodes=2,walltime=8 [\u2026]\n\n\n\n\n\n\n\n2 nodes belonging to the different enclosures:\n\n\n$\n oarsub -l enclosure=2/core=1,walltime=8 [\u2026]\n\n\n\n\n\n\n\nNow for Lazy / frustrated persons\n\n\nYou will find in the \nUL HPC tutorial\n\nrepository, under the \nadvanced/OSU_MicroBenchmarks\n directory, a set of tools / script that\nfacilitate the running and analysis of this tutorial that you can use/adapt to\nsuit your needs. \n\n\nIn particular, once in the \nadvanced/OSU_MicroBenchmarks\n directory: \n\n\n\n\nrunning \nmake fetch\n will automatically download the archives for the \nOSU micro-benchmarks\n in the \nsrc/\n directory\n\n\nyou will find the patch file to apply to the version 4.3 in \nsrc/osu-micro-benchmarks-4.3/mpi/one-sided/Makefile.am.patch\n\n\nThe different configuration files for the \nMPI generic launcher\n in \nruns/\n\n\nSome sample output data in \nruns/data/\n\n\nrun \nmake build\n to build the different versions of the OSU Micro-benchmarks\n\n\nrun \nmake run_interactive\n to run the benchmarks, assuming you are in an interactive job\n\n\nrun \nmake run\n to run a passive job executing the benchmarks\n\n\nrun \nmake plot\n to invoke the \nGnuplot\n script\n  \nplots/benchmark_OSU.gnuplot\n and generate various plots from the sample\n  runs. \n\n\n\n\nIn particular, you'll probably want to see the comparison figure extracted from\nthe sample run in \nplots/benchmark_OSU_2H_latency.pdf\n and \nplots/benchmark_OSU_2H_bandwidth.pdf\n\n\nA PNG version of these plots is available on Github: \n\nOSU latency\n -- \nOSU Bandwidth", 
            "title": "The OSU Micro-benchmarks"
        }, 
        {
            "location": "/advanced/OSU_MicroBenchmarks/README/#ul-hpc-tutorial-osu-micro-benchmarks-on-ul-hpc-platform", 
            "text": "The objective of this tutorial is to compile and run on of the  OSU micro-benchmarks  which permit to measure the performance of an MPI implementation.    You can work in groups for this training, yet individual work is encouraged to\nensure you understand and practice the usage of an HPC platform.    In all cases, ensure you are able to  connect to the chaos and gaia cluster .   /!\\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A COMPUTING NODE\n\n(access)$   oarsub -I -l enclosure=1/nodes=2,walltime=4  Advanced users only : rely on  screen  (see\n   tutorial ) on the frontend\n  prior to running any  oarsub  command to be more resilient to disconnection.    The latest version of this tutorial is available on Github", 
            "title": "UL HPC Tutorial: OSU Micro-Benchmarks on UL HPC platform"
        }, 
        {
            "location": "/advanced/OSU_MicroBenchmarks/README/#objectives", 
            "text": "The  OSU micro-benchmarks  feature a series of MPI benchmarks that measure the performances of various MPI operations:    Point-to-Point MPI Benchmarks : Latency, multi-threaded latency, multi-pair latency, multiple bandwidth / message rate test bandwidth, bidirectional bandwidth  Collective MPI Benchmarks : Collective latency tests for various MPI collective operations such as MPI_Allgather, MPI_Alltoall, MPI_Allreduce, MPI_Barrier, MPI_Bcast, MPI_Gather, MPI_Reduce, MPI_Reduce_Scatter, MPI_Scatter and vector collectives.  One-sided MPI Benchmarks : one-sided put latency (active/passive), one-sided put bandwidth (active/passive), one-sided put bidirectional bandwidth, one-sided get latency (active/passive), one-sided get bandwidth (active/passive), one-sided accumulate latency (active/passive), compare and swap latency (passive), and fetch and operate (passive) for MVAPICH2 (MPI-2 and MPI-3).   The 4.3 version also features OpenSHMEM benchmarks, a 1-sided communications library.  In this tutorial, we will focus on two of the available tests:   osu_get_latency  - Latency Test   The latency tests are carried out in a ping-pong fashion. The sender sends a message with a certain data size to the receiver and waits for a reply from the receiver. The receiver receives the message from the sender and sends back a reply with the same data size. Many iterations of this ping-pong test are carried out and average one-way latency numbers are obtained. Blocking version of MPI functions (MPI_Send and MPI_Recv) are used in the tests.\n*  osu_get_bw  - Bandwidth Test  The bandwidth tests were carried out by having the sender sending out a fixed number (equal to the window size) of back-to-back messages to the receiver and then waiting for a reply from the receiver. The receiver sends the reply only after receiving all these messages. This process is repeated for several iterations and the bandwidth is calculated based on the elapsed time (from the time sender sends the first message until the time it receives the reply back from the receiver) and the number of bytes sent by the sender. The objective of this bandwidth test is to determine the maximum sustained date rate that can be achieved at the network level. Thus, non-blocking version of MPI functions (MPI_Isend and MPI_Irecv) were used in the test.  The idea is to compare the different MPI implementations available on the  UL HPC platform .:    Intel MPI  OpenMPI  MVAPICH2   The benchamrking campain will typically involves for each MPI suit:    two nodes, belonging to the same enclosure  two nodes, belonging to different enclosures", 
            "title": "Objectives"
        }, 
        {
            "location": "/advanced/OSU_MicroBenchmarks/README/#pre-requisites", 
            "text": "Clone the  launcher-script repository  $  cd \n$  mkdir -p git/ULHPC   cd  git/ULHPC\n$  git clone https://github.com/ULHPC/launcher-scripts.git  Now you shall get the 4.3 release of the  OSU micro-benchmarks  $  mkdir ~/TP \n$  cd ~/TP\n$  wget --no-check-certificate https://scm.mvapich.cse.ohio-state.edu/benchmarks/osu-micro-benchmarks-4.3.tar.gz\n$  tar xvzf osu-micro-benchmarks-4.3.tar.gz\n$  cd osu-micro-benchmarks-4.3", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/advanced/OSU_MicroBenchmarks/README/#osu-micro-benchmarks-with-intel-mpi", 
            "text": "We are first going to use the Intel Cluster Toolkit Compiler Edition ,\nwhich provides Intel C/C++ and Fortran compilers, Intel MPI.   We will compile the  OSU micro-benchmarks  in a specific directory (that a good habbit)  $  cd ~/TP/osu-micro-benchmarks-4.3\n$  module avail 2 1 | grep -i MPI\n$  module load ictce/6.1.5\n$  module list\nCurrently Loaded Modulefiles:\n   1) icc/2013_sp1.1.106     2) ifort/2013_sp1.1.106   3) impi/4.1.3.045         4) imkl/11.1.1.106        5) ictce/6.1.5\n$  mkdir build.impi   cd build.impi\n$  ../configure CC=mpiicc --prefix=`pwd`/install\n$  make   make install  If everything goes fine, you shall have the  OSU micro-benchmarks  installed in the directory  install/libexec/osu-micro-benchmarks/mpi/ .  Once compiled, ensure you are able to run it:   $  cd install/libexec/osu-micro-benchmarks/mpi/one-sided/\n$  mpirun -hostfile $OAR_NODEFILE -perhost 1 ./osu_get_latency\n$  mpirun -hostfile $OAR_NODEFILE -perhost 1 ./osu_get_bw  Now you can use the  MPI generic launcher  to run the code:   $  cd ~/TP/osu-micro-benchmarks-4.3/\n$  mkdir runs    cd runs\n$  ln -s ~/git/ULHPC/launcher-scripts/bash/MPI/mpi_launcher.sh launcher_osu_impi\n$  ./launcher_osu_impi --basedir $HOME/TP/osu-micro-benchmarks-4.3/build.impi/install/libexec/osu-micro-benchmarks/mpi/one-sided --npernode 1 --module ictce/6.1.5 --exe osu_get_latency,osu_get_bw  If you want to avoid this long list of arguments, just create a file  launcher_osu_impi.default.conf  to contain:   $  cat launcher_osu_impi.default.conf # this command will fail if you have not already created the file !\n# Defaults settings for running the OSU Micro benchmarks compiled with Intel MPI\nNAME=impi\n\nMODULE_TO_LOADstr=ictce/6.1.5\nMPI_PROG_BASEDIR=$HOME/TP/osu-micro-benchmarks-4.3/build.impi/install/libexec/osu-micro-benchmarks/mpi/one-sided/\n\nMPI_PROGstr=osu_get_latency,osu_get_bw\nMPI_NPERNODE=1  Now you can run the launcher script interactively.  $  ./launcher_osu_impi  You might want also to host the output files in the local directory (under the date)  $  ./launcher_osu_impi --datadir data/`date +%Y-%m-%d`", 
            "title": "OSU Micro-benchmarks with Intel MPI"
        }, 
        {
            "location": "/advanced/OSU_MicroBenchmarks/README/#osu-micro-benchmarks-with-openmpi", 
            "text": "We will repeat the procedure, this time using OpenMPI.   $  cd ~/TP/osu-micro-benchmarks-4.3/\n$  module purge\n$  module load OpenMPI/1.7.3-GCC-4.8.2\n$  mkdir build.openmpi   cd build.openmpi\n$  ../configure CC=mpicc --prefix=`pwd`/install\n$  make   make install  If everything goes fine, you shall have the  OSU micro-benchmarks  installed in the directory  install/libexec/osu-micro-benchmarks/mpi/ .  Once compiled, ensure you are able to run it:   $  cd install/libexec/osu-micro-benchmarks/mpi/one-sided/\n$  mpirun -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -npernode 1 ./osu_get_latency\n$  mpirun -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE -npernode 1 ./osu_get_bw  Again, we will rely on the  MPI generic launcher  to run the code:   $  cd ~/TP/osu-micro-benchmarks-4.3/runs\n$  ln -s ~/git/ULHPC/launcher-scripts/bash/MPI/mpi_launcher.sh launcher_osu_openmpi\n$  cat launcher_osu_openmpi.default.conf  # this command will fail if you have not already created the file !\n# Defaults settings for running the OSU Micro benchmarks wompiled with OpenMPI\nNAME=openmpi\n\nMODULE_TO_LOADstr=OpenMPI/1.7.3-GCC-4.8.2\nMPI_PROG_BASEDIR=$HOME/TP/osu-micro-benchmarks-4.3/build.openmpi/install/libexec/osu-micro-benchmarks/mpi/one-sided/\n\nMPI_PROGstr=osu_get_latency,osu_get_bw\nMPI_NPERNODE=1  Now you can run the launcher script interactively.  $  ./launcher_osu_openmpi  You might want also to host the output files in the local directory (under the date)  $  ./launcher_osu_openmpi --datadir data/`date +%Y-%m-%d`", 
            "title": "OSU Micro-benchmarks with OpenMPI"
        }, 
        {
            "location": "/advanced/OSU_MicroBenchmarks/README/#osu-micro-benchmarks-with-mvapich2", 
            "text": "Repeat the procedure, this time using MVAPICH2.   $  cd ~/TP/osu-micro-benchmarks-4.3/\n$  module purge\n$  module load MVAPICH2/1.7-GCC-4.6.3\n$  mkdir build.mvapich2   cd build.mvapich2\n$  ../configure CC=mpicc --prefix=`pwd`/install\n$  make   make install  If everything goes fine, you shall have the  OSU micro-benchmarks  installed in the directory  install/libexec/osu-micro-benchmarks/mpi/ .  As before, rely on the  MPI generic launcher  to run the code:   $  cd ~/TP/osu-micro-benchmarks-4.3/runs\n$  ln -s ~/git/ULHPC/launcher-scripts/bash/MPI/mpi_launcher.sh launcher_osu_mvapich2\n$  cat launcher_osu_mvapich2.default.conf # this command will fail if you have not already created the file !\n# Defaults settings for running the OSU Micro benchmarks wompiled with MVAPICH2\nNAME=mvapich2\n\nMODULE_TO_LOADstr=MVAPICH2/1.7-GCC-4.6.3\nMPI_PROG_BASEDIR=$HOME/TP/osu-micro-benchmarks-4.3/build.mvapich2/install/libexec/osu-micro-benchmarks/mpi/one-sided/\n\nMPI_PROGstr=osu_get_latency,osu_get_bw\nMPI_NPERNODE=1  Now you can run the launcher script interactively.  $  ./launcher_osu_mvapich2  You might want also to host the output files in the local directory (under the date)  $  ./launcher_osu_mvapich2 --datadir data/`date +%Y-%m-%d`", 
            "title": "OSU Micro-benchmarks with MVAPICH2"
        }, 
        {
            "location": "/advanced/OSU_MicroBenchmarks/README/#benchmarking-on-two-nodes", 
            "text": "Operate the benchmarking campain (in the three cases) in the following context:     2 nodes belonging to the same enclosure. Use for that:  $  oarsub -l enclosure=1/nodes=2,walltime=8 [\u2026]    2 nodes belonging to the different enclosures:  $  oarsub -l enclosure=2/core=1,walltime=8 [\u2026]", 
            "title": "Benchmarking on two nodes"
        }, 
        {
            "location": "/advanced/OSU_MicroBenchmarks/README/#now-for-lazy-frustrated-persons", 
            "text": "You will find in the  UL HPC tutorial \nrepository, under the  advanced/OSU_MicroBenchmarks  directory, a set of tools / script that\nfacilitate the running and analysis of this tutorial that you can use/adapt to\nsuit your needs.   In particular, once in the  advanced/OSU_MicroBenchmarks  directory:    running  make fetch  will automatically download the archives for the  OSU micro-benchmarks  in the  src/  directory  you will find the patch file to apply to the version 4.3 in  src/osu-micro-benchmarks-4.3/mpi/one-sided/Makefile.am.patch  The different configuration files for the  MPI generic launcher  in  runs/  Some sample output data in  runs/data/  run  make build  to build the different versions of the OSU Micro-benchmarks  run  make run_interactive  to run the benchmarks, assuming you are in an interactive job  run  make run  to run a passive job executing the benchmarks  run  make plot  to invoke the  Gnuplot  script\n   plots/benchmark_OSU.gnuplot  and generate various plots from the sample\n  runs.    In particular, you'll probably want to see the comparison figure extracted from\nthe sample run in  plots/benchmark_OSU_2H_latency.pdf  and  plots/benchmark_OSU_2H_bandwidth.pdf  A PNG version of these plots is available on Github:  OSU latency  --  OSU Bandwidth", 
            "title": "Now for Lazy / frustrated persons"
        }, 
        {
            "location": "/advanced/HPL/README/", 
            "text": "-\n- mode: markdown; mode: auto-fill; fill-column: 80 -\n-\n\n\nREADME.md\n\n\nCopyright (c) 2013 Sebastien Varrette \n\n\n    Time-stamp: \nDim 2013-11-10 19:21 svarrette\n\n\n\n\n\n\nUL HPC Tutorial: HPL benchmarking on UL HPC platform\n\n\nThe objective of this tutorial is to compile and run on of the reference HPC\nbenchmarks, \nHPL\n, on top of the\n\nUL HPC\n platform.  \n\n\nYou can work in groups for this training, yet individual work is encouraged to\nensure you understand and practice the usage of an HPC platform.  \n\n\nIn all cases, ensure you are able to \nconnect to the chaos and gaia cluster\n. \n\n\n/!\\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A COMPUTING NODE\n\n(access)$\n  oarsub -I -l nodes=1,walltime=4\n\n\n\nAdvanced users only\n: rely on \nscreen\n (see\n  \ntutorial\n) on the frontend\n  prior to running any \noarsub\n command to be more resilient to disconnection.  \n\n\nThe latest version of this tutorial is available on\n\nGithub\n\n\nObjectives\n\n\nHPL\n is a portable implementation of\nHPLinpack used to provide data for the \nTop500\n list  \n\n\nHPL rely on an efficient implementation of the Basic Linear Algebra Subprograms\n(BLAS). You have several choices at this level: \n\n\n\n\nIntel MKL\n\n\nATLAS\n\n\nGotoBlas\n  \n\n\n\n\nThe objective of this practical session is to compare the performances of HPL\nruns compiled under different combination:  \n\n\n\n\nHPL + Intel MKL + Intel MPI\n\n\nHPL + GCC + GotoBLAS2 + Open MPI\n\n\nHPL + GCC + ATLAS + MPICH2\n\n\n\n\nThe benchamrking campain will typically involves successively: \n\n\n\n\na single node \n\n\ntwo nodes, ideally belonging to the same enclosure\n\n\ntwo nodes, belonging to different enclosures\n\n\n\n\nRuns on a single node\n\n\nHigh-Performance Linpack (HPL) with Intel Suite\n\n\nWe are first going to use the\n\nIntel Cluster Toolkit Compiler Edition\n,\nwhich provides Intel C/C++ and Fortran compilers, Intel MPI \n Intel MKL. \n\n\nResources: \n\n\n\n\nHPL\n\n\n\n\nGet the latest release: \n\n\n$\n mkdir ~/TP \n cd ~/TP\n$\n wget http://www.netlib.org/benchmark/hpl/hpl-2.1.tar.gz\n$\n tar xvzf hpl-2.1.tar.gz\n$\n cd hpl-2.1     \n$\n module avail 2\n1 | grep -i MPI\n$\n module load ictce\n$\n module list\nCurrently Loaded Modulefiles:\n        1) icc/2013.5.192     2) ifort/2013.5.192   3) impi/4.1.1.036     4) imkl/11.0.5.192    5) ictce/5.5.0\n$\n module show impi\n$\n module show imkl\n\n\n\nYou notice that Intel MKL is now loaded. \n\n\nRead the \nINSTALL\n file. \n\n\nIn particular, you'll have to edit and adapt a new makefile \nMake.intel64\n\n(inspired from \nsetup/Make.Linux_PII_CBLAS\n typically) and run the compilation\nby  \n\n\n$\n make arch=intel64 clean_arch_all\n$\n make arch=intel64\n\n\n\nSome hint to succeed: \n\n\n\n\nrely on \nmpiicc\n as compiler and linker\n\n\nrely on \n$(I_MPI_ROOT)\n (see \nmodule show impi\n) for the \nMPdir\n entry \n\n\nsimilarly, use \n$(MKLROOT)\n for the \nLAdir\n entry (see \nmodule show imkl\n)\n\n\nEffective use of MKL (in particular): \nLAdir        = $(MKLROOT)\nLAinc        = -I$(LAdir)/mkl/include\nLAlib        = -L$(LAdir)/mkl/lib/intel64 -Wl,--start-group $(LAdir)/mkl/lib/intel64/libmkl_intel_lp64.a $(LAdir)/mkl/lib/intel64/libmkl_intel_thread.a $(LAdir)/mkl/lib/intel64/libmkl_core.a -Wl,--end-group -lpthread\n\n\n\n\n\n\n\nOnce compiled, ensure you are able to run it: \n\n\n$\n cd bin/intel64\n$\n cat HPL.dat\n$\n mpirun -hostfile $OAR_NODEFILE ./xhpl\n\n\n\nNow you'll have to find the optimal set of parameters for using a single\nnode. You can use the following site:\n\nHPL Calculator\n to find good parameters\nand expected performances and adapt \nbin/intel64/HPL.dat\n accordingly. \n\n\nHPL with GCC and GotoBLAS2 and Open MPI\n\n\nAnother alternative is to rely on \nGotoBlas\n.  \n\n\nGet the sources and compile them: \n\n\n # A copy of `GotoBLAS2-1.13.tar.gz` is available in `/tmp` on the access nodes of the cluster\n $\n cd ~/TP\n $\n module purge\n $\n module load OpenMPI\n $\n tar xvzf /tmp/GotoBLAS2-1.13.tar.gz\n $\n mv GotoBLAS2 GotoBLAS2-1.13\n [\u2026]\n $\n make BINARY=64 TARGET=NEHALEM NUM_THREADS=1\n [...]\n  GotoBLAS build complete.\n\n    OS               ... Linux\n    Architecture     ... x86_64\n    BINARY           ... 64bit\n    C compiler       ... GCC  (command line : gcc)\n    Fortran compiler ... GFORTRAN  (command line : gfortran)\n    Library Name     ... libgoto2_nehalemp-r1.13.a (Multi threaded; Max num-threads is 12)\n\n\n\nIf you don't use \nTARGET=NEHALEM\n, you'll encounter the error mentionned\n\nhere\n) \n\n\nNow you can restart HPL compilation by creating (and adapting) a\n\nMake.gotoblas2\n and running the compilation by:  \n\n\n$\n make arch=gotoblas2\n\n\n\nOnce compiled, ensure you are able to run it: \n\n\n$\n cd bin/gotoblas2\n$\n cat HPL.dat\n$\n mpirun -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE ./xhpl\n\n\n\nHPL with GCC and ATLAS and MVAPICH2\n\n\nHere we will rely on the \nAutomatically Tuned Linear Algebra Software (ATLAS)\n\n\nDownload the \nlatest version\n \n(3.11.32 at the time of writing) and compile it:  \n\n\n $\n cd ~/TP\n $\n tar xvf atlas3.11.32.tar.bz2\n $\n mv ATLAS ATLAS-3.11.32 \n cd ATLAS-3.11.32\n $\n module purge\n $\n module load MVAPICH2    \n $\n less INSTALL.txt\n $\n mkdir build.gcc_mvapich2 \n cd build.gcc_mvapich2\n $\n ../configure\n $\n grep ! ../INSTALL.txt\n        make              ! tune and compile library\n        make check        ! perform sanity tests\n        make ptcheck      ! checks of threaded code for multiprocessor systems\n        make time         ! provide performance summary as % of clock rate\n        make install      ! Copy library and include files to other directories\n $\n make\n\n\n\nTake a coffee there, it will compile for a Loooooooooooooong time\n\n\nNow you can restart HPL compilation by creating (and adapting) a \nMake.atlas\n\nand running the compilation by:  \n\n\n$\n make arch=atlas\n\n\n\nOnce compiled, ensure you are able to run it: \n\n\n$\n cd bin/atlas\n$\n cat HPL.dat\n$\n mpirun -launcher ssh -launcher-exec /usr/bin/oarsh -hostfile $OAR_NODEFILE ./xhpl\n\n\n\nBenchmarking on two nodes\n\n\nRestart the benchmarking campain (in the three cases) in the following context: \n\n\n\n\n\n\n2 nodes belonging to the same enclosure. Use for that:\n\n\n$\n oarsub -l enclosure=1/nodes=2,walltime=8 [\u2026]\n\n\n\n\n\n\n\n2 nodes belonging to the different enclosures:\n\n\n$\n oarsub -l enclosure=2/core=1,walltime=8 [\u2026]\n\n\n\n\n\n\n\nNow for Lazy / frustrated persons\n\n\nYou will find in the \nUL HPC tutorial\n\nrepository, under the \nadvanced/HPL\n directory, a set of tools / script that\nfacilitate the running and analysis of this tutorial that you can use/adapt to\nsuit your needs. \n\n\nIn particular, once in the \nadvanced/HPL\n directory: \n\n\n\n\nrunning \nmake fetch\n will automatically download the archives for HPL,\n  GotoBLAS2 and ATLAS (press enter at the end of the last download)\n\n\nsome examples of working \nMakefile\n for HPL used in sample experiments are\n  proposed in \nsrc/hpl-2.1\n\n\na launcher script is proposed in \nruns/launch_hpl_bench\n. This script was used\n  to collect some sample runs for the three experiments mentionned in this\n  tutorial as follows:  \n# Run for HPL with iMKL (icc + iMPI)\n(access-chaos)$\n oarsub -S \"./launch_hpl_bench --module ictce --arch intel64 --serious\"\n\n# Run for HPL with GotoBLAS2 (gcc + OpenMPI)\n(access-chaos)$\n oarsub -S \"./launch_hpl_bench --module OpenMPI --arch gotoblas2 --serious\"\n\n# Run for HPL with ATLAS (gcc + MVAPICH2)\n(access-chaos)$\n oarsub -S \"./launch_hpl_bench --module MVAPICH2 --arch atlas --serious\"\n\n\n\n\n\n\n\nIn particular, the \nruns/data/\n directory host the results of these runs on 2\n  nodes belonging to the same enclosure \n\n\n\n\nrun \nmake plot\n to invoke the \nGnuplot\n script\n  \nplots/benchmark_HPL.gnuplot\n and generate various plots from the sample\n  runs. \n\n\n\n\nIn particular, you'll probably want to see the comparison figure extracted from\nthe sample run \nplots/benchmark_HPL_2H.pdf\n\n\nA PNG version of this plot is available on\n\nGithub", 
            "title": "High Performance Linpack (HPL)"
        }, 
        {
            "location": "/advanced/HPL/README/#ul-hpc-tutorial-hpl-benchmarking-on-ul-hpc-platform", 
            "text": "The objective of this tutorial is to compile and run on of the reference HPC\nbenchmarks,  HPL , on top of the UL HPC  platform.    You can work in groups for this training, yet individual work is encouraged to\nensure you understand and practice the usage of an HPC platform.    In all cases, ensure you are able to  connect to the chaos and gaia cluster .   /!\\ FOR ALL YOUR COMPILING BUSINESS, ENSURE YOU WORK ON A COMPUTING NODE\n\n(access)$   oarsub -I -l nodes=1,walltime=4  Advanced users only : rely on  screen  (see\n   tutorial ) on the frontend\n  prior to running any  oarsub  command to be more resilient to disconnection.    The latest version of this tutorial is available on Github", 
            "title": "UL HPC Tutorial: HPL benchmarking on UL HPC platform"
        }, 
        {
            "location": "/advanced/HPL/README/#objectives", 
            "text": "HPL  is a portable implementation of\nHPLinpack used to provide data for the  Top500  list    HPL rely on an efficient implementation of the Basic Linear Algebra Subprograms\n(BLAS). You have several choices at this level:    Intel MKL  ATLAS  GotoBlas      The objective of this practical session is to compare the performances of HPL\nruns compiled under different combination:     HPL + Intel MKL + Intel MPI  HPL + GCC + GotoBLAS2 + Open MPI  HPL + GCC + ATLAS + MPICH2   The benchamrking campain will typically involves successively:    a single node   two nodes, ideally belonging to the same enclosure  two nodes, belonging to different enclosures", 
            "title": "Objectives"
        }, 
        {
            "location": "/advanced/HPL/README/#runs-on-a-single-node", 
            "text": "High-Performance Linpack (HPL) with Intel Suite  We are first going to use the Intel Cluster Toolkit Compiler Edition ,\nwhich provides Intel C/C++ and Fortran compilers, Intel MPI   Intel MKL.   Resources:    HPL   Get the latest release:   $  mkdir ~/TP   cd ~/TP\n$  wget http://www.netlib.org/benchmark/hpl/hpl-2.1.tar.gz\n$  tar xvzf hpl-2.1.tar.gz\n$  cd hpl-2.1     \n$  module avail 2 1 | grep -i MPI\n$  module load ictce\n$  module list\nCurrently Loaded Modulefiles:\n        1) icc/2013.5.192     2) ifort/2013.5.192   3) impi/4.1.1.036     4) imkl/11.0.5.192    5) ictce/5.5.0\n$  module show impi\n$  module show imkl  You notice that Intel MKL is now loaded.   Read the  INSTALL  file.   In particular, you'll have to edit and adapt a new makefile  Make.intel64 \n(inspired from  setup/Make.Linux_PII_CBLAS  typically) and run the compilation\nby    $  make arch=intel64 clean_arch_all\n$  make arch=intel64  Some hint to succeed:    rely on  mpiicc  as compiler and linker  rely on  $(I_MPI_ROOT)  (see  module show impi ) for the  MPdir  entry   similarly, use  $(MKLROOT)  for the  LAdir  entry (see  module show imkl )  Effective use of MKL (in particular):  LAdir        = $(MKLROOT)\nLAinc        = -I$(LAdir)/mkl/include\nLAlib        = -L$(LAdir)/mkl/lib/intel64 -Wl,--start-group $(LAdir)/mkl/lib/intel64/libmkl_intel_lp64.a $(LAdir)/mkl/lib/intel64/libmkl_intel_thread.a $(LAdir)/mkl/lib/intel64/libmkl_core.a -Wl,--end-group -lpthread    Once compiled, ensure you are able to run it:   $  cd bin/intel64\n$  cat HPL.dat\n$  mpirun -hostfile $OAR_NODEFILE ./xhpl  Now you'll have to find the optimal set of parameters for using a single\nnode. You can use the following site: HPL Calculator  to find good parameters\nand expected performances and adapt  bin/intel64/HPL.dat  accordingly.   HPL with GCC and GotoBLAS2 and Open MPI  Another alternative is to rely on  GotoBlas .    Get the sources and compile them:    # A copy of `GotoBLAS2-1.13.tar.gz` is available in `/tmp` on the access nodes of the cluster\n $  cd ~/TP\n $  module purge\n $  module load OpenMPI\n $  tar xvzf /tmp/GotoBLAS2-1.13.tar.gz\n $  mv GotoBLAS2 GotoBLAS2-1.13\n [\u2026]\n $  make BINARY=64 TARGET=NEHALEM NUM_THREADS=1\n [...]\n  GotoBLAS build complete.\n\n    OS               ... Linux\n    Architecture     ... x86_64\n    BINARY           ... 64bit\n    C compiler       ... GCC  (command line : gcc)\n    Fortran compiler ... GFORTRAN  (command line : gfortran)\n    Library Name     ... libgoto2_nehalemp-r1.13.a (Multi threaded; Max num-threads is 12)  If you don't use  TARGET=NEHALEM , you'll encounter the error mentionned here )   Now you can restart HPL compilation by creating (and adapting) a Make.gotoblas2  and running the compilation by:    $  make arch=gotoblas2  Once compiled, ensure you are able to run it:   $  cd bin/gotoblas2\n$  cat HPL.dat\n$  mpirun -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE ./xhpl  HPL with GCC and ATLAS and MVAPICH2  Here we will rely on the  Automatically Tuned Linear Algebra Software (ATLAS)  Download the  latest version  \n(3.11.32 at the time of writing) and compile it:     $  cd ~/TP\n $  tar xvf atlas3.11.32.tar.bz2\n $  mv ATLAS ATLAS-3.11.32   cd ATLAS-3.11.32\n $  module purge\n $  module load MVAPICH2    \n $  less INSTALL.txt\n $  mkdir build.gcc_mvapich2   cd build.gcc_mvapich2\n $  ../configure\n $  grep ! ../INSTALL.txt\n        make              ! tune and compile library\n        make check        ! perform sanity tests\n        make ptcheck      ! checks of threaded code for multiprocessor systems\n        make time         ! provide performance summary as % of clock rate\n        make install      ! Copy library and include files to other directories\n $  make  Take a coffee there, it will compile for a Loooooooooooooong time  Now you can restart HPL compilation by creating (and adapting) a  Make.atlas \nand running the compilation by:    $  make arch=atlas  Once compiled, ensure you are able to run it:   $  cd bin/atlas\n$  cat HPL.dat\n$  mpirun -launcher ssh -launcher-exec /usr/bin/oarsh -hostfile $OAR_NODEFILE ./xhpl", 
            "title": "Runs on a single node"
        }, 
        {
            "location": "/advanced/HPL/README/#benchmarking-on-two-nodes", 
            "text": "Restart the benchmarking campain (in the three cases) in the following context:     2 nodes belonging to the same enclosure. Use for that:  $  oarsub -l enclosure=1/nodes=2,walltime=8 [\u2026]    2 nodes belonging to the different enclosures:  $  oarsub -l enclosure=2/core=1,walltime=8 [\u2026]", 
            "title": "Benchmarking on two nodes"
        }, 
        {
            "location": "/advanced/HPL/README/#now-for-lazy-frustrated-persons", 
            "text": "You will find in the  UL HPC tutorial \nrepository, under the  advanced/HPL  directory, a set of tools / script that\nfacilitate the running and analysis of this tutorial that you can use/adapt to\nsuit your needs.   In particular, once in the  advanced/HPL  directory:    running  make fetch  will automatically download the archives for HPL,\n  GotoBLAS2 and ATLAS (press enter at the end of the last download)  some examples of working  Makefile  for HPL used in sample experiments are\n  proposed in  src/hpl-2.1  a launcher script is proposed in  runs/launch_hpl_bench . This script was used\n  to collect some sample runs for the three experiments mentionned in this\n  tutorial as follows:   # Run for HPL with iMKL (icc + iMPI)\n(access-chaos)$  oarsub -S \"./launch_hpl_bench --module ictce --arch intel64 --serious\"\n\n# Run for HPL with GotoBLAS2 (gcc + OpenMPI)\n(access-chaos)$  oarsub -S \"./launch_hpl_bench --module OpenMPI --arch gotoblas2 --serious\"\n\n# Run for HPL with ATLAS (gcc + MVAPICH2)\n(access-chaos)$  oarsub -S \"./launch_hpl_bench --module MVAPICH2 --arch atlas --serious\"    In particular, the  runs/data/  directory host the results of these runs on 2\n  nodes belonging to the same enclosure    run  make plot  to invoke the  Gnuplot  script\n   plots/benchmark_HPL.gnuplot  and generate various plots from the sample\n  runs.    In particular, you'll probably want to see the comparison figure extracted from\nthe sample run  plots/benchmark_HPL_2H.pdf  A PNG version of this plot is available on Github", 
            "title": "Now for Lazy / frustrated persons"
        }, 
        {
            "location": "/advanced/MATLAB/README/", 
            "text": "README.md\n\n\nCopyright (c) 2014 Valentin Plugaru \n\n\n\n\nUL HPC Tutorial: MATLAB execution on the UL HPC platform\n\n\nThe objective of this tutorial is to exemplify the execution of \nMATLAB\n - \na high-level language and interactive environment for numerical computation, \nvisualization and programming, on top of the \nUL HPC\n platform.\n\n\nThe tutorial will show you:\n\n\n\n\nhow to run MATLAB in interactive mode, with either the full graphical interface or the text-mode interface\n\n\nhow to run MATLAB in passive (batch) mode, enabling unattended execution on the clusters\n\n\nhow to use MATLAB script (.m) files\n\n\nhow to plot data, saving the plots to file\n\n\nhow to take advantage of some of the paralelization capabilities of MATLAB to speed up your tasks\n\n\n\n\nPrerequisites\n\n\nAs part of this tutorial two Matlab example scripts have been developed and you will need to download them,\nalong with their dependencies, before following the instructions in the next sections:\n\n\n    (gaia-frontend)$\n mkdir ~/matlab-tutorial\n    (gaia-frontend)$\n cd ~/matlab-tutorial\n    (gaia-frontend)$\n wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/MATLAB/example1.m\n    (gaia-frontend)$\n wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/MATLAB/example2.m\n    (gaia-frontend)$\n wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/MATLAB/yahoo_finance_data.m\n\n\n\nOr simply clone the full tutorials repository and make a link to the MATLAB tutorial:\n\n\n    (gaia-frontend)$\n git clone https://github.com/ULHPC/tutorials.git\n    (gaia-frontend)$\n ln -s tutorials/advanced/MATLAB/ ~/matlab-tutorial\n\n\n\nMatlab execution in interactive mode\n\n\nLaunching the full graphical environment\n\n\nRunning the full MATLAB environment (e.g. on the Gaia cluster) requires an \ninteractive OAR session\n. When connecting to the clusters you will\nneed to enable X11 forwarding in order for the graphical environment to be shown on your \nlocal machine:\n\n\n\n\n\n\non Linux simply follow the commands below\n\n\n\n\n\n\non OS X (depending on version) you may not have the X Window System installed, \n  and thus will need to install \nXQuartz\n if\n  the first command below returns an 'X11 forwarding request failed on channel 0' error\n\n\n\n\n\n\non Windows you will need to run \nXMing\n first\n  then to configure Putty (Connection -\n SSH -\n X11 -\n Enable X11 forwarding) before\n  logging in to the clusters.\n\n\n# Connect to Gaia with X11 forwarding enabled (Linux/OS X):\n(yourmachine)$\n ssh access-gaia.uni.lu -X\n\n# Request an interactive job (the default parameters get you 1 core for 2 hours):\n(gaia-frontend)$\n oarsub -I\n\n# Check the Matlab versions installed on the clusters:\n(node)$\n module available 2\n1 | grep -i matlab\n\n# Load a specific MATLAB version:\n(node)$\n module load MATLAB/2013a\n\n# Check that it has been loaded, along with Java:\n(node)$\n module list\n\n# Launch MATLAB\n(node)$\n matlab\n\n\n\n\n\n\n\nAfter a delay, the full Matlab interface will be displayed on your machine and you will be able to run commands, load and edit\nscripts and generate plots. An alternative to the graphical interface is the command-line (text-mode) interface, which is\nenabled through specific parameters, described in the following section.\n\n\nLaunching the command-line environment\n\n\nRunning the text-mode MATLAB interface in an interactive session, is much faster than\nusing the full graphical environment through the network and is useful for commands/scripts testing and \nquick executions:\n\n\n    # First, connect to an UL cluster (e.g. Gaia):\n\n    (yourmachine)$\n ssh access-gaia.uni.lu\n    (gaia-frontend)$\n oarsub -I\n    (node)$\n module load MATLAB/2013a\n\n    # Launch MATLAB with the graphical display mode disabled (critical parameters):\n    (node)$\n matlab -nodisplay -nosplash\n    Opening log file:  /home/users/vplugaru/java.log.3258\n                                                                    \n M A T L A B (R) \n\n                                                          Copyright 1984-2013 The MathWorks, Inc.\n                                                            R2013a (8.1.0.604) 64-bit (glnxa64)\n                                                                     February 15, 2013\n    To get started, type one of these: helpwin, helpdesk, or demo.\n    For product information, visit www.mathworks.com.\n    \n version()\n    ans =\n    8.1.0.604 (R2013a)\n\n\n\nIn this command line you are now able to run Matlab commands, load and edit scripts, but cannot display plots - they can\nhowever be generated and exported to file, which you will need to transfer to your own machine for visualisation.\nWhile the text mode interface is spartan, you still benefit from tab-completion (type the first few letters of\na command then press TAB twice to see possible completions) and can run the integrated help with \nhelp command_name\n \n(e.g. help plot3).\n\n\nExample usage of Matlab in interactive mode\n\n\nAt this point you should have downloaded the example scripts and started Matlab either with the graphical or the text-mode\ninterface. We will now test some Matlab commands by using the yahoo_finance_data function defined in \nyahoo_finance_data.m\n.\nThis function downloads stock market data through the Yahoo! Finance API, and we will use it to get 1 month worth of stock data\nfor IBM (whose stock symbol is 'IBM'):\n\n\n     \n cd('~/matlab-tutorial')\n     \n [hist_date, hist_high, hist_low, hist_open, hist_close, hist_vol] = yahoo_finance_data('IBM', 2014, 2, 1, 2014, 3, 1);\n     \n size(hist_date)                                                                                                       \n     ans =\n         19     1\n     \n [hist_date{1} ' ' hist_date{end}]     \n     ans =\n     2014-02-03 2014-02-28\n     \n min(hist_low)                                                                                                         \n     ans =\n       171.2512\n     \n max(hist_high)\n     ans =\n       186.1200\n     \n mean(hist_close)\n     ans =\n       180.3184\n     \n std(hist_close) \n     ans =\n         4.5508\n\n\n\nThrough these commands we have seen that the function returns column vectors, we were able to get 19 days' worth of information and \nwe used simple statistic functions to get an idea of how the stock varied in the given period. \n\n\nNow we will use the example1.m script that shows: \n  - how to use different plotting methods on the data retrieved with the yahoo_finance_data function\n  - how to export the plots in different graphic formats instead of displaying them (which is only available when running the \n  full graphical environment and also allows the user to visually interact with the plot)\n\n\n     \n example1\n     Elapsed time is 2.421686 seconds.\n     \n quit\n     (node)$\n\n     (node)$\n ls *pdf *eps\n     example1-2dplot.eps  example1-2dplot.pdf  example1-scatter.eps\n\n\n\nWe have run the example1.m script which has downloaded Apple ('AAPL' ticker) stock data for the year 2013 and generated three plots:\n\n\n\n\nexample1-2dplot.pdf : color PDF generated with the saveas function, plotting dates (x-axis) vs closing stock price (y-axis)\n\n\nexample1-2dplot.eps : high quality black and white Enhanced PostScript (EPS) generated with the print function, same data as above\n\n\nexample1-scatter.eps : high quality color EPS generated with the print function, showing also the trading volume (z-axis) and \nusing different color datapoints (red) where the closing share price was above 500\n\n\n\n\nThe script has also used the tic/toc Matlab commands to time it's execution and we can see it took less than 3 seconds to download\nand process data from the Yahoo Finance API and generate the plots. \n\n\nFinally, we have closed our Matlab session and were returned to the cluster's command line prompt where we found the generated plots.\n\n\nA PNG version of the latter two plots is shown below:\n\n\n\n\n\nFurther examples showing serial and parallel executions are given below in the 'Example usage of Matlab in passive mode' section.\n\n\nMatlab execution in passive mode\n\n\nFor non-interactive or long executions, MATLAB can be ran in passive mode, reading all commands from\nan input file you provide (e.g. named INPUTFILE.m) and saving the results in an output file (e.g. named OUTPUTFILE.out),\nby either:\n\n\n\n\n\n\nusing redirection operators:\n\n\n$\n matlab -nodisplay -nosplash \n INPUTFILE.m \n OUTPUTFILE.out\n\n\n\n\n\n\n\nrunning the input file as a command (notice the missing '.m' extension) and copying output \n(as a log) to the output file:\n\n\n$\n matlab -nodisplay -nosplash -r INPUTFILE -logfile OUTPUTFILE.out\n\n\n\n\n\n\n\nThe second usage mode is recommended as it corresponds to the batch-mode execution. In the first case your \noutput file will contain the '\n' characters generated by Matlab as if ran interactively, along with the\nresults of your own commands.\n\n\nHowever as the second usage mode runs your script as a command, it \nmust\n contain the \nquit\n command at\nthe end in order to close Matlab, otherwise after the script has executed Matlab will stay open,\nwaiting for further input until the end of the walltime you set for the passive job, tying up compute\nresources needlessly.\n\n\nThe following minimal example shows how to run a serial (1 core) MATLAB script for 24 hours in passive mode:\n\n\n    (gaia-frontend)$\n oarsub -l walltime=24:00:00 \"source /etc/profile; module load MATLAB; matlab -nodisplay -nosplash \n INPUTFILE.m \n OUTPUTFILE.out\"\n\n\n\nIdeally you \nwould not\n run MATLAB jobs like this but instead \ncreate/adapt a launcher script\n to contain those instructions. A minimal shell script (e.g. named 'your_matlab_launcher.sh') could be:\n\n\n    #!/bin/bash\n    source /etc/profile\n    # REMEMBER to change the following to the correct paths of the input/output files:\n    INPUTFILE=your_input_file_name_without_extension\n    OUTPUTFILE=your_output_file_name_with_extension.out\n    # Load a specific version of MATLAB and run the input script:\n    module load MATLAB/2013b\n    matlab -nodisplay -nosplash -r $INPUTFILE -logfile $OUTPUTFILE\n\n\n\nthen launch it in a job (e.g. requesting 6 cores on 1 node for 10 hours - assuming your input file takes advantage of the parallel cores):\n\n\n    (gaia-frontend)$\n oarsub -l nodes=1/core=6,walltime=10:00:00 your_matlab_launcher.sh\n\n\n\nRemember! that the Matlab script you run with the '-r' parameter must contain the \nquit\n command at the end\nin order to close Matlab properly when the script finishes.\n\n\nExample usage of Matlab in passive mode\n\n\nIn this section we will use the \nexample2.m\n script which shows:\n  - the serial execution of time consuming operations; 1 core on 1 node\n  - the parallel execution (based on the \nparfor\n command) and relative speedup vs serial execution, setting\n    the maximum number of parallel threads through environment variables; up to 1 full node\n  - GPU-based parallel execution; available only on \nGPU-enabled nodes\n\n\nBy default the parallel section of the script uses up to 4 threads, thus for a first test we will request 4 cores on 1\ncompute node for 5 minutes:\n\n\n    (gaia-frontend)$\n cd ~/matlab-tutorial\n    # Create a file called matlab-minlauncher.sh with launching commands\n    (gaia-frontend)$\n cat \n EOF \n matlab-minlauncher.sh\n    #!/bin/bash\n    source /etc/profile\n    module load MATLAB/2013a\n    cd ~/matlab-tutorial\n    matlab -nodisplay -nosplash -r example2 -logfile example2.out\n    EOF\n    (gaia-frontend)$\n chmod +x matlab-minlauncher.sh\n    (gaia-frontend)$\n oarsub -l nodes=1/core=4,walltime=00:05:00 ~/matlab-tutorial/matlab-minlauncher.sh\n    # we now wait for the job to complete execution\n    (gaia-frontend)$\n cat example2.out\n                \n M A T L A B (R) \n\n          Copyright 1984-2013 The MathWorks, Inc.\n            R2013a (8.1.0.604) 64-bit (glnxa64)\n                February 15, 2013\n\n\nTo get started, type one of these: helpwin, helpdesk, or demo. \nFor product information, visit www.mathworks.com.\n\n-- Will perform 24 iterations on a 1000x1000 matrix\n\n-- Serial test\n-- Execution time: 27.870898s.\n-- Parallel tests with up to 4 cores\n\n-- Parallel test using 2 cores\nStarting matlabpool ... connected to 2 workers.\nSending a stop signal to all the workers ... stopped.\n-- Execution time: 19.869666s.\n-- Execution time with overhead: 39.025023s.\n\n-- Parallel test using 3 cores\nStarting matlabpool ... connected to 3 workers.\nSending a stop signal to all the workers ... stopped.\n-- Execution time: 14.584377s.\n-- Execution time with overhead: 25.587958s.\n\n-- Parallel test using 4 cores\nStarting matlabpool ... connected to 4 workers.\nSending a stop signal to all the workers ... stopped.\n-- Execution time: 12.298823s.\n-- Execution time with overhead: 22.379418s.\n\n-- Number of processes, parallel execution time (s), parallel execution time with overhead(s), speedup, speedup with overhead:\n    1.0000   27.8709   27.8709    1.0000    1.0000\n    2.0000   19.8697   39.0250    1.4027    0.7142\n    3.0000   14.5844   25.5880    1.9110    1.0892\n    4.0000   12.2988   22.3794    2.2661    1.2454\n\n\n-- GPU-Parallel test not available on this system.\n\n\n\nThe script is also able to read an environment variable \nMATLABMP\n and create as many parallel threads as specified in this variable.\nWe will now generate another launcher which will set this variable to the number of cores we specified to OAR.\n\n\n    (gaia-frontend)$\n cd ~/matlab-tutorial\n    (gaia-frontend)$\n cat \n EOF \n matlab-minlauncher2.sh\n    #!/bin/bash\n    source /etc/profile\n    module load MATLAB/2013a\n    cd ~/matlab-tutorial\n    export MATLABMP=$(cat $OAR_NODEFILE | wc -l)\n    matlab -nodisplay -nosplash -r example2 -logfile example2b.out\n    EOF\n    (gaia-frontend)$\n chmod +x matlab-minlauncher2.sh\n    (gaia-frontend)$\n oarsub -l nodes=1/core=6,walltime=00:05:00 ~/matlab-tutorial/matlab-minlauncher2.sh\n    # we now wait for the job to complete execution\n    (gaia-frontend)$\n head -n 17 example2b.out \n                  \n M A T L A B (R) \n\n            Copyright 1984-2013 The MathWorks, Inc.\n              R2013a (8.1.0.604) 64-bit (glnxa64)\n                  February 15, 2013\n\n\n  To get started, type one of these: helpwin, helpdesk, or demo.\n  For product information, visit www.mathworks.com.\n\n  -- Will perform 24 iterations on a 1000x1000 matrix\n\n  -- Serial test\n  -- Execution time: 28.193131s.\n\n  -- Found environment variable MATLABMP=6.\n  -- Parallel tests with up to 6 cores\n    (gaia-frontend)$\n\n\n\n\nWe have submitted an OAR job requesting 6 cores for 5 minutes and used the second launcher. It can be seen that the example2.m script \nhas read the MATLABMP environment variable and has used in its execution.\n\n\nAs shown previously, the jobs we have submitted did not run on GPU-enabled nodes, thus in this last example we will specifically\ntarget GPU nodes and see that the last test of example2.m will also be executed.\nBefore testing the following commands, edit the \nmatlab-minlauncher2.sh\n script and make MATLAB store its output in a \nexample2c.out\n\nfile.\n\n\n  (gaia-frontend)$\n cd ~/matlab-tutorial\n  (gaia-frontend)$\n oarsub -l nodes=1/core=6,walltime=00:05:00 -p \"gpu='YES'\" ~/matlab-tutorial/matlab-minlauncher2.sh\n  # now wait for the job to complete execution, then check the output file\n  (gaia-frontend)$\n tail -n 5 example2c.out \n    -- GPU test \n    -- GPU Execution time: 28.192080s.\n    -- GPU Execution time with overhead: 30.499892s.\n    -- GPU vs Serial speedup: 1.102579.\n    -- GPU with overhead vs Serial speedup: 1.019151.\n\n\n\nThe following plot shows a sample speedup obtained by using parfor on Gaia, with up to 12 parallel threads:\n\n\n\nRelative to the fast execution of the inner instruction (which calculates the eigenvalues of a matrix) \nthe overhead given by the creation of the parallel pool and the task assignations is quite high in this example,\nwhere for 12 cores the speedup is 5.26x but taking the overhead into account it is only 4x.\n\n\nUseful references\n\n\n\n\nParallel Computing Toolbox documentation\n\n\nParallel for-Loops (parfor) documentation\n\n\nGPU Computing documentation", 
            "title": "MATLAB"
        }, 
        {
            "location": "/advanced/MATLAB/README/#ul-hpc-tutorial-matlab-execution-on-the-ul-hpc-platform", 
            "text": "The objective of this tutorial is to exemplify the execution of  MATLAB  - \na high-level language and interactive environment for numerical computation, \nvisualization and programming, on top of the  UL HPC  platform.  The tutorial will show you:   how to run MATLAB in interactive mode, with either the full graphical interface or the text-mode interface  how to run MATLAB in passive (batch) mode, enabling unattended execution on the clusters  how to use MATLAB script (.m) files  how to plot data, saving the plots to file  how to take advantage of some of the paralelization capabilities of MATLAB to speed up your tasks", 
            "title": "UL HPC Tutorial: MATLAB execution on the UL HPC platform"
        }, 
        {
            "location": "/advanced/MATLAB/README/#prerequisites", 
            "text": "As part of this tutorial two Matlab example scripts have been developed and you will need to download them,\nalong with their dependencies, before following the instructions in the next sections:      (gaia-frontend)$  mkdir ~/matlab-tutorial\n    (gaia-frontend)$  cd ~/matlab-tutorial\n    (gaia-frontend)$  wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/MATLAB/example1.m\n    (gaia-frontend)$  wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/MATLAB/example2.m\n    (gaia-frontend)$  wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/MATLAB/yahoo_finance_data.m  Or simply clone the full tutorials repository and make a link to the MATLAB tutorial:      (gaia-frontend)$  git clone https://github.com/ULHPC/tutorials.git\n    (gaia-frontend)$  ln -s tutorials/advanced/MATLAB/ ~/matlab-tutorial", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/advanced/MATLAB/README/#matlab-execution-in-interactive-mode", 
            "text": "Launching the full graphical environment  Running the full MATLAB environment (e.g. on the Gaia cluster) requires an  interactive OAR session . When connecting to the clusters you will\nneed to enable X11 forwarding in order for the graphical environment to be shown on your \nlocal machine:    on Linux simply follow the commands below    on OS X (depending on version) you may not have the X Window System installed, \n  and thus will need to install  XQuartz  if\n  the first command below returns an 'X11 forwarding request failed on channel 0' error    on Windows you will need to run  XMing  first\n  then to configure Putty (Connection -  SSH -  X11 -  Enable X11 forwarding) before\n  logging in to the clusters.  # Connect to Gaia with X11 forwarding enabled (Linux/OS X):\n(yourmachine)$  ssh access-gaia.uni.lu -X\n\n# Request an interactive job (the default parameters get you 1 core for 2 hours):\n(gaia-frontend)$  oarsub -I\n\n# Check the Matlab versions installed on the clusters:\n(node)$  module available 2 1 | grep -i matlab\n\n# Load a specific MATLAB version:\n(node)$  module load MATLAB/2013a\n\n# Check that it has been loaded, along with Java:\n(node)$  module list\n\n# Launch MATLAB\n(node)$  matlab    After a delay, the full Matlab interface will be displayed on your machine and you will be able to run commands, load and edit\nscripts and generate plots. An alternative to the graphical interface is the command-line (text-mode) interface, which is\nenabled through specific parameters, described in the following section.  Launching the command-line environment  Running the text-mode MATLAB interface in an interactive session, is much faster than\nusing the full graphical environment through the network and is useful for commands/scripts testing and \nquick executions:      # First, connect to an UL cluster (e.g. Gaia):\n\n    (yourmachine)$  ssh access-gaia.uni.lu\n    (gaia-frontend)$  oarsub -I\n    (node)$  module load MATLAB/2013a\n\n    # Launch MATLAB with the graphical display mode disabled (critical parameters):\n    (node)$  matlab -nodisplay -nosplash\n    Opening log file:  /home/users/vplugaru/java.log.3258\n                                                                      M A T L A B (R)  \n                                                          Copyright 1984-2013 The MathWorks, Inc.\n                                                            R2013a (8.1.0.604) 64-bit (glnxa64)\n                                                                     February 15, 2013\n    To get started, type one of these: helpwin, helpdesk, or demo.\n    For product information, visit www.mathworks.com.\n      version()\n    ans =\n    8.1.0.604 (R2013a)  In this command line you are now able to run Matlab commands, load and edit scripts, but cannot display plots - they can\nhowever be generated and exported to file, which you will need to transfer to your own machine for visualisation.\nWhile the text mode interface is spartan, you still benefit from tab-completion (type the first few letters of\na command then press TAB twice to see possible completions) and can run the integrated help with  help command_name  \n(e.g. help plot3).  Example usage of Matlab in interactive mode  At this point you should have downloaded the example scripts and started Matlab either with the graphical or the text-mode\ninterface. We will now test some Matlab commands by using the yahoo_finance_data function defined in  yahoo_finance_data.m .\nThis function downloads stock market data through the Yahoo! Finance API, and we will use it to get 1 month worth of stock data\nfor IBM (whose stock symbol is 'IBM'):         cd('~/matlab-tutorial')\n       [hist_date, hist_high, hist_low, hist_open, hist_close, hist_vol] = yahoo_finance_data('IBM', 2014, 2, 1, 2014, 3, 1);\n       size(hist_date)                                                                                                       \n     ans =\n         19     1\n       [hist_date{1} ' ' hist_date{end}]     \n     ans =\n     2014-02-03 2014-02-28\n       min(hist_low)                                                                                                         \n     ans =\n       171.2512\n       max(hist_high)\n     ans =\n       186.1200\n       mean(hist_close)\n     ans =\n       180.3184\n       std(hist_close) \n     ans =\n         4.5508  Through these commands we have seen that the function returns column vectors, we were able to get 19 days' worth of information and \nwe used simple statistic functions to get an idea of how the stock varied in the given period.   Now we will use the example1.m script that shows: \n  - how to use different plotting methods on the data retrieved with the yahoo_finance_data function\n  - how to export the plots in different graphic formats instead of displaying them (which is only available when running the \n  full graphical environment and also allows the user to visually interact with the plot)         example1\n     Elapsed time is 2.421686 seconds.\n       quit\n     (node)$ \n     (node)$  ls *pdf *eps\n     example1-2dplot.eps  example1-2dplot.pdf  example1-scatter.eps  We have run the example1.m script which has downloaded Apple ('AAPL' ticker) stock data for the year 2013 and generated three plots:   example1-2dplot.pdf : color PDF generated with the saveas function, plotting dates (x-axis) vs closing stock price (y-axis)  example1-2dplot.eps : high quality black and white Enhanced PostScript (EPS) generated with the print function, same data as above  example1-scatter.eps : high quality color EPS generated with the print function, showing also the trading volume (z-axis) and \nusing different color datapoints (red) where the closing share price was above 500   The script has also used the tic/toc Matlab commands to time it's execution and we can see it took less than 3 seconds to download\nand process data from the Yahoo Finance API and generate the plots.   Finally, we have closed our Matlab session and were returned to the cluster's command line prompt where we found the generated plots.  A PNG version of the latter two plots is shown below:   Further examples showing serial and parallel executions are given below in the 'Example usage of Matlab in passive mode' section.", 
            "title": "Matlab execution in interactive mode"
        }, 
        {
            "location": "/advanced/MATLAB/README/#matlab-execution-in-passive-mode", 
            "text": "For non-interactive or long executions, MATLAB can be ran in passive mode, reading all commands from\nan input file you provide (e.g. named INPUTFILE.m) and saving the results in an output file (e.g. named OUTPUTFILE.out),\nby either:    using redirection operators:  $  matlab -nodisplay -nosplash   INPUTFILE.m   OUTPUTFILE.out    running the input file as a command (notice the missing '.m' extension) and copying output \n(as a log) to the output file:  $  matlab -nodisplay -nosplash -r INPUTFILE -logfile OUTPUTFILE.out    The second usage mode is recommended as it corresponds to the batch-mode execution. In the first case your \noutput file will contain the ' ' characters generated by Matlab as if ran interactively, along with the\nresults of your own commands.  However as the second usage mode runs your script as a command, it  must  contain the  quit  command at\nthe end in order to close Matlab, otherwise after the script has executed Matlab will stay open,\nwaiting for further input until the end of the walltime you set for the passive job, tying up compute\nresources needlessly.  The following minimal example shows how to run a serial (1 core) MATLAB script for 24 hours in passive mode:      (gaia-frontend)$  oarsub -l walltime=24:00:00 \"source /etc/profile; module load MATLAB; matlab -nodisplay -nosplash   INPUTFILE.m   OUTPUTFILE.out\"  Ideally you  would not  run MATLAB jobs like this but instead  create/adapt a launcher script  to contain those instructions. A minimal shell script (e.g. named 'your_matlab_launcher.sh') could be:      #!/bin/bash\n    source /etc/profile\n    # REMEMBER to change the following to the correct paths of the input/output files:\n    INPUTFILE=your_input_file_name_without_extension\n    OUTPUTFILE=your_output_file_name_with_extension.out\n    # Load a specific version of MATLAB and run the input script:\n    module load MATLAB/2013b\n    matlab -nodisplay -nosplash -r $INPUTFILE -logfile $OUTPUTFILE  then launch it in a job (e.g. requesting 6 cores on 1 node for 10 hours - assuming your input file takes advantage of the parallel cores):      (gaia-frontend)$  oarsub -l nodes=1/core=6,walltime=10:00:00 your_matlab_launcher.sh  Remember! that the Matlab script you run with the '-r' parameter must contain the  quit  command at the end\nin order to close Matlab properly when the script finishes.  Example usage of Matlab in passive mode  In this section we will use the  example2.m  script which shows:\n  - the serial execution of time consuming operations; 1 core on 1 node\n  - the parallel execution (based on the  parfor  command) and relative speedup vs serial execution, setting\n    the maximum number of parallel threads through environment variables; up to 1 full node\n  - GPU-based parallel execution; available only on  GPU-enabled nodes  By default the parallel section of the script uses up to 4 threads, thus for a first test we will request 4 cores on 1\ncompute node for 5 minutes:      (gaia-frontend)$  cd ~/matlab-tutorial\n    # Create a file called matlab-minlauncher.sh with launching commands\n    (gaia-frontend)$  cat   EOF   matlab-minlauncher.sh\n    #!/bin/bash\n    source /etc/profile\n    module load MATLAB/2013a\n    cd ~/matlab-tutorial\n    matlab -nodisplay -nosplash -r example2 -logfile example2.out\n    EOF\n    (gaia-frontend)$  chmod +x matlab-minlauncher.sh\n    (gaia-frontend)$  oarsub -l nodes=1/core=4,walltime=00:05:00 ~/matlab-tutorial/matlab-minlauncher.sh\n    # we now wait for the job to complete execution\n    (gaia-frontend)$  cat example2.out\n                  M A T L A B (R)  \n          Copyright 1984-2013 The MathWorks, Inc.\n            R2013a (8.1.0.604) 64-bit (glnxa64)\n                February 15, 2013\n\n\nTo get started, type one of these: helpwin, helpdesk, or demo. \nFor product information, visit www.mathworks.com.\n\n-- Will perform 24 iterations on a 1000x1000 matrix\n\n-- Serial test\n-- Execution time: 27.870898s.\n-- Parallel tests with up to 4 cores\n\n-- Parallel test using 2 cores\nStarting matlabpool ... connected to 2 workers.\nSending a stop signal to all the workers ... stopped.\n-- Execution time: 19.869666s.\n-- Execution time with overhead: 39.025023s.\n\n-- Parallel test using 3 cores\nStarting matlabpool ... connected to 3 workers.\nSending a stop signal to all the workers ... stopped.\n-- Execution time: 14.584377s.\n-- Execution time with overhead: 25.587958s.\n\n-- Parallel test using 4 cores\nStarting matlabpool ... connected to 4 workers.\nSending a stop signal to all the workers ... stopped.\n-- Execution time: 12.298823s.\n-- Execution time with overhead: 22.379418s.\n\n-- Number of processes, parallel execution time (s), parallel execution time with overhead(s), speedup, speedup with overhead:\n    1.0000   27.8709   27.8709    1.0000    1.0000\n    2.0000   19.8697   39.0250    1.4027    0.7142\n    3.0000   14.5844   25.5880    1.9110    1.0892\n    4.0000   12.2988   22.3794    2.2661    1.2454\n\n\n-- GPU-Parallel test not available on this system.  The script is also able to read an environment variable  MATLABMP  and create as many parallel threads as specified in this variable.\nWe will now generate another launcher which will set this variable to the number of cores we specified to OAR.      (gaia-frontend)$  cd ~/matlab-tutorial\n    (gaia-frontend)$  cat   EOF   matlab-minlauncher2.sh\n    #!/bin/bash\n    source /etc/profile\n    module load MATLAB/2013a\n    cd ~/matlab-tutorial\n    export MATLABMP=$(cat $OAR_NODEFILE | wc -l)\n    matlab -nodisplay -nosplash -r example2 -logfile example2b.out\n    EOF\n    (gaia-frontend)$  chmod +x matlab-minlauncher2.sh\n    (gaia-frontend)$  oarsub -l nodes=1/core=6,walltime=00:05:00 ~/matlab-tutorial/matlab-minlauncher2.sh\n    # we now wait for the job to complete execution\n    (gaia-frontend)$  head -n 17 example2b.out \n                    M A T L A B (R)  \n            Copyright 1984-2013 The MathWorks, Inc.\n              R2013a (8.1.0.604) 64-bit (glnxa64)\n                  February 15, 2013\n\n\n  To get started, type one of these: helpwin, helpdesk, or demo.\n  For product information, visit www.mathworks.com.\n\n  -- Will perform 24 iterations on a 1000x1000 matrix\n\n  -- Serial test\n  -- Execution time: 28.193131s.\n\n  -- Found environment variable MATLABMP=6.\n  -- Parallel tests with up to 6 cores\n    (gaia-frontend)$   We have submitted an OAR job requesting 6 cores for 5 minutes and used the second launcher. It can be seen that the example2.m script \nhas read the MATLABMP environment variable and has used in its execution.  As shown previously, the jobs we have submitted did not run on GPU-enabled nodes, thus in this last example we will specifically\ntarget GPU nodes and see that the last test of example2.m will also be executed.\nBefore testing the following commands, edit the  matlab-minlauncher2.sh  script and make MATLAB store its output in a  example2c.out \nfile.    (gaia-frontend)$  cd ~/matlab-tutorial\n  (gaia-frontend)$  oarsub -l nodes=1/core=6,walltime=00:05:00 -p \"gpu='YES'\" ~/matlab-tutorial/matlab-minlauncher2.sh\n  # now wait for the job to complete execution, then check the output file\n  (gaia-frontend)$  tail -n 5 example2c.out \n    -- GPU test \n    -- GPU Execution time: 28.192080s.\n    -- GPU Execution time with overhead: 30.499892s.\n    -- GPU vs Serial speedup: 1.102579.\n    -- GPU with overhead vs Serial speedup: 1.019151.  The following plot shows a sample speedup obtained by using parfor on Gaia, with up to 12 parallel threads:  Relative to the fast execution of the inner instruction (which calculates the eigenvalues of a matrix) \nthe overhead given by the creation of the parallel pool and the task assignations is quite high in this example,\nwhere for 12 cores the speedup is 5.26x but taking the overhead into account it is only 4x.", 
            "title": "Matlab execution in passive mode"
        }, 
        {
            "location": "/advanced/MATLAB/README/#useful-references", 
            "text": "Parallel Computing Toolbox documentation  Parallel for-Loops (parfor) documentation  GPU Computing documentation", 
            "title": "Useful references"
        }, 
        {
            "location": "/advanced/R/README/", 
            "text": "README.md\n\n\nCopyright (c) 2014 Joseph Emeras \n\n\n\n\nR Tutorial\n\n\nThrough this tutorial you will learn how to use R from your local machine or from one of the \nUL HPC platform\n clusters.\nWe will also use the \nggplot\n library to generate nice graphics and export them as pdf files. \nThen, we will see how to organize and group data. Finally we will illustrate how R can benefit from multicore and cluster parallelization.\n\n\nWarning: this tutorial does not focus on the learning of R language but aims at showing you nice startup tips.\n\nIf you're also looking for a good tutorial on R's data structures you can take a look at: \nHadley Wickham's page\n.\n\n\nConventions used in this tutorial:\n\n\n\n\ncommands that have to be typed on the cluster start with a prompt like this: \njdoe@access:~$\n\n\ncommands that have to be typed on your local machine start with a prompt like this: \njdoe@localhost:~$\n\n\ncode blocks containing one or several \n should not be pasted \"as it\", they are meant for you to observe the output of each function; others can be pasted in R terminal \"as it\".\n\n\n\n\nPre-requisites\n\n\nOptional: On your local machine\n\n\nFirst of all, let's install R. You will find releases for various distributions available at \nCRAN Archive\n.\nOnce installed, to use R interactive session interface, simply open a terminal and type:\n\n\njdoe@localhost:~$ R\n\n\n\nYou will also find handy to use the \nR-Studio\n graphical IDE. R-Studio embeds a R shell where you can call R functions as in the interactive session interface.\nThus you can use whether R interactive shell or R-Studio embedded shell.\n\n\nOn the cluster\n\n\nR is already available in \nChaos\n and \nGaia\n clusters as a module. \nThe first step is the reservation of a resource. Connect to your favorite cluster frontend (here: \ngaia\n)\n\n\njdoe@localhost:~$ ssh gaia-cluster\n\n\n\nOnce connected to the user frontend, book 1 core for half an hour (as we will use R in single-threaded mode, we will need only one core).\n\n\njdoe@access:~$ oarsub -I -l core=1,walltime=\"00:30:00\"\n\n\n\nWhen the job is running and you are connected load R module (version compiled with Intel Compiler).\nFor a complete list of availbale modules see: \nSoftware page\n.\n\n\njdoe@access:~$ module load lang/R/3.2.0-ictce-7.3.5-bare\n\n\n\n\n\n\nNow you should be able to invoke R and see something like this:\n\n\njdoe@cluster-node-1:~$ R\nR version 3.2.0 (2015-04-16) -- \"Full of Ingredients\"\nCopyright (C) 2015 The R Foundation for Statistical Computing\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n\n\n\n\n\nInstalling R Packages\n\n\nsessionInfo()\n function gives information about R version, loaded libraries etc.\n\n\n sessionInfo()\nR version 3.2.0 (2015-04-16)    \nPlatform: x86_64-unknown-linux-gnu (64-bit)\nRunning under: Debian GNU/Linux 7 (wheezy)\n\nlocale:\n[1] C\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base\n\n\n\n\n\nTo install libraries you can use the \ninstall.packages()\n function. e.g\n\n\n\n install.packages(\"ggplot2\")\n\n\n\nThis will install the \nggplot2\n library.\n\n\nNote: on the first run, R might ask you various questions during the installation. e.g. selecting a CRAN mirror to use for downloading packages. Select a mirror close to your location. For other questions, using default values is ok.\n\n\nNow, to load this library call:\n\n\n library(ggplot2)\n\n\n\nA call to \nsessionInfo()\n function will return \nggplot2\n version as it is now attached to the current session.\n\n\nWarm-up Session -- Simple Plotting\n\n\nFrom Single Dataset\n\n\nMovies dataset, derived from data provided by \nIMDB\n is a sample dataset available in \nggplot2\n for testing purpose. Its data description can be found \nhere\n.\nThus, when loading \nggplot2\n library, this dataset is available under the name: \nmovies\n.\n\n\n(OPTIONAL) An other way to get the dataset would be to download, extract and read it with:\n\n\nmovies_url = \"http://had.co.nz/data/movies/movies.tab.gz\"                               # this is the http url of data file\n## Download the file from given url to given destination file\nuser_ = Sys.getenv(\"USER\")\ndest_dir = paste0(\"/tmp/\", user_)\nsystem(paste0(\"mkdir \", dest_dir))\ndest_file = paste0(dest_dir, \"/movies.tab.gz\")                  \ndownload.file(movies_url, destfile=dest_file)                                           # we download the data file with `download.file()` function and save it in /tmp\n## Extract the .gz using system's gzip command\nsystem(paste0(\"gzip -d \", dest_file))                                                   # `system()` function executes system calls\ndest_file = paste0(dest_dir, \"/movies.tab\")\n## Load the file in a dataframe with read.table() function\nmovies = read.table(dest_file, sep=\"\\t\", header=TRUE, quote=\"\", comment=\"\")             # `read.table()` function reads a file and stores it in a data.frame object\n\n\n\n(OPTIONAL 2) An third way to get the dataset is by using the \nreadr\n library that can uncompress by itself:\n\n\nlibrary(readr)\nsystem(\"wget http://had.co.nz/data/movies/movies.tab.gz\")\nmovies = read_tsv(\"movies.tab.gz\", col_names = TRUE)\n\n\n\nNow let's take a (reproducible) sample of 1000 movies and plot their distribution regarding their rating.\n\n\nlibrary(ggplot2)                                                                        # load ggplot2 library to use packages functions\nset.seed(5689)                                                                          # set the seed for random selection used in `sample()` function\nmovies_sample = movies[sample(nrow(movies), 1000), ]                                    # movies is the data.frame name, from this data.frame, randomly select 1000 rows\ngraph = ggplot(data=movies_sample) + geom_histogram(aes(x=rating), binwidth=0.5)        # construct the graph -- movies_sample will be used as data, we will plot an histogram where x=movies_sample$rating and with a bin size=0.5\nggsave(graph, file=\"movies_hist.pdf\", width=8, height=4)                                # save the graph in a pdf file\n\n\n\nNow you retrieve the generated pdf on your local workstation for visualization:\n\n\njdoe@localhost:~$ scp gaia-cluster:movies_hist.pdf .\n\n\n\nggplot2\n proposes many functions to plot data according to your needs. Do not hesitate to wander in the \nggplot2 documentation\n and to read at provided examples to better understand how to use it.\nThe \nggsave()\n function is convenient to export ggplot graphics as .pdf or .png files\n\n\nFrom Several Datasets\n\n\nNow, let's say we have two different datasets: \ndiamonds_fair\n and \ndiamonds_good\n that are both extracts from the \ndiamonds\n dataset (also provided in ggplot2).\nIn this example we will consider that these two datasets come from different sources, so do not try to understand the next lines, they are just here to setup the example (simply copy-paste these in your R prompt).\n\n\nset.seed(2109)  \ndiamonds_fair = data.frame(carat=diamonds$carat[which(diamonds$cut == 'Fair')], price=diamonds$price[which(diamonds$cut == 'Fair')])\ndiamonds_fair = diamonds_fair[sample(nrow(diamonds_fair), 20), ]\ndiamonds_good = data.frame(carat=diamonds$carat[which(diamonds$cut == 'Good')], price=diamonds$price[which(diamonds$cut == 'Good')])\ndiamonds_good = diamonds_good[sample(nrow(diamonds_good), 20), ]\n\n\n\nTo know the class of an R object you can use the \nclass()\n function\n\n\n class(diamonds_fair)\n  [1] \"data.frame\"\n\n\n\nSo we have these two datasets, being of class dataframe. In R, a \ndata.frame\n is one kind of data structure whose columns have names and that can contain several rows. Basically it looks like a matrix with columns identified by an index \nand\n a name, and with rows identified by an index.\nLet's check how they are organized with the \nnames()\n function that gives a dataset column names.\n\n\n names(diamonds_fair)\n  [1] \"carat\" \"price\"\n\n\n names(diamonds_good)\n  [1] \"carat\" \"price\"\n\n\n\nThus for each dataset row we have the price and the carat value for a given diamond.\nWe want to add a column to datasets that will describe from which one it comes from, then we will merge these into one single dataset.\n\n\ndiamonds_fair = cbind(diamonds_fair, cut_class=\"Fair\")                                  # add a column named cut_class with all values being \"Fair\" to data.frame diamonds_fair\ndiamonds_good = cbind(diamonds_good, cut_class=\"Good\")                                  # same with \"Good\"\ndiamonds_merge = rbind(diamonds_fair, diamonds_good)                                    # combine the 2 data.frame with `rbind()` as they both have the same structure\n\n\n\ncbind()\n function is used to add a column to a dataframe, \nrbind()\n to combine rows of two dataframes (c is for column, r is for row).\nNow we have all data merged in a dataframe and a column that describes the origin of data (the column \ncut_class\n), let's plot data.\n\n\nNote: To visualize an extract of your data you can do:\n\n\n diamonds_merge[1:10,]         # returns rows 1 to 10\n\n diamonds_merge[,3]            # returns column no.3\n\n diamonds_merge$cut_class      # returns column named cut_class\n\n\n\nThen we construct and save the graph.\n\n\ngraph = ggplot(data=diamonds_merge) + geom_point(aes(x=carat, y=price, colour=cut_class))   # this time we use ggplot's function `geom_point()` to plot data points. colour=cut_class aestetics option will plot the points according to cut_class values\nggsave(graph, file=\"diamonds_plot.pdf\", width=8, height=4)\n\n\n\nRemember, to get help about a particular function you can type \n?function_name\n. e.g.\n\n\n ?cbind\n\n\n\nTo get package and meta information on a function you can type \n??function_name\n. e.g.\n\n\n ??ggsave\n\n\n\nOrganizing your Data\n\n\nLet's say we are working with the full \ndiamonds\n dataset and we want to have the average price for a given diamond cut. \n\n\n names(diamonds)\n [1] \"carat\"   \"cut\"     \"color\"   \"clarity\" \"depth\"   \"table\"   \"price\"\n [8] \"x\"       \"y\"       \"z\"\n\n\n\nWe could do a for loop to aggregate the data per cuts and manually compute the average price, but in R loops are generally a bad idea. For large datasets it is very long to compute.\nThus instead of looping around the dataset, we will use a function from the \nplyr\n package: \nddply()\n. \nYou will first need to install and load \nplyr\n.\n\n\ninstall.packages(\"plyr\")\nlibrary(plyr)\n\n\n\nNow we are ready to call \nddply()\n. The first parameter will be the dataset, the second will be the column of the dataset we want to aggregate on, third parameter will be the call to the \nsummarize()\n function that will enable to aggregate data on the \ncut\n column. The forth parameter will be the operation we will do for each of the aggregated classes. Thus: \n\n\n ddply(diamonds, .(cut), summarize, avg_price=mean(price))             # in the data.frame named diamonds, aggregate by column named cut and apply the function mean() on the price of aggregated rows\n        cut avg_price\n1      Fair  4358.758\n2      Good  3928.864\n3 Very Good  3981.760\n4   Premium  4584.258\n5     Ideal  3457.542\n\n\n\nwill give us what we wanted.\n\n\nNote: \nddply()\n from the \nplyr\n package is similar to \naggregate()\n from base package, you can use indifferently one or the other, \nplyr\n functions simply provide a more consistent naming convention.\n\n\nPerfomance Considerations\n\n\nIn the previous section for the aggregation, instead of using \nddply\n, we could also have used \nlapply\n (but in a slightlier more complicated way):\n\n\n as.data.frame(cbind(cut=as.character(unique(diamonds$cut)), avg_price=lapply(unique(diamonds$cut), function(x) mean(diamonds$price[which(diamonds$cut == x)]))))\n\n\n\nSo, we want to know which one of the two versions is the most efficient, for that purpose, the library \nmicrobenchmark\n is handy.\n\n\ninstall.packages(\"microbenchmark\")\nlibrary(microbenchmark)\n\n\n\nWe can use the \nmicrobenchmark()\n function on several expressions, with a given repetition number to compare them:\n\n\n m = microbenchmark(DDPLY=ddply(diamonds, .(cut), summarize, avg_price=mean(price)), LAPPLY=as.data.frame(cbind(cut=as.character(unique(diamonds$cut)), avg_price=lapply(unique(diamonds$cut), function(x) mean(diamonds$price[which(diamonds$cut == x)])))), times=1000)\n\n m\nUnit: milliseconds\n   expr      min       lq   median       uq       max neval\n  DDPLY 24.73218 29.10263 65.50023 69.80662 140.54594  1000\n LAPPLY 22.85223 24.44387 25.55315 27.45517  96.94869  1000\n\n\n\nPlotting the benchmark result gives us a boxplot graph:\n\n\n## save the output graph as png file\npng(\"benchmark_boxplot.png\")                                        # other method to save graphics that are not generated with ggplot. We give a name to the output graphic\n## plot the graph\nplot(m)                                                             # then we plot it\n## flush the output device to save the graph\ndev.off()                                                           # finally we close the output device, this will save the graphic in the output file\n\n\n\nNote: the \ndplyr\n library is a new package which provides a set of tools for efficiently manipulating datasets in R. \ndplyr\n is the next iteration of \nplyr\n, focussing on only data frames. \ndplyr\n is faster, has a more consistent API and should be easier to use.\nBy using \ndplyr\n instead of \nddply()\n from the \nplyr\n package in this example you can obtain a significant speedup, however its syntax may first seem a bit confusing.\n\n\ndiamonds %\n% group_by(cut) %\n% summarize(avg_price = mean(price))\n\n\n\nUsing \ndata.table\n Package\n\n\nAccording to \ndata.table documentation\n \ndata.table\n inherits from \ndata.frame\n to offer fast subset, fast grouping, fast update, fast ordered\njoins and list columns in a short and flexible syntax, for faster development. It uses binary search instead of vector scan to perform its operations and thus is scalable.\nWe can convert easily a \ndata.frame\n to a \ndata.table\n.\n\n\nFirst install and load the \"data.table\" package then convert the \ndata.frame\n to a \ndata.table\n:\n\n\n MOVIES = data.table(movies)\n\n\n\nAs \ndata.table\n uses binary search, we have to define manually the keys that will be used for this search, this is done with \nsetkey()\n function.\n\n\nLet's now create a new \ndata.frame\n. We will make it large enough to demonstrate the difference between a vector scan and a binary search.\n\n\ngrpsize = ceiling(1e7/26^2) # 10 million rows, 676 groups\nsystem.time( DF \n- data.frame(\n    x=rep(LETTERS,each=26*grpsize),\n    y=rep(letters,each=grpsize),\n    v=runif(grpsize*26^2),\n    stringsAsFactors=FALSE)\n)\n\n\n\nThis generated a data.frame named DF with 3 columns. Column x is a repetition of uppercase letters from A to Z, column y is minorcase letters. Column v is a random uniform value.\nTo illustrate the difference, we take as example the selection in this large dataset of rows where x==\"R\" and y==\"h\".\n\n\n system.time(ans1 \n- DF[DF$x==\"R\" \n DF$y==\"h\",])       # vector scan. we select rows where x=\"R\" and y=\"h\". For this we have to scan the full data.frame twice.\n\n\n DT = data.table(DF)                                   # convert the data.frame to a data.table\n\n setkey(DT,x,y)                                        # set column x and y as data.table keys.\n\n system.time(ans2 \n- DT[J(\"R\",\"h\")])                   # binary search. We select rows that match the join between DT and the data.table row: data.table(\"R\",\"h\"). This will return the same result as before but much faster.\n\n\n\nIn the first case, we scan the full table twice (once for selecting x's that are equal to \"R\", then y's that are equal to \"h\"), then do the selection.\nIn the second case, we are joining DT to the 1 row, 2 column table returned by data.table(\"R\",\"h\"). We use the alias for joining data.tables called J(), short for join. As we defined x and y as keys, this works like a database join.\nYou can see that vector scan is very long compared to binary search.\n\n\nGrouping\n\n\ndata.table\n also provides faster operations for reading files and grouping data.\n\n\nNow you can compare the same aggregation operation with \ndata.frame\n and \ndata.table\n. In both examples we aggregate on x and apply the function \nsum()\n to corresponding v.\n\n\ndata.frame\n style:\n\n\nsystem.time(tapply(DT$v,DT$x,sum))\n\n\n\ndata.table\n style, using \nby\n:\n\n\nsystem.time(DT[,sum(v),by=x])\n\n\n\nQuestion: use \nddply()\n instead of \ntapply()\n in the first example.\n\n\n\n\n\nQuestion: return the min and max instead of the sum.\n\n\nHint: you can create a function named min_max to help you doing this. Example:\n\n\ndummy_function = function(x){ x }                   # dummy_function(x) will return x.\ndummy_function2 = function(x, y){ c(x, y) }         # dummy_function2(x, y) will return a vector (x,y).\n\n\n\n\n\n\nParallel R\n\n\nThe first part of the tutorial is now over, you can connect to \ngaia\n cluster and submit an other job requesting several machines.\n\n\njdoe@localhost:~$ ssh gaia-cluster\n\njdoe@access:~$ oarsub -I -l nodes=2,walltime=1\n\n\n\n\n\n\nWhen the job is running and you are connected load R module (version compiled with Intel Compiler), then run R.\n\n\njdoe@access:~$ module load lang/R/3.2.0-ictce-7.3.5-bare\njdoe@access:~$ R\n\n\n\nWe will use a large dataset (400K+ rows) to illustrate the effect of parallelization in R (as dataset is large, the following line may take time to complete depending on your network speed).\n\n\n air = read.csv(url(\"http://packages.revolutionanalytics.com/datasets/AirOnTimeCSV2012/airOT201201.csv\"))\n\n\n\nNOTE\n: If downloading the air dataset (above line) takes too much time you can load it from a file on the cluster:\n\n\n load(\"~jemeras/data/air.rda\")\n\n\n\n(For information you can save your \nair\n object just as showed above with: \nsave(air,file= \"~/data/air.rda\")\n)\n\n\nIf we want to have the number of flights for each destination \nDEST\n we can do the following:\n\n\ndests = as.character(unique(air$DEST))\ncount_flights = function(x){length(which(air$DEST == x))}\nas.data.frame(cbind(dest=dests, nb=lapply(dests, count_flights)))\n\n\n\nAs the dataframe is large it takes some time to compute\n\n\n microbenchmark(LAPPLY=lapply(dests, count_flights), times=10)\nUnit: seconds\n   expr      min       lq   median       uq      max neval\n LAPPLY 1.607961 1.609036 1.609638 1.610269 2.023961    10\n\n\n\nSingle Machine Parallelization\n\n\nTo parallelize the lapply function we can use \nmclapply()\n from \nparallel\n package and give it the number of cores to use.\n\nmclapply()\n uses the underlying operating system fork() functionality to achieve parallelization.\nUsing several cores makes the process shorter.\n\n\n library(parallel)\n\n as.data.frame(cbind(dest=dests, nb=mclapply(dests, count_flights, mc.cores=12)))\n\n\n\n microbenchmark(MCLAPPLY=mclapply(dests, count_flights, mc.cores=12), times=10)  # or use `detectCores()` from `parallel` package instead of giving cores value. \nUnit: milliseconds\n     expr      min       lq   median       uq     max neval\n MCLAPPLY 233.8035 235.1089 235.9138 236.6393 263.934    10\n\n\n\nIt is nice to visualize all your cores working on your node with \nhtop\n for example. You can connect to the same node from another terminal by typing:\n\n\njdoe@access:~$ oarstat -u\nJob id     Name           User           Submission Date     S Queue\n---------- -------------- -------------- ------------------- - ----------\n6664321                   jdoe           2015-06-03 14:24:23 R default\n\n\n\nNote the \nJob id\n field. Put this job id in the next command:\n\n\njdoe@access:~$ oarsub -C 6664321\n\n\n\nThen \nhtop\n will show you your cores working if you call again the \nmclapply()\n function.\n\n\nFinally you can save the \nair\n R object to reuse it in an other R session.\n\n\n save(air, file=\"./air.rda\")\n\n\n\nThen quit your current R session but \ndo not\n end your current oar job.\n\n\nCluster Parallelization\n\n\nThe \nparLapply()\n function will create a cluster of processes, which could even reside on different machines on the network, and they communicate via TCP/IP or MPI in order to pass the tasks and results between each other.\nThus you have to load necessary packages and export necessary data and functions to the global environment of the cluster workers.\n\n\nFirst, add the module loading at bash login too for enabling it on the nodes. To do so, within a shell type:\n\n\necho 'module load lang/R/3.2.0-ictce-7.3.5-bare' \n ~/.bash_login       # /!\\ TO REMOVE AT THE END OF PS /!\\\nmodule purge\nmodule load lang/R/3.2.0-ictce-7.3.5-bare\n\n\n\nWarning: do not forget to clean your ~/.bash_login file after the PS (remove the 'module load lang/R/3.2.0-ictce-7.3.5-bare' line).\n\n\nSocket Communications\n\n\nFirst, let's load data and initialize variables.\n\n\nlibrary(parallel)\n\n# air = read.csv(url(\"http://packages.revolutionanalytics.com/datasets/AirOnTimeCSV2012/airOT201201.csv\"))  # load again the air data.table from http\nload(\"./air.rda\")   # read saved R object from file \"air.rda\"\ndests = as.character(unique(air$DEST))\ncount_flights = function(x){length(which(air$DEST == x))}\n\n## set cluster characteristics -- get OAR nodes to use, type of communication\nnodes = scan(Sys.getenv(\"OAR_NODE_FILE\"), what=character(0))\noar_job_id = as.numeric(Sys.getenv(\"OAR_JOB_ID\"))\nconnector = paste0(\"OAR_JOB_ID=\", oar_job_id)\nconnector = paste0(connector, \" ~jemeras/bin/roarsh\")\ncomm_type = \"PSOCK\"\n\n\n\nThen, setup the cluster.\n\n\n## set up the cluster\ncl = makeCluster(nodes, type = comm_type, rshcmd = connector)   \n## If a particular library \nLIB\n is needed, load it on the nodes with\n# clusterEvalQ(cl, { library(\nLIB\n) })\n## or give the full environment with\n# clusterEvalQ(cl, sessionInfo())\n## export air dataset on all the nodes\nclusterExport(cl, varlist=c(\"air\"))\n\n\n\nDo the parallel computation.\n\n\n## compute in parallel through sockets\nas.data.frame(cbind(dest=dests, nb=parLapply(cl, dests, count_flights)))\n\n\n\nFinalize and cleanup things.\n\n\nstopCluster(cl)\n\n\n\nExercise: Plot a speedup graph with different number of cores and/or machines used.\n\n\nMPI Communications\n\n\nIn this section we will use the same example as previously but with MPI connections. \nFor that purpose we need to install two libraries: \nrmpi\n and \nsnow\n.\n\n\nAs we are using R compiled with Intel compiler we will have to specify manually some paths and which version of MPI we are using when installing \nrmpi\n.\n\n\n install.packages(\"Rmpi\",\n                   configure.args =\n                   c(paste(\"--with-Rmpi-include=\",Sys.getenv(\"EBROOTIMPI\"),\"/include64\",sep=\"\"),\n                     paste(\"--with-Rmpi-libpath=\",Sys.getenv(\"EBROOTIMPI\"),\"/lib64\",sep=\"\"),\n                     paste(\"--with-mpi=\",Sys.getenv(\"EBROOTIMPI\"),sep=\"\"),\n                     \"--with-Rmpi-type=OPENMPI\"))\n\n\n\nNOTE\n: if the installation fails you can try using the module with \nRmpi\n already installed:\n\n\nmodule purge\nmodule load lang/R/3.2.0-ictce-7.3.5-Rmpi\n\n\n\nThen, outside of R shell write a file named \nparallelAirDests.R\n with the following code.\n\n\nWarning\n: when using parallelization in R, please keep in mind that communication is much slower than computation. Thus if your problem is involving a long computation then you are right to use it, otherwise, if your problem is a large quantity of data you must use \ndata.table\n or \ndplyr\n.\n\n\nlibrary(Rmpi)\nlibrary(snow)\n\n# Initiate the cluster\ncluster \n- makeMPIcluster(length(readLines(Sys.getenv(\"OAR_NODE_FILE\"))))\n\n# Function that prints the hostname of the caller, just for fun\nsayhello \n- function()\n{\n    info \n- Sys.info()[c(\"nodename\", \"machine\")]\n    paste(\"Hello from\", info[1])\n}\n\n# Call the 'sayhello' function on each node of the cluster\nnames \n- clusterCall(cluster, sayhello)\nprint(unlist(names))\n\n\n# Compute the number of flights for a given destination. \n# Same as previous examples but with MPI communications.\nload(\"~jemeras/data/air.rda\")\ndests = as.character(unique(air$DEST))\ncount_flights = function(x){length(which(air$DEST == x))}\n#as.data.frame(cbind(dest=dests, nb=lapply(dests, count_flights)))\nclusterExport(cluster, \"air\")\n\nprint(as.data.frame(cbind(dest=dests, nb=parLapply(cluster, dests, count_flights))))\n\n# Terminate the cluster\nstopCluster(cluster)\n\n\n\nThen, still outside of R and on your job head node run: \n\n\nmpirun -np 1 -machinefile $OAR_NODE_FILE Rscript parallelAirDests.R\n\n\n\nYou may find strange the \n-np 1\n, in fact this is because it is \nsnow\n that manages the processes spawning.\n\n\n\n\n\n\n\n\nUsefull links\n\n\n\n\n\n\nCRAN Archive\n\n\n\n\n\n\nCRAN HPC Packages\n\n\n\n\n\n\nggplot2 Documentation\n\n\n\n\n\n\nAdvanced R programming by Hadley Wickham", 
            "title": "R - statistical computing "
        }, 
        {
            "location": "/advanced/R/README/#r-tutorial", 
            "text": "Through this tutorial you will learn how to use R from your local machine or from one of the  UL HPC platform  clusters.\nWe will also use the  ggplot  library to generate nice graphics and export them as pdf files. \nThen, we will see how to organize and group data. Finally we will illustrate how R can benefit from multicore and cluster parallelization.  Warning: this tutorial does not focus on the learning of R language but aims at showing you nice startup tips. \nIf you're also looking for a good tutorial on R's data structures you can take a look at:  Hadley Wickham's page .  Conventions used in this tutorial:   commands that have to be typed on the cluster start with a prompt like this:  jdoe@access:~$  commands that have to be typed on your local machine start with a prompt like this:  jdoe@localhost:~$  code blocks containing one or several   should not be pasted \"as it\", they are meant for you to observe the output of each function; others can be pasted in R terminal \"as it\".", 
            "title": "R Tutorial"
        }, 
        {
            "location": "/advanced/R/README/#pre-requisites", 
            "text": "Optional: On your local machine  First of all, let's install R. You will find releases for various distributions available at  CRAN Archive .\nOnce installed, to use R interactive session interface, simply open a terminal and type:  jdoe@localhost:~$ R  You will also find handy to use the  R-Studio  graphical IDE. R-Studio embeds a R shell where you can call R functions as in the interactive session interface.\nThus you can use whether R interactive shell or R-Studio embedded shell.  On the cluster  R is already available in  Chaos  and  Gaia  clusters as a module. \nThe first step is the reservation of a resource. Connect to your favorite cluster frontend (here:  gaia )  jdoe@localhost:~$ ssh gaia-cluster  Once connected to the user frontend, book 1 core for half an hour (as we will use R in single-threaded mode, we will need only one core).  jdoe@access:~$ oarsub -I -l core=1,walltime=\"00:30:00\"  When the job is running and you are connected load R module (version compiled with Intel Compiler).\nFor a complete list of availbale modules see:  Software page .  jdoe@access:~$ module load lang/R/3.2.0-ictce-7.3.5-bare   Now you should be able to invoke R and see something like this:  jdoe@cluster-node-1:~$ R\nR version 3.2.0 (2015-04-16) -- \"Full of Ingredients\"\nCopyright (C) 2015 The R Foundation for Statistical Computing\nPlatform: x86_64-unknown-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.   Installing R Packages  sessionInfo()  function gives information about R version, loaded libraries etc.   sessionInfo()\nR version 3.2.0 (2015-04-16)    \nPlatform: x86_64-unknown-linux-gnu (64-bit)\nRunning under: Debian GNU/Linux 7 (wheezy)\n\nlocale:\n[1] C\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base   To install libraries you can use the  install.packages()  function. e.g   install.packages(\"ggplot2\")  This will install the  ggplot2  library.  Note: on the first run, R might ask you various questions during the installation. e.g. selecting a CRAN mirror to use for downloading packages. Select a mirror close to your location. For other questions, using default values is ok.  Now, to load this library call:   library(ggplot2)  A call to  sessionInfo()  function will return  ggplot2  version as it is now attached to the current session.", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/advanced/R/README/#warm-up-session-simple-plotting", 
            "text": "From Single Dataset  Movies dataset, derived from data provided by  IMDB  is a sample dataset available in  ggplot2  for testing purpose. Its data description can be found  here .\nThus, when loading  ggplot2  library, this dataset is available under the name:  movies .  (OPTIONAL) An other way to get the dataset would be to download, extract and read it with:  movies_url = \"http://had.co.nz/data/movies/movies.tab.gz\"                               # this is the http url of data file\n## Download the file from given url to given destination file\nuser_ = Sys.getenv(\"USER\")\ndest_dir = paste0(\"/tmp/\", user_)\nsystem(paste0(\"mkdir \", dest_dir))\ndest_file = paste0(dest_dir, \"/movies.tab.gz\")                  \ndownload.file(movies_url, destfile=dest_file)                                           # we download the data file with `download.file()` function and save it in /tmp\n## Extract the .gz using system's gzip command\nsystem(paste0(\"gzip -d \", dest_file))                                                   # `system()` function executes system calls\ndest_file = paste0(dest_dir, \"/movies.tab\")\n## Load the file in a dataframe with read.table() function\nmovies = read.table(dest_file, sep=\"\\t\", header=TRUE, quote=\"\", comment=\"\")             # `read.table()` function reads a file and stores it in a data.frame object  (OPTIONAL 2) An third way to get the dataset is by using the  readr  library that can uncompress by itself:  library(readr)\nsystem(\"wget http://had.co.nz/data/movies/movies.tab.gz\")\nmovies = read_tsv(\"movies.tab.gz\", col_names = TRUE)  Now let's take a (reproducible) sample of 1000 movies and plot their distribution regarding their rating.  library(ggplot2)                                                                        # load ggplot2 library to use packages functions\nset.seed(5689)                                                                          # set the seed for random selection used in `sample()` function\nmovies_sample = movies[sample(nrow(movies), 1000), ]                                    # movies is the data.frame name, from this data.frame, randomly select 1000 rows\ngraph = ggplot(data=movies_sample) + geom_histogram(aes(x=rating), binwidth=0.5)        # construct the graph -- movies_sample will be used as data, we will plot an histogram where x=movies_sample$rating and with a bin size=0.5\nggsave(graph, file=\"movies_hist.pdf\", width=8, height=4)                                # save the graph in a pdf file  Now you retrieve the generated pdf on your local workstation for visualization:  jdoe@localhost:~$ scp gaia-cluster:movies_hist.pdf .  ggplot2  proposes many functions to plot data according to your needs. Do not hesitate to wander in the  ggplot2 documentation  and to read at provided examples to better understand how to use it.\nThe  ggsave()  function is convenient to export ggplot graphics as .pdf or .png files  From Several Datasets  Now, let's say we have two different datasets:  diamonds_fair  and  diamonds_good  that are both extracts from the  diamonds  dataset (also provided in ggplot2).\nIn this example we will consider that these two datasets come from different sources, so do not try to understand the next lines, they are just here to setup the example (simply copy-paste these in your R prompt).  set.seed(2109)  \ndiamonds_fair = data.frame(carat=diamonds$carat[which(diamonds$cut == 'Fair')], price=diamonds$price[which(diamonds$cut == 'Fair')])\ndiamonds_fair = diamonds_fair[sample(nrow(diamonds_fair), 20), ]\ndiamonds_good = data.frame(carat=diamonds$carat[which(diamonds$cut == 'Good')], price=diamonds$price[which(diamonds$cut == 'Good')])\ndiamonds_good = diamonds_good[sample(nrow(diamonds_good), 20), ]  To know the class of an R object you can use the  class()  function   class(diamonds_fair)\n  [1] \"data.frame\"  So we have these two datasets, being of class dataframe. In R, a  data.frame  is one kind of data structure whose columns have names and that can contain several rows. Basically it looks like a matrix with columns identified by an index  and  a name, and with rows identified by an index.\nLet's check how they are organized with the  names()  function that gives a dataset column names.   names(diamonds_fair)\n  [1] \"carat\" \"price\"  names(diamonds_good)\n  [1] \"carat\" \"price\"  Thus for each dataset row we have the price and the carat value for a given diamond.\nWe want to add a column to datasets that will describe from which one it comes from, then we will merge these into one single dataset.  diamonds_fair = cbind(diamonds_fair, cut_class=\"Fair\")                                  # add a column named cut_class with all values being \"Fair\" to data.frame diamonds_fair\ndiamonds_good = cbind(diamonds_good, cut_class=\"Good\")                                  # same with \"Good\"\ndiamonds_merge = rbind(diamonds_fair, diamonds_good)                                    # combine the 2 data.frame with `rbind()` as they both have the same structure  cbind()  function is used to add a column to a dataframe,  rbind()  to combine rows of two dataframes (c is for column, r is for row).\nNow we have all data merged in a dataframe and a column that describes the origin of data (the column  cut_class ), let's plot data.  Note: To visualize an extract of your data you can do:   diamonds_merge[1:10,]         # returns rows 1 to 10  diamonds_merge[,3]            # returns column no.3  diamonds_merge$cut_class      # returns column named cut_class  Then we construct and save the graph.  graph = ggplot(data=diamonds_merge) + geom_point(aes(x=carat, y=price, colour=cut_class))   # this time we use ggplot's function `geom_point()` to plot data points. colour=cut_class aestetics option will plot the points according to cut_class values\nggsave(graph, file=\"diamonds_plot.pdf\", width=8, height=4)  Remember, to get help about a particular function you can type  ?function_name . e.g.   ?cbind  To get package and meta information on a function you can type  ??function_name . e.g.   ??ggsave", 
            "title": "Warm-up Session -- Simple Plotting"
        }, 
        {
            "location": "/advanced/R/README/#organizing-your-data", 
            "text": "Let's say we are working with the full  diamonds  dataset and we want to have the average price for a given diamond cut.    names(diamonds)\n [1] \"carat\"   \"cut\"     \"color\"   \"clarity\" \"depth\"   \"table\"   \"price\"\n [8] \"x\"       \"y\"       \"z\"  We could do a for loop to aggregate the data per cuts and manually compute the average price, but in R loops are generally a bad idea. For large datasets it is very long to compute.\nThus instead of looping around the dataset, we will use a function from the  plyr  package:  ddply() . \nYou will first need to install and load  plyr .  install.packages(\"plyr\")\nlibrary(plyr)  Now we are ready to call  ddply() . The first parameter will be the dataset, the second will be the column of the dataset we want to aggregate on, third parameter will be the call to the  summarize()  function that will enable to aggregate data on the  cut  column. The forth parameter will be the operation we will do for each of the aggregated classes. Thus:    ddply(diamonds, .(cut), summarize, avg_price=mean(price))             # in the data.frame named diamonds, aggregate by column named cut and apply the function mean() on the price of aggregated rows\n        cut avg_price\n1      Fair  4358.758\n2      Good  3928.864\n3 Very Good  3981.760\n4   Premium  4584.258\n5     Ideal  3457.542  will give us what we wanted.  Note:  ddply()  from the  plyr  package is similar to  aggregate()  from base package, you can use indifferently one or the other,  plyr  functions simply provide a more consistent naming convention.  Perfomance Considerations  In the previous section for the aggregation, instead of using  ddply , we could also have used  lapply  (but in a slightlier more complicated way):   as.data.frame(cbind(cut=as.character(unique(diamonds$cut)), avg_price=lapply(unique(diamonds$cut), function(x) mean(diamonds$price[which(diamonds$cut == x)]))))  So, we want to know which one of the two versions is the most efficient, for that purpose, the library  microbenchmark  is handy.  install.packages(\"microbenchmark\")\nlibrary(microbenchmark)  We can use the  microbenchmark()  function on several expressions, with a given repetition number to compare them:   m = microbenchmark(DDPLY=ddply(diamonds, .(cut), summarize, avg_price=mean(price)), LAPPLY=as.data.frame(cbind(cut=as.character(unique(diamonds$cut)), avg_price=lapply(unique(diamonds$cut), function(x) mean(diamonds$price[which(diamonds$cut == x)])))), times=1000)  m\nUnit: milliseconds\n   expr      min       lq   median       uq       max neval\n  DDPLY 24.73218 29.10263 65.50023 69.80662 140.54594  1000\n LAPPLY 22.85223 24.44387 25.55315 27.45517  96.94869  1000  Plotting the benchmark result gives us a boxplot graph:  ## save the output graph as png file\npng(\"benchmark_boxplot.png\")                                        # other method to save graphics that are not generated with ggplot. We give a name to the output graphic\n## plot the graph\nplot(m)                                                             # then we plot it\n## flush the output device to save the graph\ndev.off()                                                           # finally we close the output device, this will save the graphic in the output file  Note: the  dplyr  library is a new package which provides a set of tools for efficiently manipulating datasets in R.  dplyr  is the next iteration of  plyr , focussing on only data frames.  dplyr  is faster, has a more consistent API and should be easier to use.\nBy using  dplyr  instead of  ddply()  from the  plyr  package in this example you can obtain a significant speedup, however its syntax may first seem a bit confusing.  diamonds % % group_by(cut) % % summarize(avg_price = mean(price))  Using  data.table  Package  According to  data.table documentation   data.table  inherits from  data.frame  to offer fast subset, fast grouping, fast update, fast ordered\njoins and list columns in a short and flexible syntax, for faster development. It uses binary search instead of vector scan to perform its operations and thus is scalable.\nWe can convert easily a  data.frame  to a  data.table .  First install and load the \"data.table\" package then convert the  data.frame  to a  data.table :   MOVIES = data.table(movies)  As  data.table  uses binary search, we have to define manually the keys that will be used for this search, this is done with  setkey()  function.  Let's now create a new  data.frame . We will make it large enough to demonstrate the difference between a vector scan and a binary search.  grpsize = ceiling(1e7/26^2) # 10 million rows, 676 groups\nsystem.time( DF  - data.frame(\n    x=rep(LETTERS,each=26*grpsize),\n    y=rep(letters,each=grpsize),\n    v=runif(grpsize*26^2),\n    stringsAsFactors=FALSE)\n)  This generated a data.frame named DF with 3 columns. Column x is a repetition of uppercase letters from A to Z, column y is minorcase letters. Column v is a random uniform value.\nTo illustrate the difference, we take as example the selection in this large dataset of rows where x==\"R\" and y==\"h\".   system.time(ans1  - DF[DF$x==\"R\"   DF$y==\"h\",])       # vector scan. we select rows where x=\"R\" and y=\"h\". For this we have to scan the full data.frame twice.  DT = data.table(DF)                                   # convert the data.frame to a data.table  setkey(DT,x,y)                                        # set column x and y as data.table keys.  system.time(ans2  - DT[J(\"R\",\"h\")])                   # binary search. We select rows that match the join between DT and the data.table row: data.table(\"R\",\"h\"). This will return the same result as before but much faster.  In the first case, we scan the full table twice (once for selecting x's that are equal to \"R\", then y's that are equal to \"h\"), then do the selection.\nIn the second case, we are joining DT to the 1 row, 2 column table returned by data.table(\"R\",\"h\"). We use the alias for joining data.tables called J(), short for join. As we defined x and y as keys, this works like a database join.\nYou can see that vector scan is very long compared to binary search.  Grouping  data.table  also provides faster operations for reading files and grouping data.  Now you can compare the same aggregation operation with  data.frame  and  data.table . In both examples we aggregate on x and apply the function  sum()  to corresponding v.  data.frame  style:  system.time(tapply(DT$v,DT$x,sum))  data.table  style, using  by :  system.time(DT[,sum(v),by=x])  Question: use  ddply()  instead of  tapply()  in the first example.   Question: return the min and max instead of the sum.  Hint: you can create a function named min_max to help you doing this. Example:  dummy_function = function(x){ x }                   # dummy_function(x) will return x.\ndummy_function2 = function(x, y){ c(x, y) }         # dummy_function2(x, y) will return a vector (x,y).", 
            "title": "Organizing your Data"
        }, 
        {
            "location": "/advanced/R/README/#parallel-r", 
            "text": "The first part of the tutorial is now over, you can connect to  gaia  cluster and submit an other job requesting several machines.  jdoe@localhost:~$ ssh gaia-cluster\n\njdoe@access:~$ oarsub -I -l nodes=2,walltime=1   When the job is running and you are connected load R module (version compiled with Intel Compiler), then run R.  jdoe@access:~$ module load lang/R/3.2.0-ictce-7.3.5-bare\njdoe@access:~$ R  We will use a large dataset (400K+ rows) to illustrate the effect of parallelization in R (as dataset is large, the following line may take time to complete depending on your network speed).   air = read.csv(url(\"http://packages.revolutionanalytics.com/datasets/AirOnTimeCSV2012/airOT201201.csv\"))  NOTE : If downloading the air dataset (above line) takes too much time you can load it from a file on the cluster:   load(\"~jemeras/data/air.rda\")  (For information you can save your  air  object just as showed above with:  save(air,file= \"~/data/air.rda\") )  If we want to have the number of flights for each destination  DEST  we can do the following:  dests = as.character(unique(air$DEST))\ncount_flights = function(x){length(which(air$DEST == x))}\nas.data.frame(cbind(dest=dests, nb=lapply(dests, count_flights)))  As the dataframe is large it takes some time to compute   microbenchmark(LAPPLY=lapply(dests, count_flights), times=10)\nUnit: seconds\n   expr      min       lq   median       uq      max neval\n LAPPLY 1.607961 1.609036 1.609638 1.610269 2.023961    10  Single Machine Parallelization  To parallelize the lapply function we can use  mclapply()  from  parallel  package and give it the number of cores to use. mclapply()  uses the underlying operating system fork() functionality to achieve parallelization.\nUsing several cores makes the process shorter.   library(parallel)  as.data.frame(cbind(dest=dests, nb=mclapply(dests, count_flights, mc.cores=12)))  microbenchmark(MCLAPPLY=mclapply(dests, count_flights, mc.cores=12), times=10)  # or use `detectCores()` from `parallel` package instead of giving cores value. \nUnit: milliseconds\n     expr      min       lq   median       uq     max neval\n MCLAPPLY 233.8035 235.1089 235.9138 236.6393 263.934    10  It is nice to visualize all your cores working on your node with  htop  for example. You can connect to the same node from another terminal by typing:  jdoe@access:~$ oarstat -u\nJob id     Name           User           Submission Date     S Queue\n---------- -------------- -------------- ------------------- - ----------\n6664321                   jdoe           2015-06-03 14:24:23 R default  Note the  Job id  field. Put this job id in the next command:  jdoe@access:~$ oarsub -C 6664321  Then  htop  will show you your cores working if you call again the  mclapply()  function.  Finally you can save the  air  R object to reuse it in an other R session.   save(air, file=\"./air.rda\")  Then quit your current R session but  do not  end your current oar job.  Cluster Parallelization  The  parLapply()  function will create a cluster of processes, which could even reside on different machines on the network, and they communicate via TCP/IP or MPI in order to pass the tasks and results between each other.\nThus you have to load necessary packages and export necessary data and functions to the global environment of the cluster workers.  First, add the module loading at bash login too for enabling it on the nodes. To do so, within a shell type:  echo 'module load lang/R/3.2.0-ictce-7.3.5-bare'   ~/.bash_login       # /!\\ TO REMOVE AT THE END OF PS /!\\\nmodule purge\nmodule load lang/R/3.2.0-ictce-7.3.5-bare  Warning: do not forget to clean your ~/.bash_login file after the PS (remove the 'module load lang/R/3.2.0-ictce-7.3.5-bare' line).  Socket Communications  First, let's load data and initialize variables.  library(parallel)\n\n# air = read.csv(url(\"http://packages.revolutionanalytics.com/datasets/AirOnTimeCSV2012/airOT201201.csv\"))  # load again the air data.table from http\nload(\"./air.rda\")   # read saved R object from file \"air.rda\"\ndests = as.character(unique(air$DEST))\ncount_flights = function(x){length(which(air$DEST == x))}\n\n## set cluster characteristics -- get OAR nodes to use, type of communication\nnodes = scan(Sys.getenv(\"OAR_NODE_FILE\"), what=character(0))\noar_job_id = as.numeric(Sys.getenv(\"OAR_JOB_ID\"))\nconnector = paste0(\"OAR_JOB_ID=\", oar_job_id)\nconnector = paste0(connector, \" ~jemeras/bin/roarsh\")\ncomm_type = \"PSOCK\"  Then, setup the cluster.  ## set up the cluster\ncl = makeCluster(nodes, type = comm_type, rshcmd = connector)   \n## If a particular library  LIB  is needed, load it on the nodes with\n# clusterEvalQ(cl, { library( LIB ) })\n## or give the full environment with\n# clusterEvalQ(cl, sessionInfo())\n## export air dataset on all the nodes\nclusterExport(cl, varlist=c(\"air\"))  Do the parallel computation.  ## compute in parallel through sockets\nas.data.frame(cbind(dest=dests, nb=parLapply(cl, dests, count_flights)))  Finalize and cleanup things.  stopCluster(cl)  Exercise: Plot a speedup graph with different number of cores and/or machines used.  MPI Communications  In this section we will use the same example as previously but with MPI connections. \nFor that purpose we need to install two libraries:  rmpi  and  snow .  As we are using R compiled with Intel compiler we will have to specify manually some paths and which version of MPI we are using when installing  rmpi .   install.packages(\"Rmpi\",\n                   configure.args =\n                   c(paste(\"--with-Rmpi-include=\",Sys.getenv(\"EBROOTIMPI\"),\"/include64\",sep=\"\"),\n                     paste(\"--with-Rmpi-libpath=\",Sys.getenv(\"EBROOTIMPI\"),\"/lib64\",sep=\"\"),\n                     paste(\"--with-mpi=\",Sys.getenv(\"EBROOTIMPI\"),sep=\"\"),\n                     \"--with-Rmpi-type=OPENMPI\"))  NOTE : if the installation fails you can try using the module with  Rmpi  already installed:  module purge\nmodule load lang/R/3.2.0-ictce-7.3.5-Rmpi  Then, outside of R shell write a file named  parallelAirDests.R  with the following code.  Warning : when using parallelization in R, please keep in mind that communication is much slower than computation. Thus if your problem is involving a long computation then you are right to use it, otherwise, if your problem is a large quantity of data you must use  data.table  or  dplyr .  library(Rmpi)\nlibrary(snow)\n\n# Initiate the cluster\ncluster  - makeMPIcluster(length(readLines(Sys.getenv(\"OAR_NODE_FILE\"))))\n\n# Function that prints the hostname of the caller, just for fun\nsayhello  - function()\n{\n    info  - Sys.info()[c(\"nodename\", \"machine\")]\n    paste(\"Hello from\", info[1])\n}\n\n# Call the 'sayhello' function on each node of the cluster\nnames  - clusterCall(cluster, sayhello)\nprint(unlist(names))\n\n\n# Compute the number of flights for a given destination. \n# Same as previous examples but with MPI communications.\nload(\"~jemeras/data/air.rda\")\ndests = as.character(unique(air$DEST))\ncount_flights = function(x){length(which(air$DEST == x))}\n#as.data.frame(cbind(dest=dests, nb=lapply(dests, count_flights)))\nclusterExport(cluster, \"air\")\n\nprint(as.data.frame(cbind(dest=dests, nb=parLapply(cluster, dests, count_flights))))\n\n# Terminate the cluster\nstopCluster(cluster)  Then, still outside of R and on your job head node run:   mpirun -np 1 -machinefile $OAR_NODE_FILE Rscript parallelAirDests.R  You may find strange the  -np 1 , in fact this is because it is  snow  that manages the processes spawning.    Usefull links    CRAN Archive    CRAN HPC Packages    ggplot2 Documentation    Advanced R programming by Hadley Wickham", 
            "title": "Parallel R"
        }, 
        {
            "location": "/advanced/Bioinformatics/README/", 
            "text": "README.md\n\n\nCopyright (c) 2014, 2015 Valentin Plugaru \n and Sarah Diehl \n\n\n\n\nUL HPC Tutorial: Bioinformatics software on the UL HPC platform\n\n\nThe objective of this tutorial is to exemplify the execution of several\nBioinformatics packages on top of the \nUL HPC\n platform.\n\n\nThe targeted applications are:\n\n\n\n\nABySS\n\n\nGromacs\n\n\nBowtie2\n / \nTopHat\n\n\nmpiBLAST\n\n\n\n\nThe tutorial will:\n\n\n\n\nshow you how to load and run pre-configured versions of these applications on the clusters\n\n\nshow you how to download and use updated versions of Bowtie2/TopHat\n\n\ndiscuss the parallelization capabilities of these applications\n\n\n\n\nPrerequisites\n\n\nWhen you look at the \nsoftware page\n you will notice that some of the applications are part of the \nlcsb\n software set. The modules in this set are not visible be default. To use them within a job you have to do:\n\n\n(node)$\n module use $RESIF_ROOTINSTALL/lcsb/modules/all\n\n\n\nIf you want them to always be available, you can add the following line to your \n.bash_private\n:\n\n\ncommand -v module \n/dev/null 2\n1 \n module use $RESIF_ROOTINSTALL/lcsb/modules/all\n\n\n\nThis tutorial relies on several input files for the bioinformatics packages, thus you will need to download them\nbefore following the instructions in the next sections:\n\n\n    (gaia-frontend)$\n mkdir -p ~/bioinfo-tutorial/gromacs ~/bioinfo-tutorial/tophat ~/bioinfo-tutorial/mpiblast\n    (gaia-frontend)$\n cd ~/bioinfo-tutorial\n    (gaia-frontend)$\n wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/gromacs/pr.tpr -O gromacs/pr.tpr\n    (gaia-frontend)$\n wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/tophat/test_data.tar.gz -O tophat/test_data.tar.gz\n    (gaia-frontend)$\n wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/tophat/test2_path -O tophat/test2_path\n    (gaia-frontend)$\n wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/mpiblast/test.fa -O mpiblast/test.fa\n\n\n\nOr simply clone the full tutorials repository and make a link to the Bioinformatics tutorial:\n\n\n    (gaia-frontend)$\n git clone https://github.com/ULHPC/tutorials.git\n    (gaia-frontend)$\n ln -s tutorials/advanced/Bioinformatics/ ~/bioinfo-tutorial\n\n\n\nABySS\n\n\nDescription\n\n\nABySS\n: Assembly By Short Sequences\n\n\nABySS is a de novo, parallel, paired-end sequence assembler that is designed for short reads. \nThe single-processor version is useful for assembling genomes up to 100 Mbases in size. \nThe parallel version is implemented using MPI and is capable of assembling larger genomes \n[*]\n.\n\n\nExample\n\n\nThis example will be ran in an \ninteractive OAR session\n, with batch-mode executions\nbeing proposed later on as exercises.\n\n\n    # Connect to Gaia (Linux/OS X):\n    (yourmachine)$\n ssh access-gaia.uni.lu\n\n    # Request 1 full node in an interactive job:\n    (gaia-frontend)$\n oarsub -I -l nodes=1,walltime=00:30:00\n\n    # Check the ABySS versions installed on the clusters:\n    (node)$\n module avail 2\n1 | grep -i abyss\n\n    # Load a specific ABySS version:\n    (node)$\n module load bio/ABySS/1.5.2-goolf-1.4.10\n\n    # Check that it has been loaded, along with its dependencies:\n    (node)$\n module list\n\n    # All the ABySS binaries are now in your path (check with TAB autocompletion)\n    (node)$\n abyss-\nTAB\n\n\n\n\nIn the ABySS package only the \nABYSS-P\n application is parallelized using MPI and can be run on several cores (and across several nodes) using\nthe \nabyss-pe\n launcher.\n\n\n    # Create a test directory and go to it\n    (node)$\n mkdir ~/bioinfo-tutorial/abyss\n    (node)$\n cd ~/bioinfo-tutorial/abyss\n\n    # Set the input files' directory in the environment\n    (node)$\n export ABYSSINPDIR=/scratch/users/vplugaru/bioinfo-inputs/abyss\n\n    # Give a name to the experiment\n    (node)$\n export ABYSSNAME='abysstest'\n\n    # Set the number of cores to use based on OAR's host file\n    (node)$\n export ABYSSNPROC=$(cat $OAR_NODEFILE | wc -l)\n\n    # Launch the paired end assembler:\n    (node)$\n abyss-pe mpirun=\"mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE\" name=${ABYSSNAME} np=${ABYSSNPROC} k=31 n=10 lib=pairedend pairedend=\"${ABYSSINPDIR}/SRR001666_1.fastq.bz2 ${ABYSSINPDIR}/SRR001666_2.fastq.bz2\" \n ${ABYSSNAME}.out 2\n ${ABYSSNAME}.err\n\n\n\nQuestion: Why do we use the -x VARIABLE parameters for mpirun?\n\n\nSeveral options seen on the \nabyss-pe\n command line are crucial:\n\n\n\n\nwe explicitly set the mpirun command\n\n\nwe export several environment variables to all the remote nodes, otherwise required paths (for the binaries, libraries) would not be known by the MPI processes running there\n\n\nwe do not specify \n-np $ABYSSNPROC\n in the mpirun command, as it set with \nabyss-pe\n's np parameter and internally passed on to mpirun\n\n\n\n\nThe execution should take around 12 minutes, meanwhile we can check its progress by monitoring the .out/.err output files:\n\n\n     (gaia-frontend)$\n tail -f ~/bioinfo-tutorial/abyss/abysstest.*\n     # We exit the tail program with CTRL-C\n\n\n\nWe can also connect to the job (recall oarsub -C $JOBID) from a different terminal or Screen window and see the different ABySS phases with \nhtop\n.\n\n\nBecause the \nabyss-pe\n workflow (pipeline) includes several processing steps with different applications of which only ABYSS-P is MPI-parallel,\nthe speedup obtained by using more than one node will be limited to ABYSS-P's execution. Several of the other applications that are part of the \nprocessing stages are however parallelized using OpenMP and pthreads and will thus take advantage of the cores available on the node where\n\nabyss-pe\n was started.\n\n\nThe used input dataset is a well known \nIllumina run of E. coli\n.\n\n\nProposed exercises\n\n\nSeveral exercises are proposed for ABySS:\n\n\n\n\ncreate a launcher for ABySS using the commands shown in the previous section\n\n\nlaunch jobs using 1 node: 4, 8 and 12 cores, then 2 and 4 nodes and measure the speedup obtained\n\n\nunpack the two input files and place them on a node's /dev/shm, then rerun the experiment with 4, 8 and 12 cores and measure the speedup\n\n\n\n\nGROMACS\n\n\nDescription\n\n\nGROMACS\n: GROningen MAchine for Chemical Simulations\n\n\nGROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.\nIt is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers \n[*]\n.\n\n\nExample\n\n\nThis example will be ran in an \ninteractive OAR session\n, with batch-mode executions\nbeing proposed later on as exercises.\n\n\n    # Connect to Gaia (Linux/OS X):\n    (yourmachine)$\n ssh access-gaia.uni.lu\n\n    # Request 1 full node in an interactive job:\n    (gaia-frontend)$\n oarsub -I -l nodes=1,walltime=00:30:00\n\n    # Check the GROMACS versions installed on the clusters:\n    (node)$\n module avail 2\n1 | grep -i gromacs\n\n\n\nSeveral GROMACS builds are available, we will focus only on the ones corresponding to the version 4.6.5:\n\n\n\n\nbio/GROMACS/4.6.5-goolf-1.4.10-hybrid\n\n\nbio/GROMACS/4.6.5-goolf-1.4.10-mt\n\n\n\n\nWe notice that there is a \nhybrid\n and a \nmt\n version\n\n\n\n\nthe hybrid version is OpenMP and MPI-enabled, all binaries have a '_mpi' suffix\n\n\nthe mt version is only OpenMP-enabled, as such it can take advantage of only one node's cores (however it may be faster on\nsingle-node executions than the hybrid version)\n\n\n\n\nWe will perform our tests with the hybrid version:\n\n\n    # Load the MPI-enabled Gromacs, without CUDA support:\n    (node)$\n module load bio/GROMACS/4.6.5-goolf-1.4.10-hybrid\n\n    # Check that it has been loaded, along with its dependencies:\n    (node)$\n module list\n\n    # Check the capabilities of the mdrun binary, note its suffix:\n    (node)$\n mdrun_mpi -version 2\n/dev/null\n\n    # Go to the test directory\n    (node)$\n cd ~/bioinfo-tutorial/gromacs\n\n    # Set the number of OpenMP threads to 1\n    (node)$\n export OMP_NUM_THREADS=1\n\n    # Perform a position restrained Molecular Dynamics run\n    (node)$\n mpirun -np 12 -hostfile $OAR_NODEFILE -x OMP_NUM_THREADS -x PATH -x LD_LIBRARY_PATH mdrun_mpi -v -s pr -e pr -o pr -c after_pr -g prlog \n test.out 2\n1\n\n\n\nWe notice here that we are running \nmdrun_mpi\n in parallel with mpirun on 12 cores, and we explicitly export the OMP_NUM_THREADS\nvariable to any remote node such that only one thread per MPI process will be created.\n\n\nQuestion: What will happen if we do not set the number of OpenMP threads to 1?\n \n\n\nGROMACS has many parallelization options and several parameters can be tuned to give you better performance depending on your workflow, see the references in the last section of this tutorial.\n\n\nThe used input corresponds to the \nRibonuclease S-peptide\n example,\nwhich has been changed to perform 50k steps in the Molecular Dynamics run with position restraints on the peptide.\n\n\nProposed exercises\n\n\nSeveral exercises are proposed for GROMACS:\n\n\n\n\ncreate a launcher for GROMACS using the commands shown in the previous section\n\n\nlaunch jobs using 1 node: 1, 2, 4, 8, 10 and 12 cores and measure the speedup obtained\n\n\ncheck what happens when executing mdrun with 16 and 24 cores\n\n\nlaunch a job using one full node that has GPU cards and run the GPU-enabled GROMACS to see if a speedup is obtained\n\n\n\n\nBowtie2/TopHat\n\n\nDescription\n\n\nBowtie2\n: Fast and sensitive read alignment\n\n\nBowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes \n[*]\n.\n\n\nTopHat\n : A spliced read mapper for RNA-Seq\n\n\nTopHat is a program that aligns RNA-Seq reads to a genome in order to identify exon-exon splice junctions. It is built on the ultrafast short read mapping program Bowtie \n[*]\n.\n\n\nExample\n\n\nThis example will show you how to use the latest version of TopHat in conjunction with the latest Bowtie2, by using the \nversions prebuilt for Linux by the developers.\n\n\n    # Connect to Gaia (Linux/OS X):\n    (yourmachine)$\n ssh access-gaia.uni.lu\n\n    # Request 1 full node in an interactive job:\n    (gaia-frontend)$\n oarsub -I -l nodes=1,walltime=00:30:00\n\n    # Create a folder for the new software and go to it\n    (node)$\n mkdir $WORK/newsoft\n    (node)$\n cd $WORK/newsoft\n\n    # Download latest Bowtie2 and Tophat, plus the SAM tools dependency:\n    (node)$\n wget http://downloads.sourceforge.net/project/bowtie-bio/bowtie2/2.2.5/bowtie2-2.2.5-linux-x86_64.zip\n    (node)$\n wget http://ccb.jhu.edu/software/tophat/downloads/tophat-2.0.14.Linux_x86_64.tar.gz\n    (node)$\n wget http://downloads.sourceforge.net/project/samtools/samtools/1.2/samtools-1.2.tar.bz2\n\n    # Unpack the three archives\n    (node)$\n unzip bowtie2-2.2.5-linux-x86_64.zip\n    (node)$\n tar xzvf tophat-2.0.14.Linux_x86_64.tar.gz\n    (node)$\n tar xjvf samtools-1.2.tar.bz2\n\n    # SAM tools requires compilation\n    (node)$\n cd samtools-1.2 \n make \n cd ..\n\n    # Create a file containing the paths to the binaries, to be sourced when needed\n    (node)$\n echo \"export PATH=$WORK/newsoft/bowtie2-2.2.5:\\$PATH\" \n newsoft\n    (node)$\n echo \"export PATH=$WORK/newsoft/tophat-2.0.14.Linux_x86_64:\\$PATH\" \n newsoft\n    (node)$\n echo \"export PATH=$WORK/newsoft/samtools-1.2:\\$PATH\" \n newsoft\n    (node)$\n source newsoft\n\n    # You can now check that both main applications can be run:\n    (node)$\n bowtie2 --version\n    (node)$\n tophat2 --version\n\n\n\nNow we will make a quick TopHat test, using the provided sample files:\n\n\n    # Go to the test directory, unpack the sample dataset and go to it\n    (node)$\n cd ~/bioinfo-tutorial/tophat\n    (node)$\n tar xzvf test_data.tar.gz\n    (node)$\n cd test_data\n\n\n    # Launch TopHat, with Bowtie2 in serial mode\n    (node)$\n tophat -r 20 test_ref reads_1.fq reads_2.fq\n\n    # Launch TopHat, with Bowtie2 in parallel mode\n    (node)$\n tophat -p 12 -r 20 test_ref reads_1.fq reads_2.fq\n\n\n\nWe can see that for this fast execution, increasing the number of threads does not improve the calculation time due to the relatively high overhead of thread creation.\nNote that TopHat / Bowtie are not MPI applications and as such can take advantage of at most one compute node.\n\n\nNext, we will make a longer test, where it will be interesting to monitor the TopHat pipeline (with \nhtop\n for example) to see the transitions between the serial\nand parallel stages (left as an exercise).\n\n\n    # Load the file which will export $TOPHATTEST2 in the environment\n    (node)$\n source ~/bioinfo-tutorial/tophat/test2_path\n\n    # Launch TopHat, with Bowtie2 in parallel mode\n    (node)$\n tophat2 -p 12 -g 1 -r 200 --mate-std-dev 30 -o ./  $TOPHATTEST2/chr10.hs $TOPHATTEST2/SRR027888.SRR027890_chr10_1.fastq $TOPHATTEST2/SRR027888.SRR027890_chr10_2.fastq\n\n\n\nThe input data for the first test corresponds to the \nTopHat test set\n,\nwhile the second test is an example of aligning reads to the chromosome 10 of the human genome \nas given here\n.\n\n\nProposed exercises\n\n\nThe following exercises are proposed for TopHat/Bowtie2:\n\n\n\n\ncreate a launcher for TopHat using the commands shown in the previous section\n\n\nlaunch jobs with 1, 2, 4, 8 and 10 cores on one node, using the second test files, and measure the speedup obtained\n\n\n\n\nmpiBLAST\n\n\nDescription\n\n\nmpiBLAST\n: Open-Source Parallel BLAST\n\n\nmpiBLAST is a freely available, open-source, parallel implementation of NCBI BLAST. By efficiently utilizing distributed computational resources through database fragmentation, query segmentation, intelligent scheduling, and parallel I/O, mpiBLAST improves NCBI BLAST performance by several orders of magnitude while scaling to hundreds of processors  \n[*]\n.\n\n\nExample\n\n\nThis example will be ran in an \ninteractive OAR session\n, with batch-mode executions\nbeing proposed later on as exercises.\n\n\n    # Connect to Gaia (Linux/OS X):\n    (yourmachine)$\n ssh access-gaia.uni.lu\n\n    # Request 1 full node in an interactive job:\n    (gaia-frontend)$\n oarsub -I -l nodes=1,walltime=00:30:00\n\n    # Load the lcsb software set\n    (node)$\n module use $RESIF_ROOTINSTALL/lcsb/modules/all\n\n    # Check the mpiBLAST versions installed on the clusters:\n    (node)$\n module avail 2\n1 | grep -i mpiblast\n\n    # Load a specific mpiBLAST version:\n    (node)$\n module load bio/mpiBLAST/1.6.0-goolf-1.4.10\n\n    # Check that it has been loaded, along with its dependencies:\n    (node)$\n module list\n\n    # The mpiBLAST binaries should now be in your path\n    (node)$\n mpiformatdb --version\n    (node)$\n mpiblast --version\n\n\n\nmpiBLAST requires access to NCBI substitution matrices and pre-formatted BLAST databases. For the purposes of this tutorial, a FASTA (NR) \ndatabase has been formatted and split into 12 fragments, enabling the parallel alignment of a query against the database. \nA \n.ncbirc\n file containing the paths to the necessary data files can be downloaded from \nhere\n\nand placed in your $HOME directory (make sure to backup an existing $HOME/.ncbirc before overwriting it with the one in this tutorial).\n\n\nQuestion: Knowing that the databases can take tens of gigabytes, what is an appropriate storage location for them on the clusters?\n \n\n\nWe will run a test using mpiBLAST. Note that mpiBLAST requires running with at least 3 processes, 2 dedicated for scheduling tasks and \ncoordinating file output, with the additional processes performing the search.\n\n\n    # Go to the test directory and execute mpiBLAST with one core for search\n    (node)$\n cd ~/bioinfo-tutorial/mpiblast\n    (node)$\n mpirun -np 3 mpiblast -p blastp -d nr -i test.fa -o test.out\n\n    # Note the speedup when using a full node of 12 cores\n    (node)$\n mpirun -np 14 mpiblast -p blastp -d nr -i test.fa -o test.out\n\n\n\nProposed exercises\n\n\nThe following exercises are proposed for mpiBLAST:\n\n\n\n\ncreate a launcher for mpiBLAST, making sure to export the required environment to the remote nodes\n\n\nlaunch jobs with 8, 14 and 24 cores across two nodes and measure the speedup obtained\n\n\n\n\nUseful references\n\n\n\n\nABySS at SEQanswers wiki\n\n\nGromacs parallelization\n\n\nGromacs GPU acceleration\n\n\nGromacs USA workshop\n\n\nTutorial on GROMACS parallelization schemes", 
            "title": "Running bioinformatics software"
        }, 
        {
            "location": "/advanced/Bioinformatics/README/#ul-hpc-tutorial-bioinformatics-software-on-the-ul-hpc-platform", 
            "text": "The objective of this tutorial is to exemplify the execution of several\nBioinformatics packages on top of the  UL HPC  platform.  The targeted applications are:   ABySS  Gromacs  Bowtie2  /  TopHat  mpiBLAST   The tutorial will:   show you how to load and run pre-configured versions of these applications on the clusters  show you how to download and use updated versions of Bowtie2/TopHat  discuss the parallelization capabilities of these applications", 
            "title": "UL HPC Tutorial: Bioinformatics software on the UL HPC platform"
        }, 
        {
            "location": "/advanced/Bioinformatics/README/#prerequisites", 
            "text": "When you look at the  software page  you will notice that some of the applications are part of the  lcsb  software set. The modules in this set are not visible be default. To use them within a job you have to do:  (node)$  module use $RESIF_ROOTINSTALL/lcsb/modules/all  If you want them to always be available, you can add the following line to your  .bash_private :  command -v module  /dev/null 2 1   module use $RESIF_ROOTINSTALL/lcsb/modules/all  This tutorial relies on several input files for the bioinformatics packages, thus you will need to download them\nbefore following the instructions in the next sections:      (gaia-frontend)$  mkdir -p ~/bioinfo-tutorial/gromacs ~/bioinfo-tutorial/tophat ~/bioinfo-tutorial/mpiblast\n    (gaia-frontend)$  cd ~/bioinfo-tutorial\n    (gaia-frontend)$  wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/gromacs/pr.tpr -O gromacs/pr.tpr\n    (gaia-frontend)$  wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/tophat/test_data.tar.gz -O tophat/test_data.tar.gz\n    (gaia-frontend)$  wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/tophat/test2_path -O tophat/test2_path\n    (gaia-frontend)$  wget --no-check-certificate https://raw.github.com/ULHPC/tutorials/devel/advanced/Bioinformatics/mpiblast/test.fa -O mpiblast/test.fa  Or simply clone the full tutorials repository and make a link to the Bioinformatics tutorial:      (gaia-frontend)$  git clone https://github.com/ULHPC/tutorials.git\n    (gaia-frontend)$  ln -s tutorials/advanced/Bioinformatics/ ~/bioinfo-tutorial", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/advanced/Bioinformatics/README/#abyss", 
            "text": "Description  ABySS : Assembly By Short Sequences  ABySS is a de novo, parallel, paired-end sequence assembler that is designed for short reads. \nThe single-processor version is useful for assembling genomes up to 100 Mbases in size. \nThe parallel version is implemented using MPI and is capable of assembling larger genomes  [*] .  Example  This example will be ran in an  interactive OAR session , with batch-mode executions\nbeing proposed later on as exercises.      # Connect to Gaia (Linux/OS X):\n    (yourmachine)$  ssh access-gaia.uni.lu\n\n    # Request 1 full node in an interactive job:\n    (gaia-frontend)$  oarsub -I -l nodes=1,walltime=00:30:00\n\n    # Check the ABySS versions installed on the clusters:\n    (node)$  module avail 2 1 | grep -i abyss\n\n    # Load a specific ABySS version:\n    (node)$  module load bio/ABySS/1.5.2-goolf-1.4.10\n\n    # Check that it has been loaded, along with its dependencies:\n    (node)$  module list\n\n    # All the ABySS binaries are now in your path (check with TAB autocompletion)\n    (node)$  abyss- TAB   In the ABySS package only the  ABYSS-P  application is parallelized using MPI and can be run on several cores (and across several nodes) using\nthe  abyss-pe  launcher.      # Create a test directory and go to it\n    (node)$  mkdir ~/bioinfo-tutorial/abyss\n    (node)$  cd ~/bioinfo-tutorial/abyss\n\n    # Set the input files' directory in the environment\n    (node)$  export ABYSSINPDIR=/scratch/users/vplugaru/bioinfo-inputs/abyss\n\n    # Give a name to the experiment\n    (node)$  export ABYSSNAME='abysstest'\n\n    # Set the number of cores to use based on OAR's host file\n    (node)$  export ABYSSNPROC=$(cat $OAR_NODEFILE | wc -l)\n\n    # Launch the paired end assembler:\n    (node)$  abyss-pe mpirun=\"mpirun -x PATH -x LD_LIBRARY_PATH -hostfile $OAR_NODEFILE\" name=${ABYSSNAME} np=${ABYSSNPROC} k=31 n=10 lib=pairedend pairedend=\"${ABYSSINPDIR}/SRR001666_1.fastq.bz2 ${ABYSSINPDIR}/SRR001666_2.fastq.bz2\"   ${ABYSSNAME}.out 2  ${ABYSSNAME}.err  Question: Why do we use the -x VARIABLE parameters for mpirun?  Several options seen on the  abyss-pe  command line are crucial:   we explicitly set the mpirun command  we export several environment variables to all the remote nodes, otherwise required paths (for the binaries, libraries) would not be known by the MPI processes running there  we do not specify  -np $ABYSSNPROC  in the mpirun command, as it set with  abyss-pe 's np parameter and internally passed on to mpirun   The execution should take around 12 minutes, meanwhile we can check its progress by monitoring the .out/.err output files:       (gaia-frontend)$  tail -f ~/bioinfo-tutorial/abyss/abysstest.*\n     # We exit the tail program with CTRL-C  We can also connect to the job (recall oarsub -C $JOBID) from a different terminal or Screen window and see the different ABySS phases with  htop .  Because the  abyss-pe  workflow (pipeline) includes several processing steps with different applications of which only ABYSS-P is MPI-parallel,\nthe speedup obtained by using more than one node will be limited to ABYSS-P's execution. Several of the other applications that are part of the \nprocessing stages are however parallelized using OpenMP and pthreads and will thus take advantage of the cores available on the node where abyss-pe  was started.  The used input dataset is a well known  Illumina run of E. coli .  Proposed exercises  Several exercises are proposed for ABySS:   create a launcher for ABySS using the commands shown in the previous section  launch jobs using 1 node: 4, 8 and 12 cores, then 2 and 4 nodes and measure the speedup obtained  unpack the two input files and place them on a node's /dev/shm, then rerun the experiment with 4, 8 and 12 cores and measure the speedup", 
            "title": "ABySS"
        }, 
        {
            "location": "/advanced/Bioinformatics/README/#gromacs", 
            "text": "Description  GROMACS : GROningen MAchine for Chemical Simulations  GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles.\nIt is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers  [*] .  Example  This example will be ran in an  interactive OAR session , with batch-mode executions\nbeing proposed later on as exercises.      # Connect to Gaia (Linux/OS X):\n    (yourmachine)$  ssh access-gaia.uni.lu\n\n    # Request 1 full node in an interactive job:\n    (gaia-frontend)$  oarsub -I -l nodes=1,walltime=00:30:00\n\n    # Check the GROMACS versions installed on the clusters:\n    (node)$  module avail 2 1 | grep -i gromacs  Several GROMACS builds are available, we will focus only on the ones corresponding to the version 4.6.5:   bio/GROMACS/4.6.5-goolf-1.4.10-hybrid  bio/GROMACS/4.6.5-goolf-1.4.10-mt   We notice that there is a  hybrid  and a  mt  version   the hybrid version is OpenMP and MPI-enabled, all binaries have a '_mpi' suffix  the mt version is only OpenMP-enabled, as such it can take advantage of only one node's cores (however it may be faster on\nsingle-node executions than the hybrid version)   We will perform our tests with the hybrid version:      # Load the MPI-enabled Gromacs, without CUDA support:\n    (node)$  module load bio/GROMACS/4.6.5-goolf-1.4.10-hybrid\n\n    # Check that it has been loaded, along with its dependencies:\n    (node)$  module list\n\n    # Check the capabilities of the mdrun binary, note its suffix:\n    (node)$  mdrun_mpi -version 2 /dev/null\n\n    # Go to the test directory\n    (node)$  cd ~/bioinfo-tutorial/gromacs\n\n    # Set the number of OpenMP threads to 1\n    (node)$  export OMP_NUM_THREADS=1\n\n    # Perform a position restrained Molecular Dynamics run\n    (node)$  mpirun -np 12 -hostfile $OAR_NODEFILE -x OMP_NUM_THREADS -x PATH -x LD_LIBRARY_PATH mdrun_mpi -v -s pr -e pr -o pr -c after_pr -g prlog   test.out 2 1  We notice here that we are running  mdrun_mpi  in parallel with mpirun on 12 cores, and we explicitly export the OMP_NUM_THREADS\nvariable to any remote node such that only one thread per MPI process will be created.  Question: What will happen if we do not set the number of OpenMP threads to 1?    GROMACS has many parallelization options and several parameters can be tuned to give you better performance depending on your workflow, see the references in the last section of this tutorial.  The used input corresponds to the  Ribonuclease S-peptide  example,\nwhich has been changed to perform 50k steps in the Molecular Dynamics run with position restraints on the peptide.  Proposed exercises  Several exercises are proposed for GROMACS:   create a launcher for GROMACS using the commands shown in the previous section  launch jobs using 1 node: 1, 2, 4, 8, 10 and 12 cores and measure the speedup obtained  check what happens when executing mdrun with 16 and 24 cores  launch a job using one full node that has GPU cards and run the GPU-enabled GROMACS to see if a speedup is obtained", 
            "title": "GROMACS"
        }, 
        {
            "location": "/advanced/Bioinformatics/README/#bowtie2tophat", 
            "text": "Description  Bowtie2 : Fast and sensitive read alignment  Bowtie 2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences. It is particularly good at aligning reads of about 50 up to 100s or 1,000s of characters, and particularly good at aligning to relatively long (e.g. mammalian) genomes  [*] .  TopHat  : A spliced read mapper for RNA-Seq  TopHat is a program that aligns RNA-Seq reads to a genome in order to identify exon-exon splice junctions. It is built on the ultrafast short read mapping program Bowtie  [*] .  Example  This example will show you how to use the latest version of TopHat in conjunction with the latest Bowtie2, by using the \nversions prebuilt for Linux by the developers.      # Connect to Gaia (Linux/OS X):\n    (yourmachine)$  ssh access-gaia.uni.lu\n\n    # Request 1 full node in an interactive job:\n    (gaia-frontend)$  oarsub -I -l nodes=1,walltime=00:30:00\n\n    # Create a folder for the new software and go to it\n    (node)$  mkdir $WORK/newsoft\n    (node)$  cd $WORK/newsoft\n\n    # Download latest Bowtie2 and Tophat, plus the SAM tools dependency:\n    (node)$  wget http://downloads.sourceforge.net/project/bowtie-bio/bowtie2/2.2.5/bowtie2-2.2.5-linux-x86_64.zip\n    (node)$  wget http://ccb.jhu.edu/software/tophat/downloads/tophat-2.0.14.Linux_x86_64.tar.gz\n    (node)$  wget http://downloads.sourceforge.net/project/samtools/samtools/1.2/samtools-1.2.tar.bz2\n\n    # Unpack the three archives\n    (node)$  unzip bowtie2-2.2.5-linux-x86_64.zip\n    (node)$  tar xzvf tophat-2.0.14.Linux_x86_64.tar.gz\n    (node)$  tar xjvf samtools-1.2.tar.bz2\n\n    # SAM tools requires compilation\n    (node)$  cd samtools-1.2   make   cd ..\n\n    # Create a file containing the paths to the binaries, to be sourced when needed\n    (node)$  echo \"export PATH=$WORK/newsoft/bowtie2-2.2.5:\\$PATH\"   newsoft\n    (node)$  echo \"export PATH=$WORK/newsoft/tophat-2.0.14.Linux_x86_64:\\$PATH\"   newsoft\n    (node)$  echo \"export PATH=$WORK/newsoft/samtools-1.2:\\$PATH\"   newsoft\n    (node)$  source newsoft\n\n    # You can now check that both main applications can be run:\n    (node)$  bowtie2 --version\n    (node)$  tophat2 --version  Now we will make a quick TopHat test, using the provided sample files:      # Go to the test directory, unpack the sample dataset and go to it\n    (node)$  cd ~/bioinfo-tutorial/tophat\n    (node)$  tar xzvf test_data.tar.gz\n    (node)$  cd test_data\n\n\n    # Launch TopHat, with Bowtie2 in serial mode\n    (node)$  tophat -r 20 test_ref reads_1.fq reads_2.fq\n\n    # Launch TopHat, with Bowtie2 in parallel mode\n    (node)$  tophat -p 12 -r 20 test_ref reads_1.fq reads_2.fq  We can see that for this fast execution, increasing the number of threads does not improve the calculation time due to the relatively high overhead of thread creation.\nNote that TopHat / Bowtie are not MPI applications and as such can take advantage of at most one compute node.  Next, we will make a longer test, where it will be interesting to monitor the TopHat pipeline (with  htop  for example) to see the transitions between the serial\nand parallel stages (left as an exercise).      # Load the file which will export $TOPHATTEST2 in the environment\n    (node)$  source ~/bioinfo-tutorial/tophat/test2_path\n\n    # Launch TopHat, with Bowtie2 in parallel mode\n    (node)$  tophat2 -p 12 -g 1 -r 200 --mate-std-dev 30 -o ./  $TOPHATTEST2/chr10.hs $TOPHATTEST2/SRR027888.SRR027890_chr10_1.fastq $TOPHATTEST2/SRR027888.SRR027890_chr10_2.fastq  The input data for the first test corresponds to the  TopHat test set ,\nwhile the second test is an example of aligning reads to the chromosome 10 of the human genome  as given here .  Proposed exercises  The following exercises are proposed for TopHat/Bowtie2:   create a launcher for TopHat using the commands shown in the previous section  launch jobs with 1, 2, 4, 8 and 10 cores on one node, using the second test files, and measure the speedup obtained", 
            "title": "Bowtie2/TopHat"
        }, 
        {
            "location": "/advanced/Bioinformatics/README/#mpiblast", 
            "text": "Description  mpiBLAST : Open-Source Parallel BLAST  mpiBLAST is a freely available, open-source, parallel implementation of NCBI BLAST. By efficiently utilizing distributed computational resources through database fragmentation, query segmentation, intelligent scheduling, and parallel I/O, mpiBLAST improves NCBI BLAST performance by several orders of magnitude while scaling to hundreds of processors   [*] .  Example  This example will be ran in an  interactive OAR session , with batch-mode executions\nbeing proposed later on as exercises.      # Connect to Gaia (Linux/OS X):\n    (yourmachine)$  ssh access-gaia.uni.lu\n\n    # Request 1 full node in an interactive job:\n    (gaia-frontend)$  oarsub -I -l nodes=1,walltime=00:30:00\n\n    # Load the lcsb software set\n    (node)$  module use $RESIF_ROOTINSTALL/lcsb/modules/all\n\n    # Check the mpiBLAST versions installed on the clusters:\n    (node)$  module avail 2 1 | grep -i mpiblast\n\n    # Load a specific mpiBLAST version:\n    (node)$  module load bio/mpiBLAST/1.6.0-goolf-1.4.10\n\n    # Check that it has been loaded, along with its dependencies:\n    (node)$  module list\n\n    # The mpiBLAST binaries should now be in your path\n    (node)$  mpiformatdb --version\n    (node)$  mpiblast --version  mpiBLAST requires access to NCBI substitution matrices and pre-formatted BLAST databases. For the purposes of this tutorial, a FASTA (NR) \ndatabase has been formatted and split into 12 fragments, enabling the parallel alignment of a query against the database. \nA  .ncbirc  file containing the paths to the necessary data files can be downloaded from  here \nand placed in your $HOME directory (make sure to backup an existing $HOME/.ncbirc before overwriting it with the one in this tutorial).  Question: Knowing that the databases can take tens of gigabytes, what is an appropriate storage location for them on the clusters?    We will run a test using mpiBLAST. Note that mpiBLAST requires running with at least 3 processes, 2 dedicated for scheduling tasks and \ncoordinating file output, with the additional processes performing the search.      # Go to the test directory and execute mpiBLAST with one core for search\n    (node)$  cd ~/bioinfo-tutorial/mpiblast\n    (node)$  mpirun -np 3 mpiblast -p blastp -d nr -i test.fa -o test.out\n\n    # Note the speedup when using a full node of 12 cores\n    (node)$  mpirun -np 14 mpiblast -p blastp -d nr -i test.fa -o test.out  Proposed exercises  The following exercises are proposed for mpiBLAST:   create a launcher for mpiBLAST, making sure to export the required environment to the remote nodes  launch jobs with 8, 14 and 24 cores across two nodes and measure the speedup obtained", 
            "title": "mpiBLAST"
        }, 
        {
            "location": "/advanced/Bioinformatics/README/#useful-references", 
            "text": "ABySS at SEQanswers wiki  Gromacs parallelization  Gromacs GPU acceleration  Gromacs USA workshop  Tutorial on GROMACS parallelization schemes", 
            "title": "Useful references"
        }, 
        {
            "location": "/advanced/Galaxy/README/", 
            "text": "Copyright (c) 2015 Sarah Diehl \n\n\n\n\nGalaxy Introduction Exercise: From Peaks to Genes\n\n\nIntroduction\n\n\nSlides as PDF: \nIntroduction to Galaxy\n\n\nFor a version of this tutorial with the results of important steps embedded and direct links to workflows, go to the Galaxy server, select \"Shared Data\" and then \"Published Pages\" from the top menu.\n\n\nScenario\n\n\nWe stumbled upon a paper (\nLi et al., Cell Stem Cell 2012\n) that contains the analysis of possible target genes of an interesting protein. The targets were obtained by ChIP-seq and the raw data is available through \nGEO\n. The list of genes however is neither in the supplement of the paper nor part of the GEO submission. The closest thing we can find is a list of the regions where the signal is significantly enriched (peaks). The goal of this exercise is to turn this list of genomic regions into a list of possible target genes.\n\n\n(Disclaimer: We are not affiliated with the authors of the paper and we don't make a statement about the relevance or quality of the paper. It is just a fitting example and nothing else.)\n\n\nStep 1: Upload peaks\n\n\nDownload the list of peaks (the file \"GSE37268_mof3.out.hpeak.txt.gz\") from GEO (\nclick here to get to the GEO entry\n) to your computer. Use the upload button to upload the file to Galaxy and select \"mm9\" as the genome. Galaxy will automatically unpack the file.\n\n\nThis file is not in any standard format and just by looking at it, we cannot find out what the numbers in the different columns mean. In the paper they mention that they used the peak caller \nHPeak\n. By looking at the HPeak manual we can find out that the columns contain the following information:\n\n\n\n\nchromosome name*\n\n\nstart coordinate\n\n\nend coordinate\n\n\nlength\n\n\nlocation within the peak that has the highest hypothetical DNA fragment coverage (summit)\n\n\nnot relevant\n\n\nnot relevant\n\n\n\n\n(*Note that the first column only contains the chromosome number, and X and Y are replaced by 20 and 21 for easier numerical manipulations.)\n\n\nStep 2: Get genes from UCSC\n\n\nWe also need a list of genes in mouse, which we can obtain from UCSC. Galaxy has the UCSC table browser integrated as a tool, so we don't need to download the data to our computers.\n\n\n\n\nTool: Get Data -\n UCSC main table browser\n\n\nSelect clade \"Mammal\", genome \"Mouse\", assembly \"mm9\"\n\n\nSelect group \"Genes and Gene Prediction Tracks\", track \"RefSeq Genes\"\n\n\nSelect table \"refGene\"\n\n\nSelect region \"genome\"\n\n\nSelect output format \"BED\"\n\n\nClick button \"get output\"\n\n\nClick button \"Send query to Galaxy\"\n\n\n\n\nStep 3: Adjust chromosome naming\n\n\nHave a look at both input files (either in the little preview window in the history or click on the eye icon to see one in the main frame) and find out what are the differences in the chromosome naming.\n\n\nApply the following workflow to GSE37268_mof3.out.hpeak.txt: Workflow 'Add \"chr\" at beginning of each line'.\n\n\nAfter importing you can in the future use it by scrolling to the bottom of the tool panel, click on \"All workflows\" and then on the workflow name.\n\n\nFrom carefully reading the HPeak manual, we should remember that it puts \"20\" and \"21\" instead of \"X\" and \"Y\". So now the chromosome names all start properly with \"chr\", but we still have \"chr20\" and \"chr21\" instead of \"chrX\" and \"chrY\".\n\n\n\n\nTool: Text Manipulation -\n Replace text in a specific column\n\n\nInput: result of workflow (Text reformatting on data X)\n\n\nIn colum: Column 1\n\n\nFind pattern: chr20\n\n\nReplace with: chrX\n\n\nDo the same for \"chr21\" and \"chrY\", make sure you use the result of the first replacement as input (use rerun button and change input and search/replace)\n\n\n\n\nMake sure the format of the output file is \"interval\", otherwise change it by clicking the pencil icon (do not convert to new format, but change data type).\n\n\nStep 4: Visualize peaks\n\n\nTo visualize the peaks it's best to convert them to BED format first, because most viewers cannot deal with interval (because interval format just exists in Galaxy).\n\n\n\n\nClick on the pencil icon of the latest dataset\n\n\nUnder the header \"Convert to new format\" select \"Convert Genomic Intervals to BED\"\n\n\nClick \"Convert\"\n\n\nLook at the new dataset. Some columns with generic names have been added and others were removed to comply to BED format rules.\n\n\nThis generated a new dataset in BED format which we'll use for visualization. We will however continue to work with the interval dataset.\n\n\n\n\nDisplay in IGB:\n\n\n\n\nGo to the \nIGB website\n\n\nDownload and install the Integrated Genome Browser on your computer\n\n\nStart IGB and in the right panel select species \"Mus musculus\" and  genome version \"M_musculus_Jul_2007\"\n\n\nGo back to Galaxy\n\n\nClick on the link \"View\" after \"display with IGB\" (expanded history view of BED dataset)\n\n\nType in your HPC credentials again, to allow IGB to access the data (you might also need to allow some connections and/or accept certificates)\n\n\nBack in IGB, click \"Load Data\" next to the scroll bar on top to get to see the new track\n\n\n\n\nStep 5: Add promoter region to gene records\n\n\n\n\nTool: Operate on Genomic Intervals -\n Get flanks\n\n\nInput dataset: RefSeq genes from UCSC (UCSC Main on Mouse: refGene (genome))\n\n\nOptions: Region: \"Around Start\", Location: \"Upstream\",  Offset: 10000, Length: 12000\n\n\n\n\nInspect the resulting BED file and through comparing with the input find out what this operation actually did. Just look at the contents and compare the rows in the input to the rows in the output to find out how the start and end positions changed. Rename the dataset (by clicking on the pencil icon) to reflect your findings.\n\n\nStep 6: Find overlaps\n\n\n\n\nTool: Operate on Genomic Intervals -\n Intersect\n\n\nReturn: Overlapping Intervals\n\n\nof: result of step 5 (Get flanks on data X)\n\n\nthat intersect: result of step 3 (second Replace text)\n\n\n\n\nThe order of the inputs is important! We want to end up with a list of genes, so the corresponding dataset needs to be the first input.\n\n\nStep 7: Count genes on different chromosomes\n\n\nTo get a better overview of the genes we obtained, we want to look at their distribution across the different chromosomes.\n\n\n\n\nTool: Statistics -\n Count occurrences of each record\n\n\nInput: result from step 6 (Intersect on data X and data X)\n\n\nSelect column 1 (c1) with the chromosome names\n\n\n\n\nStep 8: Draw barchart\n\n\n\n\nTool: Bar chart (use tool search to find it)\n\n\nInput: result of step 7\n\n\nUse X Tick labels: Yes, column 2\n\n\nNumerical column: c1\n\n\nPlot title is up to you\n\n\nLabel for Y axis: number of genes\n\n\n\n\nGalaxy has a second option to visualise tabular data, with built-in dynamic visualisations:\n\n\n\n\nExpand the dataset view and click on the visualization icon\n\n\nChoose \"Charts\"\n\n\nEnter a chart title, e.g. \"Genes on different chromsomes\"\n\n\nSelect \"Bar diagrams\" -\n \"Regular\"\n\n\nOn the top, click on \"Add Data\"\n\n\nEnter a label, e.g. \"count\"\n\n\nValues for x-axis: Column: 2 [str]\n\n\nValues for y-axis: Column: 1 [int]\n\n\nOn the very top, click \"Draw\"\n\n\n\n\nStep 9: Name your history\n\n\nIn the history column click on \"Unnamed history\" at the top to rename it.\n\n\nStep 10: Make a workflow out of steps 6 to 8\n\n\n\n\nClick on the history options and select \"Extract workflow\"\n\n\nOn the top click on \"Uncheck all\", then specifically check \"Treat as input dataset\" on GSE37268_mof3.out.hpeak.txt and UCSC Main on Mouse: refGene (genome), as well as \"Include\" on the Intersect, Count and Bar chart\n\n\nClick \"Create Workflow\"\n\n\n\n\nTo make sure our workflow is correct we look at it in the editor and make some small adjustments.\n\n\n\n\nTop menu: Workflow\n\n\nClick on the name of your new workflow and select \"Edit\"\n\n\n\n\nThe individual steps are displayed as boxes and their outputs and inputs are connected through lines. When you click on a box you see the tool options on the right. Besides the tools you should see two additional boxes titled \"Input dataset\". These represent the data we want to feed into our workflow. Although we have our two inputs in the workflow they are missing their connection to the first tool (Intersect), because we didn't carry over the intermediate steps. Connect each input dataset to the Intersect tool by dragging the arrow pointing outwards on the right of its box (which denotes an output) to an arrow on the left of the Intersect box pointing inwards (which denotes an input). Connect each input dataset with a different input of Intersect.\n\n\nYou should also change the names of the input datasets to remember that the first one contains genes and the second one peaks. Don't forget to save it in the end by clicking on \"Options\" (top right) and selecting \"Save\".\n\n\nStep 11: Share workflow\n\n\nShare your new workflow with the person to your left.\n\n\n\n\nTop menu: Workflow\n\n\nClick on your workflow's name and select \"Share or publish\"\n\n\nClick \"Share with a user\"\n\n\nEnter the username of the person to your left\n\n\nHit \"Share\"\n\n\nWait for the person on your right to do the same\n\n\nReload the workflows by clicking again on \"Workflow\" in the top menu\n\n\nUnder the header \"Workflows shared with you by others\" you should now see your right neighbour's workflow\n\n\nClick on its name and select \"View\"\n\n\nCompare with your workflow\n\n\n\n\nStep 12: Cleaning up\n\n\nDownload your workflow:\n\n\n\n\nTop menu: Workflow\n\n\nClick on your workflow's name and select \"Download or Export\"\n\n\nClick on \"Download workflow to file so that it can be saved or imported into another Galaxy server\"\n\n\nSave the workflow file on your computer\n\n\n\n\nClean up history:\n\nDelete all datasets that are neither initial input nor final results. Everything that can be easily recreated or is just part of an intermediate step can go. What I would keep are the extended genes, the intersect result and the bar chart (for a real analysis I would recommend to also download all final results). Deleted datasets can be undeleted for some time (see history options) and will only be ultimately removed from the server if they aren't used somewhere else or by somebody else and stay deleted for several weeks.\n\n\nYou can create new histories in the history options with \"Create New\".\n\n\nTo delete old histories:\n\n\n\n\nHistory options: Saved histories\n\n\nCheck the history you want to delete\n\n\nClick \"Delete Permanently\" on the bottom if you need to free up space or just \"Delete\"\n\n\n\n\nEND", 
            "title": "Galaxy"
        }, 
        {
            "location": "/advanced/Galaxy/README/#galaxy-introduction-exercise-from-peaks-to-genes", 
            "text": "", 
            "title": "Galaxy Introduction Exercise: From Peaks to Genes"
        }, 
        {
            "location": "/advanced/Galaxy/README/#introduction", 
            "text": "Slides as PDF:  Introduction to Galaxy  For a version of this tutorial with the results of important steps embedded and direct links to workflows, go to the Galaxy server, select \"Shared Data\" and then \"Published Pages\" from the top menu.", 
            "title": "Introduction"
        }, 
        {
            "location": "/advanced/Galaxy/README/#scenario", 
            "text": "We stumbled upon a paper ( Li et al., Cell Stem Cell 2012 ) that contains the analysis of possible target genes of an interesting protein. The targets were obtained by ChIP-seq and the raw data is available through  GEO . The list of genes however is neither in the supplement of the paper nor part of the GEO submission. The closest thing we can find is a list of the regions where the signal is significantly enriched (peaks). The goal of this exercise is to turn this list of genomic regions into a list of possible target genes.  (Disclaimer: We are not affiliated with the authors of the paper and we don't make a statement about the relevance or quality of the paper. It is just a fitting example and nothing else.)", 
            "title": "Scenario"
        }, 
        {
            "location": "/advanced/Galaxy/README/#step-1-upload-peaks", 
            "text": "Download the list of peaks (the file \"GSE37268_mof3.out.hpeak.txt.gz\") from GEO ( click here to get to the GEO entry ) to your computer. Use the upload button to upload the file to Galaxy and select \"mm9\" as the genome. Galaxy will automatically unpack the file.  This file is not in any standard format and just by looking at it, we cannot find out what the numbers in the different columns mean. In the paper they mention that they used the peak caller  HPeak . By looking at the HPeak manual we can find out that the columns contain the following information:   chromosome name*  start coordinate  end coordinate  length  location within the peak that has the highest hypothetical DNA fragment coverage (summit)  not relevant  not relevant   (*Note that the first column only contains the chromosome number, and X and Y are replaced by 20 and 21 for easier numerical manipulations.)", 
            "title": "Step 1: Upload peaks"
        }, 
        {
            "location": "/advanced/Galaxy/README/#step-2-get-genes-from-ucsc", 
            "text": "We also need a list of genes in mouse, which we can obtain from UCSC. Galaxy has the UCSC table browser integrated as a tool, so we don't need to download the data to our computers.   Tool: Get Data -  UCSC main table browser  Select clade \"Mammal\", genome \"Mouse\", assembly \"mm9\"  Select group \"Genes and Gene Prediction Tracks\", track \"RefSeq Genes\"  Select table \"refGene\"  Select region \"genome\"  Select output format \"BED\"  Click button \"get output\"  Click button \"Send query to Galaxy\"", 
            "title": "Step 2: Get genes from UCSC"
        }, 
        {
            "location": "/advanced/Galaxy/README/#step-3-adjust-chromosome-naming", 
            "text": "Have a look at both input files (either in the little preview window in the history or click on the eye icon to see one in the main frame) and find out what are the differences in the chromosome naming.  Apply the following workflow to GSE37268_mof3.out.hpeak.txt: Workflow 'Add \"chr\" at beginning of each line'.  After importing you can in the future use it by scrolling to the bottom of the tool panel, click on \"All workflows\" and then on the workflow name.  From carefully reading the HPeak manual, we should remember that it puts \"20\" and \"21\" instead of \"X\" and \"Y\". So now the chromosome names all start properly with \"chr\", but we still have \"chr20\" and \"chr21\" instead of \"chrX\" and \"chrY\".   Tool: Text Manipulation -  Replace text in a specific column  Input: result of workflow (Text reformatting on data X)  In colum: Column 1  Find pattern: chr20  Replace with: chrX  Do the same for \"chr21\" and \"chrY\", make sure you use the result of the first replacement as input (use rerun button and change input and search/replace)   Make sure the format of the output file is \"interval\", otherwise change it by clicking the pencil icon (do not convert to new format, but change data type).", 
            "title": "Step 3: Adjust chromosome naming"
        }, 
        {
            "location": "/advanced/Galaxy/README/#step-4-visualize-peaks", 
            "text": "To visualize the peaks it's best to convert them to BED format first, because most viewers cannot deal with interval (because interval format just exists in Galaxy).   Click on the pencil icon of the latest dataset  Under the header \"Convert to new format\" select \"Convert Genomic Intervals to BED\"  Click \"Convert\"  Look at the new dataset. Some columns with generic names have been added and others were removed to comply to BED format rules.  This generated a new dataset in BED format which we'll use for visualization. We will however continue to work with the interval dataset.   Display in IGB:   Go to the  IGB website  Download and install the Integrated Genome Browser on your computer  Start IGB and in the right panel select species \"Mus musculus\" and  genome version \"M_musculus_Jul_2007\"  Go back to Galaxy  Click on the link \"View\" after \"display with IGB\" (expanded history view of BED dataset)  Type in your HPC credentials again, to allow IGB to access the data (you might also need to allow some connections and/or accept certificates)  Back in IGB, click \"Load Data\" next to the scroll bar on top to get to see the new track", 
            "title": "Step 4: Visualize peaks"
        }, 
        {
            "location": "/advanced/Galaxy/README/#step-5-add-promoter-region-to-gene-records", 
            "text": "Tool: Operate on Genomic Intervals -  Get flanks  Input dataset: RefSeq genes from UCSC (UCSC Main on Mouse: refGene (genome))  Options: Region: \"Around Start\", Location: \"Upstream\",  Offset: 10000, Length: 12000   Inspect the resulting BED file and through comparing with the input find out what this operation actually did. Just look at the contents and compare the rows in the input to the rows in the output to find out how the start and end positions changed. Rename the dataset (by clicking on the pencil icon) to reflect your findings.", 
            "title": "Step 5: Add promoter region to gene records"
        }, 
        {
            "location": "/advanced/Galaxy/README/#step-6-find-overlaps", 
            "text": "Tool: Operate on Genomic Intervals -  Intersect  Return: Overlapping Intervals  of: result of step 5 (Get flanks on data X)  that intersect: result of step 3 (second Replace text)   The order of the inputs is important! We want to end up with a list of genes, so the corresponding dataset needs to be the first input.", 
            "title": "Step 6: Find overlaps"
        }, 
        {
            "location": "/advanced/Galaxy/README/#step-7-count-genes-on-different-chromosomes", 
            "text": "To get a better overview of the genes we obtained, we want to look at their distribution across the different chromosomes.   Tool: Statistics -  Count occurrences of each record  Input: result from step 6 (Intersect on data X and data X)  Select column 1 (c1) with the chromosome names", 
            "title": "Step 7: Count genes on different chromosomes"
        }, 
        {
            "location": "/advanced/Galaxy/README/#step-8-draw-barchart", 
            "text": "Tool: Bar chart (use tool search to find it)  Input: result of step 7  Use X Tick labels: Yes, column 2  Numerical column: c1  Plot title is up to you  Label for Y axis: number of genes   Galaxy has a second option to visualise tabular data, with built-in dynamic visualisations:   Expand the dataset view and click on the visualization icon  Choose \"Charts\"  Enter a chart title, e.g. \"Genes on different chromsomes\"  Select \"Bar diagrams\" -  \"Regular\"  On the top, click on \"Add Data\"  Enter a label, e.g. \"count\"  Values for x-axis: Column: 2 [str]  Values for y-axis: Column: 1 [int]  On the very top, click \"Draw\"", 
            "title": "Step 8: Draw barchart"
        }, 
        {
            "location": "/advanced/Galaxy/README/#step-9-name-your-history", 
            "text": "In the history column click on \"Unnamed history\" at the top to rename it.", 
            "title": "Step 9: Name your history"
        }, 
        {
            "location": "/advanced/Galaxy/README/#step-10-make-a-workflow-out-of-steps-6-to-8", 
            "text": "Click on the history options and select \"Extract workflow\"  On the top click on \"Uncheck all\", then specifically check \"Treat as input dataset\" on GSE37268_mof3.out.hpeak.txt and UCSC Main on Mouse: refGene (genome), as well as \"Include\" on the Intersect, Count and Bar chart  Click \"Create Workflow\"   To make sure our workflow is correct we look at it in the editor and make some small adjustments.   Top menu: Workflow  Click on the name of your new workflow and select \"Edit\"   The individual steps are displayed as boxes and their outputs and inputs are connected through lines. When you click on a box you see the tool options on the right. Besides the tools you should see two additional boxes titled \"Input dataset\". These represent the data we want to feed into our workflow. Although we have our two inputs in the workflow they are missing their connection to the first tool (Intersect), because we didn't carry over the intermediate steps. Connect each input dataset to the Intersect tool by dragging the arrow pointing outwards on the right of its box (which denotes an output) to an arrow on the left of the Intersect box pointing inwards (which denotes an input). Connect each input dataset with a different input of Intersect.  You should also change the names of the input datasets to remember that the first one contains genes and the second one peaks. Don't forget to save it in the end by clicking on \"Options\" (top right) and selecting \"Save\".", 
            "title": "Step 10: Make a workflow out of steps 6 to 8"
        }, 
        {
            "location": "/advanced/Galaxy/README/#step-11-share-workflow", 
            "text": "Share your new workflow with the person to your left.   Top menu: Workflow  Click on your workflow's name and select \"Share or publish\"  Click \"Share with a user\"  Enter the username of the person to your left  Hit \"Share\"  Wait for the person on your right to do the same  Reload the workflows by clicking again on \"Workflow\" in the top menu  Under the header \"Workflows shared with you by others\" you should now see your right neighbour's workflow  Click on its name and select \"View\"  Compare with your workflow", 
            "title": "Step 11: Share workflow"
        }, 
        {
            "location": "/advanced/Galaxy/README/#step-12-cleaning-up", 
            "text": "Download your workflow:   Top menu: Workflow  Click on your workflow's name and select \"Download or Export\"  Click on \"Download workflow to file so that it can be saved or imported into another Galaxy server\"  Save the workflow file on your computer   Clean up history: \nDelete all datasets that are neither initial input nor final results. Everything that can be easily recreated or is just part of an intermediate step can go. What I would keep are the extended genes, the intersect result and the bar chart (for a real analysis I would recommend to also download all final results). Deleted datasets can be undeleted for some time (see history options) and will only be ultimately removed from the server if they aren't used somewhere else or by somebody else and stay deleted for several weeks.  You can create new histories in the history options with \"Create New\".  To delete old histories:   History options: Saved histories  Check the history you want to delete  Click \"Delete Permanently\" on the bottom if you need to free up space or just \"Delete\"", 
            "title": "Step 12: Cleaning up"
        }, 
        {
            "location": "/advanced/Galaxy/README/#end", 
            "text": "", 
            "title": "END"
        }, 
        {
            "location": "/advanced/Allinea/README/", 
            "text": "HPC tutorial: Unified profiling and debugging with Allinea\n\n\nby P. Wohlschlegel\n\n\nDeveloping an HPC application designed for MPI or hybrid environments can be a very challenging task - especially when it comes to fixing bugs, optimizing workload or even resolving both type of issues simultaneously. Those challenges are made easier with Allinea Tools. Using Allinea environment, it is now possible for developers to adopt instantly efficient and scalable development tools and to focus immediately on their core activity : science.\n\n\nAllinea tools are available for you in University of Luxembourg, and this workshop has been arranged to help you get started with Allinea MAP and Allinea DDT. During this workshop, we will review the capabilities of Allinea tools and see, through hands-on exercises, how this environment can help reduce your time to results to the minimum.\n\n\nPresentation\n\n\nSlides\n\n\nExercise 1\n\n\nFiles:\n\n\n\n\nExercice 1 slides\n\n\ncstartmpi.c\n\n\nMakefile\n\n\n\n\nExercise 2\n\n\nFiles:\n\n\n\n\nExercise 2 slides\n\n\nslow.f90\n\n\nMakefile", 
            "title": "Allinea"
        }, 
        {
            "location": "/advanced/Allinea/README/#hpc-tutorial-unified-profiling-and-debugging-with-allinea", 
            "text": "by P. Wohlschlegel  Developing an HPC application designed for MPI or hybrid environments can be a very challenging task - especially when it comes to fixing bugs, optimizing workload or even resolving both type of issues simultaneously. Those challenges are made easier with Allinea Tools. Using Allinea environment, it is now possible for developers to adopt instantly efficient and scalable development tools and to focus immediately on their core activity : science.  Allinea tools are available for you in University of Luxembourg, and this workshop has been arranged to help you get started with Allinea MAP and Allinea DDT. During this workshop, we will review the capabilities of Allinea tools and see, through hands-on exercises, how this environment can help reduce your time to results to the minimum.", 
            "title": "HPC tutorial: Unified profiling and debugging with Allinea"
        }, 
        {
            "location": "/advanced/Allinea/README/#presentation", 
            "text": "Slides", 
            "title": "Presentation"
        }, 
        {
            "location": "/advanced/Allinea/README/#exercise-1", 
            "text": "Files:   Exercice 1 slides  cstartmpi.c  Makefile", 
            "title": "Exercise 1"
        }, 
        {
            "location": "/advanced/Allinea/README/#exercise-2", 
            "text": "Files:   Exercise 2 slides  slow.f90  Makefile", 
            "title": "Exercise 2"
        }, 
        {
            "location": "/advanced/TotalView/README/", 
            "text": "-\n- mode: markdown; mode: auto-fill; fill-column: 80 -\n-\n\n\nREADME.md\n\n\nCopyright (c) 2014 Sebastien Varrette \n\n\n    Time-stamp: \nMar 2014-05-06 13:15 svarrette\n\n\n\n\n\n\nUL HPC Tutorial: Direct, Reverse and parallel Memory debugging with TotalView\n\n\nThe objective of this tutorial is to get a brief overview of the \nTotalView\n, a GUI-based source code defect analysis tool that gives you unprecedented control over processes and thread execution and visibility into program state and variables.\n\n\nThis practical session will be organized as follows:\n\n\n\n\nStartup and Overview\n\n\nIHM Navigation and process control\n\n\nAction points (TP)\n\n\nExamination and Data Analysis (TP)\n\n\nDebugging Parallel Applications (TP)\n\n\nMemory reports using MemoryScape (TP)\n\n\nRemote Debugging\n\n\nCUDA Debugging\n\n\nXeon Phi Debugging\n\n\nMemory Debugging with MemoryScape\n\n\nEvents and memory errors (TP)\n\n\nDelayed Scripted debugging (non-interactive) (TP)\n\n\nReverse debugging using ReplayEngine (TP)\n\n\nAsynchronous control of Parallel Applications (TP)\n\n\nType Transformation (TP)\n\n\n\n\nWhile TotalView is available on the \nUL HPC Platform\n, the specific exercises proposed have been embedded into a Virtual Machine (VM) you will need to setup to run this practical session. \n\n\nPre-Requisites\n\n\n\n\nInstall \nOracle's VirtualBox\n\n\nDownload the bootable ISO \ntv_trainingcdU10_20140815.iso\n (md5sum: \n30a844ddda80ddf505c28eb4f4b6f1bf\n) which contains: \n\n\na bootable Linux Ubuntu distribition\n\n\na version of TotalView\n\n\nthe PDF documents describing the practical session.\n\n\n\n\nIf the participants did not have the time to download the ISO, they shall come with a USB stick having a capacity of at least 2GB.\n\n\nTo create a new VM: \n\n\n\n\nOpen VirtualBox\n\n\nSelect the \nNew\n icon\n\n\nName: \nTotalView\n\n\nType: \nLinux\n\n\nVersion: \nUbuntu 64 bits\n\n\nMemory Size: \n512MB\n\n\nCreate a virtual hard drive now\n\n\n\n\nClick on \"Create\": select a VDI format, dynamically allocated. \n* Now select the \nStart\n icon over the newly created VM\n  * open the folder icon to browse your disk and select the  \ntv_trainingcdU10_20140815.iso\n ISO on which you'll boot\n\n\nYou're now ready for the tutorial.\n\n\nTutorial\n\n\n\n\nContent of the tutorial\n\n\nAnswers of the tutorial", 
            "title": "TotalView"
        }, 
        {
            "location": "/advanced/TotalView/README/#ul-hpc-tutorial-direct-reverse-and-parallel-memory-debugging-with-totalview", 
            "text": "The objective of this tutorial is to get a brief overview of the  TotalView , a GUI-based source code defect analysis tool that gives you unprecedented control over processes and thread execution and visibility into program state and variables.  This practical session will be organized as follows:   Startup and Overview  IHM Navigation and process control  Action points (TP)  Examination and Data Analysis (TP)  Debugging Parallel Applications (TP)  Memory reports using MemoryScape (TP)  Remote Debugging  CUDA Debugging  Xeon Phi Debugging  Memory Debugging with MemoryScape  Events and memory errors (TP)  Delayed Scripted debugging (non-interactive) (TP)  Reverse debugging using ReplayEngine (TP)  Asynchronous control of Parallel Applications (TP)  Type Transformation (TP)   While TotalView is available on the  UL HPC Platform , the specific exercises proposed have been embedded into a Virtual Machine (VM) you will need to setup to run this practical session.", 
            "title": "UL HPC Tutorial: Direct, Reverse and parallel Memory debugging with TotalView"
        }, 
        {
            "location": "/advanced/TotalView/README/#pre-requisites", 
            "text": "Install  Oracle's VirtualBox  Download the bootable ISO  tv_trainingcdU10_20140815.iso  (md5sum:  30a844ddda80ddf505c28eb4f4b6f1bf ) which contains:   a bootable Linux Ubuntu distribition  a version of TotalView  the PDF documents describing the practical session.   If the participants did not have the time to download the ISO, they shall come with a USB stick having a capacity of at least 2GB.  To create a new VM:    Open VirtualBox  Select the  New  icon  Name:  TotalView  Type:  Linux  Version:  Ubuntu 64 bits  Memory Size:  512MB  Create a virtual hard drive now   Click on \"Create\": select a VDI format, dynamically allocated. \n* Now select the  Start  icon over the newly created VM\n  * open the folder icon to browse your disk and select the   tv_trainingcdU10_20140815.iso  ISO on which you'll boot  You're now ready for the tutorial.", 
            "title": "Pre-Requisites"
        }, 
        {
            "location": "/advanced/TotalView/README/#tutorial", 
            "text": "Content of the tutorial  Answers of the tutorial", 
            "title": "Tutorial"
        }, 
        {
            "location": "/advanced/Vagrant/README/", 
            "text": "UL HPC Tutorial: Create and reproduce work environments using Vagrant\n\n\nIntroduction\n\n\nVagrant is a tool that allows to easily and rapidly create and configure reproducible and portable work environments using Virtual Machines. This is especially useful if you want to test your work in a stable and controlled environment and minimize the various unintended or untrackable changes that may occur on a physical machine.\n\n\nIn this tutorial, we are going to explain the steps to install Vagrant and create your first basic Linux Virtual Machine with it.\n\n\nVagrant installation\n\n\nPrerequisite:\n\n\nVagrant can use many Virtual Machine providers such as \nVirtualBox\n, \nVMware\n and \nDocker\n with VirtualBox being the easiest to use, and the default option in Vagrant.\n\n\nOur first step is to install VirtualBox, you can download and install the correct version for your operating system from the \nofficial website\n. In many Linux distributions it is provided as a package from the standard repositories, thus you can use your usual package manager to install it.\n\n\nOnce this prerequisite is met, we can install Vagrant. Download the correct version for your operating system on the \nofficial website\n and install it.\n\n\nUsing Vagrant to create a Virtual Machine\n\n\nThe main advantage of Vagrant is that it lets you import and use pre-configured Virtual Machines (called \nboxes\n in this context) which can become bases for your own customizations (installed applications, libraries, etc). With Vagrant it becomes really fast and effortless to create and run a new Virtual Machine.\n\n\nThe Vagrant boxes contain the disk image of a VM without the virtual hardware details of the VM, which are initialized by Vagrant and can be edited by the user.\n\n\nThe first step is to choose a pre-configured box to use. It is possible to create your own from scratch yet this is not in the scope of the current tutorial.\n\nFreely available boxes can be found at the following two main sources:\n\n\n\n\nAtlas corps box catalog\n\n\nvagrantbox.es catalaog\n\n\n\n\nThe first catalog is the default box download location for Vagrant. This means that you can directly use the name of the boxes you find here with Vagrant (e.g. \nubuntu/trusty64\n).\n\nTo use the second catalog you would additionaly need to provide the source box URL, yet this catalog provides a much richer variety of boxes.\n\n\nAdding a new box\n\n\nTo add a box and make it usable in Vagrant, we are going to use the \nvagrant box add\n command. In the example below we will add one box from each of the catalogs in order to present the different possibilities.\n\nWe are going to add the \nubuntu/trusty64\n box from the Atlas catalog and the \nUbuntu 14.04\n box (by its \nurl\n) from the vagrantbox.es catalog.\n\n\nTo add the first box, we use the following command (which may take some time due to the time needed to download the box):\n\n\n    $\n vagrant box add ubuntu/trusty64\n    ==\n box: Loading metadata for box 'ubuntu/trusty64'\n        box: URL: https://vagrantcloud.com/ubuntu/trusty64\n    ==\n box: Adding box 'ubuntu/trusty64' (v14.04) for provider: virtualbox\n        box: Downloading: https://vagrantcloud.com/ubuntu/boxes/trusty64/versions/14.04/providers/virtualbox.box\n    ==\n box: Successfully added box 'ubuntu/trusty64' (v14.04) for 'virtualbox'!\n\n\n\nIn this case, you just had to give the name of the box and Vagrant found the box by itself and added the box under the \nubuntu/trusty64\n name.\n\n\nTo list the local boxes available to Vagrant for initialization of new VMs, we use the \nvagrant box list\n command:\n\n\n    $\n vagrant box list\n    ubuntu/trusty64    (virtualbox, 14.04)\n\n\n\nTo add the second box, you need to use a slightly different syntax since you need to precise the name you want to give to the box as well as its source URL:\n\n\n    $\n vagrant box add ubuntu14.04 https://github.com/kraksoft/vagrant-box-ubuntu/releases/download/14.04/ubuntu-14.04-amd64.box\n    ==\n box: Adding box 'ubuntu14.04' (v0) for provider: \n        box: Downloading: https://github.com/kraksoft/vagrant-box-ubuntu/releases/download/14.04/ubuntu-14.04-amd64.box\n    ==\n box: Successfully added box 'ubuntu14.04' (v0) for 'virtualbox'!\n\n\n\nNow a second box will be available to Vagrant under the name \nubuntu14.04\n:\n\n\n    $\n vagrant box list\n    ubuntu/trusty64    (virtualbox, 14.04)\n    ubuntu14.04        (virtualbox, 0)\n\n\n\nIn the rest of the tutorial we are only going to use the first box. To remove a box we use the \nvagrant box remove\n command as follows:\n\n\n    $\n vagrant box remove ubuntu14.04\n    Removing box 'ubuntu14.04' (v0) with provider 'virtualbox'...\n\n\n\nChecking that it has been removed:\n\n\n    $\n vagran box list\n    ubuntu/trusty64    (virtualbox, 14.04)\n\n\n\nCreating a new Virtual Machine\n\n\nNow we are going to create a new Virtual Machine using the \nubuntu/trusty64\n box. \nWe will initialize it in an empty directory (which is not absolutely mandatory):\n\n\n    $\n mkdir vagrant \n cd vagrant\n\n\n\nNext, we make Vagrant prepare the configuration file describing the VM:\n\n\n    $\n vagrant init ubuntu/trusty64\n    A `Vagrantfile` has been placed in this directory. You are now\n    ready to `vagrant up` your first virtual environment! Please read\n    the comments in the Vagrantfile as well as documentation on\n    `vagrantup.com` for more information on using Vagrant.\n\n\n\nYou should now see a file named \nVagrantfile\n in your directory. This file contains the minimal information for Vagrant to launch the VM. We could modify it to set up specific parameters of the VM (number of virtual cores, memory size, etc), but this constitutes advanced usage for which full documentation that can be found on the \nofficial site\n. However, it may be interesting to understand what is actually needed in this file, since it contains a lot of commented information.\n\nThe minimal content of a \nVagrantfile\n is as follows:\n\n\n    VAGRANTFILE_API_VERSION = \"2\"\n    Vagrant.configure(\"VAGRANTFILE_API_VERSION\") do |config|\n        config.vm.box = \"hashicorp/trusty64\"\n    end\n\n\n\nThis basically defines which version of the Vagrant API will be used to build the VM using the box given as a base.\n\n\nNow, to launch the VM you only need to use the single \nvagrant up\n command in the same directory where the \nVagrantfile\n exists (this may take some time since Vagrant is going to boot the VM and set its basic configuration):\n\n\n    $\n vagrant up\n    Bringing machine 'default' up with 'virtualbox' provider...\n    ==\n default: Importing base box 'ubuntu/trusty64'...\n    ==\n default: Matching MAC address for NAT networking...\n    ==\n default: Checking if box 'ubuntu/trusty64' is up to date...\n    ==\n default: Setting the name of the VM: vagrant_default_1425476252413_67101\n    ==\n default: Clearing any previously set forwarded ports...\n    ==\n default: Clearing any previously set network interfaces...\n    ==\n default: Preparing network interfaces based on configuration...\n        default: Adapter 1: nat\n    ==\n default: Forwarding ports...\n        default: 22 =\n 2222 (adapter 1)\n    ==\n default: Booting VM...\n    ==\n default: Waiting for machine to boot. This may take a few minutes...\n        default: SSH address: 127.0.0.1:2222\n        default: SSH username: vagrant\n        default: SSH auth method: private key\n        default: Warning: Connection timeout. Retrying...\n        default: Warning: Remote connection disconnect. Retrying...\n    ==\n default: Machine booted and ready!\n    ==\n default: Checking for guest additions in VM...\n    ==\n default: Mounting shared folders...\n        default: /vagrant =\n /tmp/vagrant\n\n\n\nYour VM is now up and running at this point. To access it, use the \nvagrant ssh\n command within the same directory :\n\n\n    $\n vagrant ssh\n\n\n\nYou should now be connected to your VM and ready to work.\n\n\nAn interesting feature of Vagrant is that your computer (the \"host\") shares the directory that contains the \nVagrantfile\n with your VM (the \"guest\"), where it is seen as \n/vagrant\n.\n\n\nAssuming you have a script or data files you want to access from within the VM, you simply put them in the same directory as the \nVagrantfile\n and then use them in the VM under \n/vagrant\n. The reverse is also true.\n\n\nTo learn more than the basics covered in this tutorial, we encourage you to refer to the \nofficial documentation\n.", 
            "title": "Vagrant"
        }, 
        {
            "location": "/advanced/Vagrant/README/#ul-hpc-tutorial-create-and-reproduce-work-environments-using-vagrant", 
            "text": "", 
            "title": "UL HPC Tutorial: Create and reproduce work environments using Vagrant"
        }, 
        {
            "location": "/advanced/Vagrant/README/#introduction", 
            "text": "Vagrant is a tool that allows to easily and rapidly create and configure reproducible and portable work environments using Virtual Machines. This is especially useful if you want to test your work in a stable and controlled environment and minimize the various unintended or untrackable changes that may occur on a physical machine.  In this tutorial, we are going to explain the steps to install Vagrant and create your first basic Linux Virtual Machine with it.", 
            "title": "Introduction"
        }, 
        {
            "location": "/advanced/Vagrant/README/#vagrant-installation", 
            "text": "Prerequisite:  Vagrant can use many Virtual Machine providers such as  VirtualBox ,  VMware  and  Docker  with VirtualBox being the easiest to use, and the default option in Vagrant.  Our first step is to install VirtualBox, you can download and install the correct version for your operating system from the  official website . In many Linux distributions it is provided as a package from the standard repositories, thus you can use your usual package manager to install it.  Once this prerequisite is met, we can install Vagrant. Download the correct version for your operating system on the  official website  and install it.", 
            "title": "Vagrant installation"
        }, 
        {
            "location": "/advanced/Vagrant/README/#using-vagrant-to-create-a-virtual-machine", 
            "text": "The main advantage of Vagrant is that it lets you import and use pre-configured Virtual Machines (called  boxes  in this context) which can become bases for your own customizations (installed applications, libraries, etc). With Vagrant it becomes really fast and effortless to create and run a new Virtual Machine.  The Vagrant boxes contain the disk image of a VM without the virtual hardware details of the VM, which are initialized by Vagrant and can be edited by the user.  The first step is to choose a pre-configured box to use. It is possible to create your own from scratch yet this is not in the scope of the current tutorial. \nFreely available boxes can be found at the following two main sources:   Atlas corps box catalog  vagrantbox.es catalaog   The first catalog is the default box download location for Vagrant. This means that you can directly use the name of the boxes you find here with Vagrant (e.g.  ubuntu/trusty64 ). \nTo use the second catalog you would additionaly need to provide the source box URL, yet this catalog provides a much richer variety of boxes.  Adding a new box  To add a box and make it usable in Vagrant, we are going to use the  vagrant box add  command. In the example below we will add one box from each of the catalogs in order to present the different possibilities. \nWe are going to add the  ubuntu/trusty64  box from the Atlas catalog and the  Ubuntu 14.04  box (by its  url ) from the vagrantbox.es catalog.  To add the first box, we use the following command (which may take some time due to the time needed to download the box):      $  vagrant box add ubuntu/trusty64\n    ==  box: Loading metadata for box 'ubuntu/trusty64'\n        box: URL: https://vagrantcloud.com/ubuntu/trusty64\n    ==  box: Adding box 'ubuntu/trusty64' (v14.04) for provider: virtualbox\n        box: Downloading: https://vagrantcloud.com/ubuntu/boxes/trusty64/versions/14.04/providers/virtualbox.box\n    ==  box: Successfully added box 'ubuntu/trusty64' (v14.04) for 'virtualbox'!  In this case, you just had to give the name of the box and Vagrant found the box by itself and added the box under the  ubuntu/trusty64  name.  To list the local boxes available to Vagrant for initialization of new VMs, we use the  vagrant box list  command:      $  vagrant box list\n    ubuntu/trusty64    (virtualbox, 14.04)  To add the second box, you need to use a slightly different syntax since you need to precise the name you want to give to the box as well as its source URL:      $  vagrant box add ubuntu14.04 https://github.com/kraksoft/vagrant-box-ubuntu/releases/download/14.04/ubuntu-14.04-amd64.box\n    ==  box: Adding box 'ubuntu14.04' (v0) for provider: \n        box: Downloading: https://github.com/kraksoft/vagrant-box-ubuntu/releases/download/14.04/ubuntu-14.04-amd64.box\n    ==  box: Successfully added box 'ubuntu14.04' (v0) for 'virtualbox'!  Now a second box will be available to Vagrant under the name  ubuntu14.04 :      $  vagrant box list\n    ubuntu/trusty64    (virtualbox, 14.04)\n    ubuntu14.04        (virtualbox, 0)  In the rest of the tutorial we are only going to use the first box. To remove a box we use the  vagrant box remove  command as follows:      $  vagrant box remove ubuntu14.04\n    Removing box 'ubuntu14.04' (v0) with provider 'virtualbox'...  Checking that it has been removed:      $  vagran box list\n    ubuntu/trusty64    (virtualbox, 14.04)  Creating a new Virtual Machine  Now we are going to create a new Virtual Machine using the  ubuntu/trusty64  box. \nWe will initialize it in an empty directory (which is not absolutely mandatory):      $  mkdir vagrant   cd vagrant  Next, we make Vagrant prepare the configuration file describing the VM:      $  vagrant init ubuntu/trusty64\n    A `Vagrantfile` has been placed in this directory. You are now\n    ready to `vagrant up` your first virtual environment! Please read\n    the comments in the Vagrantfile as well as documentation on\n    `vagrantup.com` for more information on using Vagrant.  You should now see a file named  Vagrantfile  in your directory. This file contains the minimal information for Vagrant to launch the VM. We could modify it to set up specific parameters of the VM (number of virtual cores, memory size, etc), but this constitutes advanced usage for which full documentation that can be found on the  official site . However, it may be interesting to understand what is actually needed in this file, since it contains a lot of commented information. \nThe minimal content of a  Vagrantfile  is as follows:      VAGRANTFILE_API_VERSION = \"2\"\n    Vagrant.configure(\"VAGRANTFILE_API_VERSION\") do |config|\n        config.vm.box = \"hashicorp/trusty64\"\n    end  This basically defines which version of the Vagrant API will be used to build the VM using the box given as a base.  Now, to launch the VM you only need to use the single  vagrant up  command in the same directory where the  Vagrantfile  exists (this may take some time since Vagrant is going to boot the VM and set its basic configuration):      $  vagrant up\n    Bringing machine 'default' up with 'virtualbox' provider...\n    ==  default: Importing base box 'ubuntu/trusty64'...\n    ==  default: Matching MAC address for NAT networking...\n    ==  default: Checking if box 'ubuntu/trusty64' is up to date...\n    ==  default: Setting the name of the VM: vagrant_default_1425476252413_67101\n    ==  default: Clearing any previously set forwarded ports...\n    ==  default: Clearing any previously set network interfaces...\n    ==  default: Preparing network interfaces based on configuration...\n        default: Adapter 1: nat\n    ==  default: Forwarding ports...\n        default: 22 =  2222 (adapter 1)\n    ==  default: Booting VM...\n    ==  default: Waiting for machine to boot. This may take a few minutes...\n        default: SSH address: 127.0.0.1:2222\n        default: SSH username: vagrant\n        default: SSH auth method: private key\n        default: Warning: Connection timeout. Retrying...\n        default: Warning: Remote connection disconnect. Retrying...\n    ==  default: Machine booted and ready!\n    ==  default: Checking for guest additions in VM...\n    ==  default: Mounting shared folders...\n        default: /vagrant =  /tmp/vagrant  Your VM is now up and running at this point. To access it, use the  vagrant ssh  command within the same directory :      $  vagrant ssh  You should now be connected to your VM and ready to work.  An interesting feature of Vagrant is that your computer (the \"host\") shares the directory that contains the  Vagrantfile  with your VM (the \"guest\"), where it is seen as  /vagrant .  Assuming you have a script or data files you want to access from within the VM, you simply put them in the same directory as the  Vagrantfile  and then use them in the VM under  /vagrant . The reverse is also true.  To learn more than the basics covered in this tutorial, we encourage you to refer to the  official documentation .", 
            "title": "Using Vagrant to create a Virtual Machine"
        }, 
        {
            "location": "/advanced/vm5k/README/", 
            "text": "Deploying virtual machines with Vm5k on Grid'5000\n\n\nThe Grid'5000 platform\n\n\nGrid\u20195000 is a scientific instrument distributed in 10 sites (mainly in France)\nfor research in large-scale parallel and distributed systems. It aims at providing\na highly reconfigurable, controllable and monitorable experimental platform to its users.\n\n\nThe infrastructure has reached 1035 nodes and 7782 cores and all sites are connected\nto RENATER with a 10 GB/s link, except Reims and Nantes (1 GB/s).\n\n\nThe goals of these tutorial is to:\n\n connect to Grid'5000\n\n discover the basic features of Grid'5000\n* use vm5k in order to deploy virtual machines on the grid\n\n\nGetting started\n\n\nUser charter\n\n\nYou should first read the \nUser Charter\n.\nThe mains points are:\n\n\n\n\nmaintain your \nuser reports\n up-to-date (publications, experiments, etc)\n\n\nduring working days and office hours (09:00 to 19:00), you should not use more than the equivalent of 2 hours of all the resources available in a cluster\n\n\nyour jobs should not cross the 09:00 and 19:00 boundaries during week days\n\n\nyou should not have more than 2 reservations in advance\n\n\n\n\nBasically: develop your experiments during day time, launch them over the nights and week-ends\n\n\nAccount\n\n\nFill the \naccount request form\n\nAt the Manager entry where you\u2019re asked the Grid\u20195000 login of the person\nwho\u2019s responsible of the account you\u2019re requesting, answer \nsvarrett\n\n\nConnection\n\n\nGrid'5000 provided 2 national access servers, located in Lille and Sophia.\nThese servers allow the user to connect the site's frontend.\n\n\n(user)   ssh \nlogin\n@access.grid5000.fr\n(access) ssh luxembourg\n\n\n\nAs an alternative, we can connect directly to the Luxembourg frontend from within the UL network:\n\n\n(user)   ssh \nlogin\n@grid5000.uni.lu\n\n\n\nReservation and deployment\n\n\nGrid'5000 uses OAR, all the oar commands you use on Gaia and Chaos clusters are valid.\n\n\n\n\nOAR documentation on hpc.uni.lu\n\n\n\n\nAdditionally to the computing nodes, G5K provides more resources:\n\n\n\n\nsubnets (ranges of IP for virtualization / cloud experiments)\n\n\nvlan (reconfigure the network equipments)\n\n\nstorage (iscsi / nfs)\n\n\n...\n\n\n\n\nThe job type \"deploy\" is also supported, which means that you can use kadeploy to reinstall\na cluster node and gain root access during the time of your jobs\n\n\nTutorials\n\n\nIt is highly recommended to read and follow the \nGetting Started tutorial\n,\nand all the others tutorials available in the \nUser Portal\n\n\nVM5K\n\n\nVm5k\n is a tool used to deploy a large number of virtual machines on the Grid\u20185000 platform.\n\n\nIn short, Vm5k\n\n\n\n\nmanages the reservation, locally if you work on one site, or globally with \noargridsub\n\n\ninstall the hosts with kadeploy, and configure the virtualization stack for you\n\n\nconfigure the network (bridges)\n\n\ndeploy the virtual machines\n\n\n\n\nInstallation (from git)\n\n\nYou will install VM5K in your home directory.\n\n\n\n\n\n\nSpecify the proxy configuration\n\n\n(frontend) export http_proxy=\"http://proxy:3128\"\n(frontend) export https_proxy=\"https://proxy:3128\"\n\n\n\n\n\n\n\nInstall execo, which is a dependency of vm5k\n\n\n(frontend) easy_install --user execo\n\n\n\n\n\n\n\nClone the git repository of vm5k and install it\n\n\n(frontend) git clone https://github.com/lpouillo/vm5k.git\n(frontend) cd vm5k\n(frontend) python setup.py  install --user\n\n\n\n\n\n\n\nIn your bashrc file, add the ~/.local/bin directory to the PATH environment variable\n\n\n(frontend) echo 'export PATH=$PATH:~/.local/bin' \n ~/.bashrc\n\n\n\n\n\n\n\nUsage\n\n\nBasic features\n\n\nEach deployment takes around 20 minutes, so you don't have to execute all the following examples:\n\n\n\n\nspawn 20 VMs on the granduc cluster, with a walltime of 30 minutes, and write the output files in the directory \nvm5k_test\n:\n(frontend) vm5k --n_vm 10 -r granduc -w 0:30:00 -o vm5k_test\n\n\n\n\n\n\n\nYou'll find the list of VMs with their ips in the file vm5k_test/vms.list\n\n\n10.172.1.45     vm-1\n10.172.1.46     vm-2\n10.172.1.47     vm-3\n10.172.1.48     vm-4\n10.172.1.49     vm-5\n10.172.1.50     vm-6\n10.172.1.51     vm-7\n10.172.1.52     vm-8\n...\n\n\n\n\n\n\n\nLet's spawn 100 VMs on 2 hosts in Nancy and Luxembourg\n\n\n(frontend) vm5k --n_vm 10 -r nancy:1,luxembourg:1 -w 0:30:00 -o vm5k_test\n\n\n\n\n\n\n\nWe can also specify the VM template (resources) with the parameter \n--vm_template\n\n\n(frontend) vm5k --n_vm 10 -r nancy:2,luxembourg:2 -w 0:30:00 -o vm5k_test --vm_template '\nvm mem=\"4096\" hdd=\"10\" n_cpu=\"4\" cpuset=\"auto\"/\n'\n\n\n\n\n\n\n\nDistribution\n\n\n\n\n\n\nBalance the nodes on all the reserved hosts\n\n\n(frontend) vm5k --n_vm 100 -r granduc:4 -o vm5k_test -d n_by_hosts\n\n\n\n\n\n\n\nConcentrate the VMs on a minimal number of hosts\n\n\n(frontend) vm5k -r grid5000:20 -n 100 -o vm5k_test -d concentrated\n\n\n\n\n\n\n\nAdvanced feature: define the deployment topology\n\n\nYou can control the deployment topology, and specify finely the clusters, nodes and virtual machines per node.\n\n\nCreate a file named \ntopology.xml\n, and change the sites, cluster and host id as needed:\n\n\nvm5k\n\n  \nsite id=\"luxembourg\"\n\n    \ncluster id=\"granduc\"\n\n      \nhost id=\"granduc-2\"\n\n        \nvm mem=\"2048\" hdd=\"4\" id=\"vm-33\" cpu=\"1\"/\n\n        \nvm mem=\"2048\" hdd=\"4\" id=\"vm-34\" cpu=\"1\"/\n\n        \nvm mem=\"2048\" hdd=\"4\" id=\"vm-35\" cpu=\"1\"/\n\n      \n/host\n\n      \nhost id=\"granduc-3\"\n\n        \nvm mem=\"2048\" hdd=\"4\" id=\"vm-43\" cpu=\"1\"/\n\n        \nvm mem=\"2048\" hdd=\"4\" id=\"vm-44\" cpu=\"1\"/\n\n        \nvm mem=\"2048\" hdd=\"4\" id=\"vm-45\" cpu=\"1\"/\n\n        \nvm mem=\"2048\" hdd=\"4\" id=\"vm-43\" cpu=\"1\"/\n\n        \nvm mem=\"2048\" hdd=\"4\" id=\"vm-44\" cpu=\"1\"/\n\n      \n/host\n\n    \n/cluster\n\n  \n/site\n\n  \nsite id=\"nancy\"\n\n    \ncluster id=\"graphene\"\n\n      \nhost id=\"graphene-30\"\n\n        \nvm mem=\"2048\" hdd=\"4\" id=\"vm-30\" cpu=\"1\"/\n\n        \nvm mem=\"2048\" hdd=\"4\" id=\"vm-31\" cpu=\"1\"/\n\n      \n/host\n\n    \n/cluster\n\n  \n/site\n\n\n/vm5k\n\n\n\n\nGive this file to vm5k\n\n\n(frontend) vm5k -i topology.xml -w 0:30:0 -o vm5k_test\n\n\n\nExperiment\n\n\nWe will deploy 100 VMs and deploy the munin monitoring software.\nThis is an example, munin will allow you to monitor the activity on all the VMs.\n\n\nInstall munin clients on all the other servers\n\n\n\n\n\n\nSpawn 100 VMs\n\n\n(frontend) vm5k --n_vm 50 -w 2:00:00 -r luxembourg -o hpcschool2015 -o vm5k_xp\n\n\n\n\n\n\n\nLaunch a script on all the VMs after their deployment, we will use \ntaktuk\n (you could also clush, pdsh, etc)\n\n\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ apt-get update ]\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ apt-get install -y munin-node stress ]\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ 'echo cidr_allow 10.0.0.0/8 \n /etc/munin/munin-node.conf' ]\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ '/etc/init.d/munin-node restart' ]\n\n\n\n\n\n\n\nInstall munin server on the first physical host\n\n\n\n\n\n\nChoose the first virtual machines\n\n\n(frontend) head -n 1 vm5k_xp/vms.list\n10.172.1.45     vm-1\n\n\n\n\n\n\n\nTransfer the list of virtual machines to the VM\n\n\n(frontend) scp vm5k_xp/vms.list root@10.172.1.45:/tmp/\n\n\n\n\n\n\n\nConnect to the VM in order to install and configure munin\n\n\n(frontend) ssh root@10.172.1.45\n\n(vm-1) apt-get install munin apache2\n\n\n\n\n\n\n\nConfigure the Apache http server\n\n\n(vm-1) sed -i '/[aA]llow/d' /etc/apache2/conf.d/munin\n(vm-1) apache2ctl restart\n\n\n\n\n\n\n\nGenerate the munin configuration\n\n\n(vm-1) cat /tmp/vms.list  | awk '{print \"[\"$2\".g5k]\\n    address \"$1\"\\n    use_node_name yes\\n\"}' \n /etc/munin/munin.conf\n(vm-1) /etc/init.d/munin restart\n\n\n\n\n\n\n\nConnect to munin\n\n\n\n\n\n\nLet's generate a fake activity, stress the VM during 60 seconds\n\n\n(frontend) taktuk -l root -f hpcschool2015/vms.list broadcast exec [ 'stress -c 1 -t 60' ]\n\n\n\n\n\n\n\nOpen a ssh tunnel on port 80\n\n\n(user) ssh -L1080:10.172.1.45:80 \nlogin\n@grid5000.uni.lu\n\n\n\n\n\n\n\nOpen a browser and navigates to \nhttp://localhost:1080", 
            "title": "Grid5000: Automatic VM deployment with VM5K"
        }, 
        {
            "location": "/advanced/vm5k/README/#deploying-virtual-machines-with-vm5k-on-grid5000", 
            "text": "", 
            "title": "Deploying virtual machines with Vm5k on Grid'5000"
        }, 
        {
            "location": "/advanced/vm5k/README/#the-grid5000-platform", 
            "text": "Grid\u20195000 is a scientific instrument distributed in 10 sites (mainly in France)\nfor research in large-scale parallel and distributed systems. It aims at providing\na highly reconfigurable, controllable and monitorable experimental platform to its users.  The infrastructure has reached 1035 nodes and 7782 cores and all sites are connected\nto RENATER with a 10 GB/s link, except Reims and Nantes (1 GB/s).  The goals of these tutorial is to:  connect to Grid'5000  discover the basic features of Grid'5000\n* use vm5k in order to deploy virtual machines on the grid", 
            "title": "The Grid'5000 platform"
        }, 
        {
            "location": "/advanced/vm5k/README/#getting-started", 
            "text": "", 
            "title": "Getting started"
        }, 
        {
            "location": "/advanced/vm5k/README/#user-charter", 
            "text": "You should first read the  User Charter .\nThe mains points are:   maintain your  user reports  up-to-date (publications, experiments, etc)  during working days and office hours (09:00 to 19:00), you should not use more than the equivalent of 2 hours of all the resources available in a cluster  your jobs should not cross the 09:00 and 19:00 boundaries during week days  you should not have more than 2 reservations in advance   Basically: develop your experiments during day time, launch them over the nights and week-ends", 
            "title": "User charter"
        }, 
        {
            "location": "/advanced/vm5k/README/#account", 
            "text": "Fill the  account request form \nAt the Manager entry where you\u2019re asked the Grid\u20195000 login of the person\nwho\u2019s responsible of the account you\u2019re requesting, answer  svarrett", 
            "title": "Account"
        }, 
        {
            "location": "/advanced/vm5k/README/#connection", 
            "text": "Grid'5000 provided 2 national access servers, located in Lille and Sophia.\nThese servers allow the user to connect the site's frontend.  (user)   ssh  login @access.grid5000.fr\n(access) ssh luxembourg  As an alternative, we can connect directly to the Luxembourg frontend from within the UL network:  (user)   ssh  login @grid5000.uni.lu", 
            "title": "Connection"
        }, 
        {
            "location": "/advanced/vm5k/README/#reservation-and-deployment", 
            "text": "Grid'5000 uses OAR, all the oar commands you use on Gaia and Chaos clusters are valid.   OAR documentation on hpc.uni.lu   Additionally to the computing nodes, G5K provides more resources:   subnets (ranges of IP for virtualization / cloud experiments)  vlan (reconfigure the network equipments)  storage (iscsi / nfs)  ...   The job type \"deploy\" is also supported, which means that you can use kadeploy to reinstall\na cluster node and gain root access during the time of your jobs", 
            "title": "Reservation and deployment"
        }, 
        {
            "location": "/advanced/vm5k/README/#tutorials", 
            "text": "It is highly recommended to read and follow the  Getting Started tutorial ,\nand all the others tutorials available in the  User Portal", 
            "title": "Tutorials"
        }, 
        {
            "location": "/advanced/vm5k/README/#vm5k", 
            "text": "Vm5k  is a tool used to deploy a large number of virtual machines on the Grid\u20185000 platform.  In short, Vm5k   manages the reservation, locally if you work on one site, or globally with  oargridsub  install the hosts with kadeploy, and configure the virtualization stack for you  configure the network (bridges)  deploy the virtual machines", 
            "title": "VM5K"
        }, 
        {
            "location": "/advanced/vm5k/README/#installation-from-git", 
            "text": "You will install VM5K in your home directory.    Specify the proxy configuration  (frontend) export http_proxy=\"http://proxy:3128\"\n(frontend) export https_proxy=\"https://proxy:3128\"    Install execo, which is a dependency of vm5k  (frontend) easy_install --user execo    Clone the git repository of vm5k and install it  (frontend) git clone https://github.com/lpouillo/vm5k.git\n(frontend) cd vm5k\n(frontend) python setup.py  install --user    In your bashrc file, add the ~/.local/bin directory to the PATH environment variable  (frontend) echo 'export PATH=$PATH:~/.local/bin'   ~/.bashrc", 
            "title": "Installation (from git)"
        }, 
        {
            "location": "/advanced/vm5k/README/#usage", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/advanced/vm5k/README/#basic-features", 
            "text": "Each deployment takes around 20 minutes, so you don't have to execute all the following examples:   spawn 20 VMs on the granduc cluster, with a walltime of 30 minutes, and write the output files in the directory  vm5k_test : (frontend) vm5k --n_vm 10 -r granduc -w 0:30:00 -o vm5k_test    You'll find the list of VMs with their ips in the file vm5k_test/vms.list  10.172.1.45     vm-1\n10.172.1.46     vm-2\n10.172.1.47     vm-3\n10.172.1.48     vm-4\n10.172.1.49     vm-5\n10.172.1.50     vm-6\n10.172.1.51     vm-7\n10.172.1.52     vm-8\n...    Let's spawn 100 VMs on 2 hosts in Nancy and Luxembourg  (frontend) vm5k --n_vm 10 -r nancy:1,luxembourg:1 -w 0:30:00 -o vm5k_test    We can also specify the VM template (resources) with the parameter  --vm_template  (frontend) vm5k --n_vm 10 -r nancy:2,luxembourg:2 -w 0:30:00 -o vm5k_test --vm_template ' vm mem=\"4096\" hdd=\"10\" n_cpu=\"4\" cpuset=\"auto\"/ '    Distribution    Balance the nodes on all the reserved hosts  (frontend) vm5k --n_vm 100 -r granduc:4 -o vm5k_test -d n_by_hosts    Concentrate the VMs on a minimal number of hosts  (frontend) vm5k -r grid5000:20 -n 100 -o vm5k_test -d concentrated    Advanced feature: define the deployment topology  You can control the deployment topology, and specify finely the clusters, nodes and virtual machines per node.  Create a file named  topology.xml , and change the sites, cluster and host id as needed:  vm5k \n   site id=\"luxembourg\" \n     cluster id=\"granduc\" \n       host id=\"granduc-2\" \n         vm mem=\"2048\" hdd=\"4\" id=\"vm-33\" cpu=\"1\"/ \n         vm mem=\"2048\" hdd=\"4\" id=\"vm-34\" cpu=\"1\"/ \n         vm mem=\"2048\" hdd=\"4\" id=\"vm-35\" cpu=\"1\"/ \n       /host \n       host id=\"granduc-3\" \n         vm mem=\"2048\" hdd=\"4\" id=\"vm-43\" cpu=\"1\"/ \n         vm mem=\"2048\" hdd=\"4\" id=\"vm-44\" cpu=\"1\"/ \n         vm mem=\"2048\" hdd=\"4\" id=\"vm-45\" cpu=\"1\"/ \n         vm mem=\"2048\" hdd=\"4\" id=\"vm-43\" cpu=\"1\"/ \n         vm mem=\"2048\" hdd=\"4\" id=\"vm-44\" cpu=\"1\"/ \n       /host \n     /cluster \n   /site \n   site id=\"nancy\" \n     cluster id=\"graphene\" \n       host id=\"graphene-30\" \n         vm mem=\"2048\" hdd=\"4\" id=\"vm-30\" cpu=\"1\"/ \n         vm mem=\"2048\" hdd=\"4\" id=\"vm-31\" cpu=\"1\"/ \n       /host \n     /cluster \n   /site  /vm5k   Give this file to vm5k  (frontend) vm5k -i topology.xml -w 0:30:0 -o vm5k_test", 
            "title": "Basic features"
        }, 
        {
            "location": "/advanced/vm5k/README/#experiment", 
            "text": "We will deploy 100 VMs and deploy the munin monitoring software.\nThis is an example, munin will allow you to monitor the activity on all the VMs.  Install munin clients on all the other servers    Spawn 100 VMs  (frontend) vm5k --n_vm 50 -w 2:00:00 -r luxembourg -o hpcschool2015 -o vm5k_xp    Launch a script on all the VMs after their deployment, we will use  taktuk  (you could also clush, pdsh, etc)  (frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ apt-get update ]\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ apt-get install -y munin-node stress ]\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ 'echo cidr_allow 10.0.0.0/8   /etc/munin/munin-node.conf' ]\n(frontend) taktuk -l root -f vm5k_xp/vms.list broadcast exec [ '/etc/init.d/munin-node restart' ]    Install munin server on the first physical host    Choose the first virtual machines  (frontend) head -n 1 vm5k_xp/vms.list\n10.172.1.45     vm-1    Transfer the list of virtual machines to the VM  (frontend) scp vm5k_xp/vms.list root@10.172.1.45:/tmp/    Connect to the VM in order to install and configure munin  (frontend) ssh root@10.172.1.45\n\n(vm-1) apt-get install munin apache2    Configure the Apache http server  (vm-1) sed -i '/[aA]llow/d' /etc/apache2/conf.d/munin\n(vm-1) apache2ctl restart    Generate the munin configuration  (vm-1) cat /tmp/vms.list  | awk '{print \"[\"$2\".g5k]\\n    address \"$1\"\\n    use_node_name yes\\n\"}'   /etc/munin/munin.conf\n(vm-1) /etc/init.d/munin restart    Connect to munin    Let's generate a fake activity, stress the VM during 60 seconds  (frontend) taktuk -l root -f hpcschool2015/vms.list broadcast exec [ 'stress -c 1 -t 60' ]    Open a ssh tunnel on port 80  (user) ssh -L1080:10.172.1.45:80  login @grid5000.uni.lu    Open a browser and navigates to  http://localhost:1080", 
            "title": "Experiment"
        }, 
        {
            "location": "/contributing/", 
            "text": "Proposing a new tutorial / Contributing to this repository\n\n\nYou're using a specific software on the UL HPC platform not listed in the above list? Then most probably you\n\n\n\n\ndeveloped a set of script to effectively run that software \n\n\nused to face issues such that you're aware (eventually unconsciously) of tricks and tips for that specific usage.\n\n\n\n\nThen your inputs are valuable for the other users and we would appreciate your help to complete this repository with new topics/entries.\n\n\nTo do that, the general approach is similar to the one proposed by \nGithub via the Forking procedure\n.\nSince we use \ngit-flow\n, your workflow for contributing to this repository should typically involve the following steps: \n\n\n\n\nFork it\n\n\nInitialize your local copy of the repository (including git submodules etc.): \nmake setup\n\n\nCreate your feature branch: \ngit flow feature start \nfeature_name\n\n\nCommit your changes: \ngit commit -am 'Added some feature'\n\n\nPublish your feature branch: \ngit flow feature publish \nfeature_name\n\n\nCreate new \nPull Request\n\n\n\n\nMore details are provided below.\n\n\ngit-flow\n\n\nThe Git branching model for this repository follows the guidelines of \ngitflow\n.\nIn particular, the central repo (on \ngithub.com\n) holds two main branches with an infinite lifetime:\n\n\n\n\nproduction\n: the \nproduction-ready\n tutorials\n\n\ndevel\n: the main branch where the latest developments interviene. This is the \ndefault\n branch you get when you clone the repo. \n\n\n\n\nNew tutorial layout\n\n\nSo assuming you have \nforked this repository\n to work freely on your own copy of it, you can now feed a new tutorial, assuming you follow the below guidelines.\n\n\nDirectory Layout\n\n\n{advanced | basic}/\nname\n  # Select the appropriate root directory\n\u251c\u2500\u2500 .root -\n ../../        # Symlink to the root directory\n\u251c\u2500\u2500 README.md              # Main tutorial file, in Markdown\n\u251c\u2500\u2500 tutorial_\nname\n.pdf    # Slides proposing an overview of the tutorial\n\u251c\u2500\u2500 Makefile               # GNU Makefile offering the targets 'fetch', 'compile', 'run' and 'run_interactive' \n\u251c\u2500\u2500 plots                  # Directory hosting the Gnuplots / R plots data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Makefile -\n .root/.submodules/Makefiles/gnuplot/Makefile # in case of Gnuplot...\n\u251c\u2500\u2500 runs/                  # Directory hosting the logs of the runs\n\u251c\u2500\u2500 scripts/               # Eventually, a directory hosting some specific scripts\n\u2514\u2500\u2500 src/                   \n    \u2514\u2500\u2500 slides/            # bearmer sources of the slides\n        \u251c\u2500\u2500 Makefile -\n .root/.submodules/Makefiles/latex/Makefile\n        \u251c\u2500\u2500 tutorial_\nname\n.tex        # Main LaTeX sources\n        \u251c\u2500\u2500 VERSION -\n .root/VERSION   # Version file\n        \u251c\u2500\u2500 __config.sty               # Specifid configs for the slides\n        \u251c\u2500\u2500 _style.sty -\n .root/.templates/slides/_style.sty\n        \u251c\u2500\u2500 beamerthemeFalkor.sty -\n .root/.submodules/beamerthemeFalkor/beamerthemeFalkor.sty\n        \u251c\u2500\u2500 figures/                   # Local figures\n        \u2514\u2500\u2500 images -\n .root/.templates/slides/images\n# Prepare the appropriate link for ReadtheDocs\ndocs/{advanced | basic}/\nname\n.md -\n ../../../{advanced | basic}/\nname\n/README.md\n\n\n\n\nYou shall stick to a single \nREADME.md\n file, (using the \nmarkdown\n format).\nRemember that they shall be understandable for users having no or very few\nknowledge on your topic!\n\n\nOne \nproposal\n to organize the workflow of your tutorial: \n\n\n\n\nSelect a typical sample example that will be used throughout all the tutorial, that is easy to fetch from the official page of the software. Adapt the \nmake fetch\n directive in your root \nMakefile\n to perform the corresponding actions.\n\n\n(eventually) detail how to build the sources (using \nRESIF\n). Adapt the \nmake build\n accordingly.\n\n\ndedicate a section to the running of this example in an \ninteractive\n job such that the reader has a better understanding of: \n\n\nthe involved modules to load \n\n\nthe classical way to execute the software\n\n\netc.\n   Adapt also the \nmake run_interactive\n accordingly\n\n\ndedicate a second section to the running of the example in a \npassive\n job, typically providing a generic launcher script adapted to your software. You might adapt / extend the \nUL HPC launcher scripts\n the same way to extend these tutorials. Adapt also the \nmake run\n accordingly.\n\n\na last section would typically involves hints / elements to benchmark the execution, add tips/tricks to improve the performances (and see the effects of those improvements) and have a way to plot the results.  Adapt the \nmake plot\n accordingly\n\n\n\n\nSemantic Versionning\n\n\nThe operation consisting of releasing a new version of this repository is automated by a set of tasks within the \nMakefile\n at the root of this repository. \n\n\nIn this context, a version number have the following format:\n\n\n  \nmajor\n.\nminor\n.\npatch\n\n\n\n\nwhere:\n\n\n\n\n major \n corresponds to the major version number\n\n\n minor \n corresponds to the minor version number\n\n\n patch \n corresponds to the patching version number\n\n\n\n\nExample: \n1.2.0\n\n\nThe current version number is stored in the file \nVERSION\n. \nDO NOT EDIT THIS FILE\n, use the below primitives to affect the number it contains.\n\nFor more information on the version, run:\n\n\n $\n make versioninfo\n\n\n\nIf a new  version number such be bumped, you simply have to run:\n\n\n $\n make start_bump_{major,minor,patch}\n\n\n\nThis will start the release process for you using \ngit-flow\n.\nThen, to make the release effective, just run:\n\n\n $\n make release\n\n\n\nThis will finalize the release using \ngit-flow\n, create the appropriate tag and merge all things the way they should be.", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#proposing-a-new-tutorial-contributing-to-this-repository", 
            "text": "You're using a specific software on the UL HPC platform not listed in the above list? Then most probably you   developed a set of script to effectively run that software   used to face issues such that you're aware (eventually unconsciously) of tricks and tips for that specific usage.   Then your inputs are valuable for the other users and we would appreciate your help to complete this repository with new topics/entries.  To do that, the general approach is similar to the one proposed by  Github via the Forking procedure .\nSince we use  git-flow , your workflow for contributing to this repository should typically involve the following steps:    Fork it  Initialize your local copy of the repository (including git submodules etc.):  make setup  Create your feature branch:  git flow feature start  feature_name  Commit your changes:  git commit -am 'Added some feature'  Publish your feature branch:  git flow feature publish  feature_name  Create new  Pull Request   More details are provided below.", 
            "title": "Proposing a new tutorial / Contributing to this repository"
        }, 
        {
            "location": "/contributing/#git-flow", 
            "text": "The Git branching model for this repository follows the guidelines of  gitflow .\nIn particular, the central repo (on  github.com ) holds two main branches with an infinite lifetime:   production : the  production-ready  tutorials  devel : the main branch where the latest developments interviene. This is the  default  branch you get when you clone the repo.", 
            "title": "git-flow"
        }, 
        {
            "location": "/contributing/#new-tutorial-layout", 
            "text": "So assuming you have  forked this repository  to work freely on your own copy of it, you can now feed a new tutorial, assuming you follow the below guidelines.  Directory Layout  {advanced | basic}/ name   # Select the appropriate root directory\n\u251c\u2500\u2500 .root -  ../../        # Symlink to the root directory\n\u251c\u2500\u2500 README.md              # Main tutorial file, in Markdown\n\u251c\u2500\u2500 tutorial_ name .pdf    # Slides proposing an overview of the tutorial\n\u251c\u2500\u2500 Makefile               # GNU Makefile offering the targets 'fetch', 'compile', 'run' and 'run_interactive' \n\u251c\u2500\u2500 plots                  # Directory hosting the Gnuplots / R plots data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Makefile -  .root/.submodules/Makefiles/gnuplot/Makefile # in case of Gnuplot...\n\u251c\u2500\u2500 runs/                  # Directory hosting the logs of the runs\n\u251c\u2500\u2500 scripts/               # Eventually, a directory hosting some specific scripts\n\u2514\u2500\u2500 src/                   \n    \u2514\u2500\u2500 slides/            # bearmer sources of the slides\n        \u251c\u2500\u2500 Makefile -  .root/.submodules/Makefiles/latex/Makefile\n        \u251c\u2500\u2500 tutorial_ name .tex        # Main LaTeX sources\n        \u251c\u2500\u2500 VERSION -  .root/VERSION   # Version file\n        \u251c\u2500\u2500 __config.sty               # Specifid configs for the slides\n        \u251c\u2500\u2500 _style.sty -  .root/.templates/slides/_style.sty\n        \u251c\u2500\u2500 beamerthemeFalkor.sty -  .root/.submodules/beamerthemeFalkor/beamerthemeFalkor.sty\n        \u251c\u2500\u2500 figures/                   # Local figures\n        \u2514\u2500\u2500 images -  .root/.templates/slides/images\n# Prepare the appropriate link for ReadtheDocs\ndocs/{advanced | basic}/ name .md -  ../../../{advanced | basic}/ name /README.md  You shall stick to a single  README.md  file, (using the  markdown  format).\nRemember that they shall be understandable for users having no or very few\nknowledge on your topic!  One  proposal  to organize the workflow of your tutorial:    Select a typical sample example that will be used throughout all the tutorial, that is easy to fetch from the official page of the software. Adapt the  make fetch  directive in your root  Makefile  to perform the corresponding actions.  (eventually) detail how to build the sources (using  RESIF ). Adapt the  make build  accordingly.  dedicate a section to the running of this example in an  interactive  job such that the reader has a better understanding of:   the involved modules to load   the classical way to execute the software  etc.\n   Adapt also the  make run_interactive  accordingly  dedicate a second section to the running of the example in a  passive  job, typically providing a generic launcher script adapted to your software. You might adapt / extend the  UL HPC launcher scripts  the same way to extend these tutorials. Adapt also the  make run  accordingly.  a last section would typically involves hints / elements to benchmark the execution, add tips/tricks to improve the performances (and see the effects of those improvements) and have a way to plot the results.  Adapt the  make plot  accordingly   Semantic Versionning  The operation consisting of releasing a new version of this repository is automated by a set of tasks within the  Makefile  at the root of this repository.   In this context, a version number have the following format:     major . minor . patch   where:    major   corresponds to the major version number   minor   corresponds to the minor version number   patch   corresponds to the patching version number   Example:  1.2.0  The current version number is stored in the file  VERSION .  DO NOT EDIT THIS FILE , use the below primitives to affect the number it contains. \nFor more information on the version, run:   $  make versioninfo  If a new  version number such be bumped, you simply have to run:   $  make start_bump_{major,minor,patch}  This will start the release process for you using  git-flow .\nThen, to make the release effective, just run:   $  make release  This will finalize the release using  git-flow , create the appropriate tag and merge all things the way they should be.", 
            "title": "New tutorial layout"
        }, 
        {
            "location": "/rtfd/", 
            "text": "The documentation for these tutorials is handled by \nRead the Docs\n, a web service dedicated to documentation management for the open source community.\n\n\n\n\nReference documentation\n\n\n\n\nBy default, the \nULHPC/tutorials\n repository is bound to the \nulhpc-tutorials\n project on Read the Docs. \n\n\nYou might wish to generate locally the docs:\n\n\n\n\nInstall \nmkdocs\n\n\nPreview your documentation from the project root by running \nmkdocs serve\n and visite with your favorite browser the URL \nhttp://localhost:8000\n\n\nbuild the full documentation locally by running \nmkdocs build\n to create the \nsite/\n directory.", 
            "title": "RTFD"
        }, 
        {
            "location": "/contacts/", 
            "text": "These tutorials has been implemented in the context of the \nUL HPC\n Platform of the \nUniversity of Luxembourg\n by the \nUL HPC Management Team\n.\n\n\nYet we are grateful to the following users of the platform who helped us to consolidate or complete these tutorials (in alphabetical order):\n\n\n\n\nXavier Besseron\n\n\nJoseph Emeras\n\n\nMaxime Schmitt\n\n\nP. Wohlschlegel\n\n\n\n\nYou can submit bug / issues / feature request using the \nULHPC/tutorials Tracker\n. \nAlternatively, you can contact the \nUL HPC Management Team\n responsible for these tutorials using the following email address: \nhpc-sysadmins@uni.lu", 
            "title": "Contacts"
        }
    ]
}